{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c190e92f-9245-4333-a532-1fae97f41c27",
   "metadata": {},
   "source": [
    "# Prompt Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5350abad-e099-4e7b-8d4d-e37fc9285fc6",
   "metadata": {},
   "source": [
    "## Load API Keys"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac06c78c-3a80-4ee3-8198-4a633045e699",
   "metadata": {},
   "source": [
    "<p style=\"background-color:#fff6e4; padding:15px; border-width:3px; border-color:#f5ecda; border-style:solid; border-radius:6px\"> ‚è≥ <b>Note <code>(Kernel Starting)</code>:</b> This notebook takes about 30 seconds to be ready to use. You may start and watch the video while you wait.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "31fe38d6-537f-416a-a3ad-08fbdd541488",
   "metadata": {
    "height": 47
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2876631c-db76-43d8-8f20-bc964d070cf1",
   "metadata": {
    "height": 115
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from utils import get_llama_api_key, get_llama_base_url, get_together_api_key\n",
    "\n",
    "llama_api_key = get_llama_api_key()\n",
    "llama_base_url = get_llama_base_url()\n",
    "together_api_key = get_together_api_key()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "24e1dee1-ece4-42d7-b47c-504e47df4853",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nprint(llama_api_key)\\nprint(llama_base_url)\\nprint(together_api_key)\\n'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "print(llama_api_key)\n",
    "print(llama_base_url)\n",
    "print(together_api_key)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "46a6a1a8-53d9-4ba1-9367-e45d62fd711c",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "#!pip install llama-prompt-ops==0.0.7"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c5e4fb0-a0a9-43cb-b636-e8c9654f7075",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#fff6ff; padding:13px; border-width:3px; border-color:#efe6ef; border-style:solid; border-radius:6px\">\n",
    "<p> üíª &nbsp; <b>Access <code>requirements.txt</code> and <code>helper.py</code> files:</b> 1) click on the <em>\"File\"</em> option on the top menu of the notebook and then 2) click on <em>\"Open\"</em>.</p>\n",
    "\n",
    "<p> ‚¨á &nbsp; <b>Download Notebooks:</b> 1) click on the <em>\"File\"</em> option on the top menu of the notebook and then 2) click on <em>\"Download as\"</em> and select <em>\"Notebook (.ipynb)\"</em>.</p>\n",
    "\n",
    "<p> üìí &nbsp; For more help, please see the <em>\"Appendix ‚Äì Tips, Help, and Download\"</em> Lesson.</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77db02ff-234d-4f19-b656-641fd125ff94",
   "metadata": {},
   "source": [
    "## Creating a sample project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "15c55a8f-5082-4166-8b9c-6f65c4c6754b",
   "metadata": {
    "height": 47
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/6] Creating project structure...\n",
      "‚úì Created project directory: my-project\n",
      "‚úì Created data directory\n",
      "‚úì Created prompts directory\n",
      "\n",
      "[2/6] Generating configuration file...\n",
      "‚úì Created config.yaml\n",
      "\n",
      "[3/6] Creating prompt template...\n",
      "‚úì Created prompt.txt\n",
      "\n",
      "[4/6] Generating sample dataset...\n",
      "‚úì Created dataset.json with 200 examples\n",
      "\n",
      "[5/6] Setting up environment...\n",
      "‚úì Created .env file\n",
      "\n",
      "[6/6] Creating documentation...\n",
      "‚úì Created README.md\n",
      "\n",
      "‚ú® Done! Project 'my-project' created successfully!\n",
      "\n",
      "To get started:\n",
      "1. cd my-project\n",
      "2. Edit the .env file to add your OPENROUTER_API_KEY\n",
      "   You can get an API key at: https://openrouter.ai/\n",
      "3. Run: llama-prompt-ops migrate\n"
     ]
    }
   ],
   "source": [
    "## Check if the folder exists: In the line [ -d \"my-project\" ] returns true if the directory is present; the || (‚Äúor‚Äù) means llama-prompt-ops create my-project executes only when that first test is false\n",
    "![ -d \"my-project\" ] || llama-prompt-ops create my-project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4b85284c-10b9-4593-bd0d-82aeb9fc06c6",
   "metadata": {
    "height": 30
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "config.yaml \u001b[34mdata\u001b[m\u001b[m        \u001b[34mprompts\u001b[m\u001b[m     README.md\n"
     ]
    }
   ],
   "source": [
    "!ls ./my-project/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7c78821-19c1-463d-9305-5fc97bee763c",
   "metadata": {},
   "source": [
    "## System prompt and dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c53d0950-b3c7-4713-99bc-47a8d1a1fc04",
   "metadata": {
    "height": 30
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are a helpful assistant. Extract and return a json with the following keys and values:\n",
      "- \"urgency\" as one of `high`, `medium`, `low`\n",
      "- \"sentiment\" as one of `negative`, `neutral`, `positive`\n",
      "- \"categories\" Create a dictionary with categories as keys and boolean values (True/False), where the value indicates whether the category is one of the best matching support category tags from: `emergency_repair_services`, `routine_maintenance_requests`, `quality_and_safety_concerns`, `specialized_cleaning_services`, `general_inquiries`, `sustainability_and_environmental_practices`, `training_and_support_requests`, `cleaning_services_scheduling`, `customer_feedback_and_complaints`, `facility_management_issues`\n",
      "Your complete message should be a valid json string that can be read directly and only contain the keys mentioned in the list above. Never enclose it in ```json...```, no newlines, no unnessacary whitespaces.\n"
     ]
    }
   ],
   "source": [
    "!cat my-project/prompts/prompt.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "08521291-31dc-4f85-8aa1-86b8b965a82c",
   "metadata": {
    "height": 30
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "  {\n",
      "    \"fields\": {\n",
      "      \"input\": \"Subject: Urgent Assistance Required for Specialized Cleaning Services\\n\\nDear ProCare Facility Solutions Support Team,\\n\\nI hope this message finds you well. My name is [Sender], and my family and I have been availing your services for our home for the past year. We have always appreciated the high standards and professionalism your team brings to maintaining our living environment.\\n\\nHowever, we are currently facing an urgent issue that requires immediate attention. We recently hosted a large gathering at our home, and despite our best efforts, there are several areas that now require specialized cleaning. Specifically, we need deep cleaning for our carpets and upholstery, as well as thorough window washing. The situation is quite pressing as we have more guests arriving soon, and we want to ensure our home is in pristine condition to welcome them.\\n\\nWe have tried some basic cleaning ourselves, but the results have not been satisfactory. Given the high standards we have come to expect from ProCare, we are confident that your team can handle this situation efficiently and effectively.\\n\\nCould you please arrange for a specialized cleaning team to visit our home at the earliest convenience? We would greatly appreciate it if this could be prioritized due to the urgency of the situation.\\n\\nThank you for your prompt attention to this matter. We look forward to your swift response and assistance.\\n\\nBest regards,\\n[Sender]\"\n",
      "    },\n",
      "    \"answer\": \"{\\\"categories\\\": {\\\"routine_maintenance_requests\\\": false, \\\"customer_feedback_and_complaints\\\": false, \\\"training_and_support_requests\\\": false, \\\"quality_and_safety_concerns\\\": false, \\\"sustainability_and_environmental_practices\\\": false, \\\"cleaning_services_scheduling\\\": false, \\\"specialized_cleaning_services\\\": true, \\\"emergency_repair_services\\\": false, \\\"facility_management_issues\\\": false, \\\"general_inquiries\\\": false}, \\\"sentiment\\\": \\\"neutral\\\", \\\"urgency\\\": \\\"high\\\"}\"\n",
      "  },\n",
      "  {\n",
      "    \"fields\": {\n",
      "      \"input\": \"Subject: Inquiry About Specialized Cleaning Services\\n\\nHi ProCare Support Team,\\n\\nI hope this message finds you well. My name is Alex, and I've been a client of ProCare Facility Solutions for a few months now. I must say, your services have been quite satisfactory so far, especially the routine maintenance and cleaning schedules.\\n\\nI am reaching out to inquire about your specialized cleaning services. Specifically, I am interested in deep cleaning and carpet maintenance for my residential property. While the regular cleaning has been great, I feel that a more thorough cleaning would really help maintain the pristine condition of my home.\\n\\nI haven't taken any steps yet to address this, as I wanted to get more information from your team first. Could you please provide me with details on how these specialized services work, the scheduling options available, and any additional costs involved?\\n\\nLooking forward to your response.\\n\\nBest regards,\\nAlex\"\n",
      "    },\n",
      "    \"answer\": \"{\\\"categories\\\": {\\\"routine_maintenance_requests\\\": false, \\\"customer_feedback_and_complaints\\\": false, \\\"training_and_support_requests\\\": false, \\\"quality_and_safety_concerns\\\": false, \\\"sustainability_and_environmental_practices\\\": false, \\\"cleaning_services_scheduling\\\": false, \\\"specialized_cleaning_services\\\": true, \\\"emergency_repair_services\\\": false, \\\"facility_management_issues\\\": false, \\\"general_inquiries\\\": true}, \\\"sentiment\\\": \\\"neutral\\\", \\\"urgency\\\": \\\"low\\\"}\"\n",
      "  },\n",
      "  {\n",
      "    \"fields\": {\n"
     ]
    }
   ],
   "source": [
    "!head -15 my-project/data/dataset.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "378a2ff9-ea3e-481f-b215-ab342340d2ee",
   "metadata": {
    "height": 30
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "system_prompt:\n",
      "  file: prompts/prompt.txt\n",
      "  inputs:\n",
      "  - question\n",
      "  outputs:\n",
      "  - answer\n",
      "dataset:\n",
      "  path: data/dataset.json\n",
      "  input_field:\n",
      "  - fields\n",
      "  - input\n",
      "  golden_output_field: answer\n",
      "model:\n",
      "  task_model: openrouter/meta-llama/llama-3.3-70b-instruct\n",
      "  proposer_model: openrouter/meta-llama/llama-3.3-70b-instruct\n",
      "metric:\n",
      "  class: llama_prompt_ops.core.metrics.FacilityMetric\n",
      "  strict_json: false\n",
      "  output_field: answer\n",
      "optimization:\n",
      "  strategy: llama\n"
     ]
    }
   ],
   "source": [
    "!cat my-project/config.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "98397945-4c85-48df-b464-b954464d2486",
   "metadata": {
    "height": 421
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting my-project/config.yaml\n"
     ]
    }
   ],
   "source": [
    "%%writefile my-project/config.yaml\n",
    "system_prompt:\n",
    "  file: prompts/prompt.txt\n",
    "  inputs:\n",
    "  - question\n",
    "  outputs:\n",
    "  - answer\n",
    "dataset:\n",
    "  path: data/dataset.json\n",
    "  input_field:\n",
    "  - fields\n",
    "  - input\n",
    "  golden_output_field: answer\n",
    "model:\n",
    "  task_model: together_ai/meta-llama/Llama-4-Scout-17B-16E-Instruct\n",
    "  proposer_model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
    "  api_base: https://api.together.xyz/v1\n",
    "metric:\n",
    "  class: llama_prompt_ops.core.metrics.FacilityMetric\n",
    "  strict_json: false\n",
    "  output_field: answer\n",
    "optimization:\n",
    "  strategy: llama\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "415d3a27-d4e8-4f44-be33-c2a1f77b1ce9",
   "metadata": {},
   "source": [
    "## Running prompt optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12fcde8a-9d4f-4b11-be4a-91c3933cf014",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#fff6ff; padding:13px; border-width:3px; border-color:#efe6ef; border-style:solid; border-radius:6px\">\n",
    "<p>Running prompt optimization can take a long time. To speed up running the notebooks, we will load saved results. You can change <code>run_optimization</code> to <code>True</code> to run the optimization.</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6f49a6d5-51fe-4a21-8b3d-f4f5f1b8870f",
   "metadata": {
    "height": 81,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded environment variables from .env\n",
      "Loaded configuration from config.yaml\n",
      " Using model with DSPy: together_ai/meta-llama/Llama-4-Scout-17B-16E-Instruct\n",
      "Using different models - Task: together_ai/meta-llama/Llama-4-Scout-17B-16E-Instruct, Proposer: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      " Using model with DSPy: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "Using metric: FacilityMetric\n",
      "Resolved relative dataset path to: /Users/ytchen/Documents/experimental/Llama4/L6/my-project/data/dataset.json\n",
      "Using dataset adapter: ConfigurableJSONAdapter\n",
      "2025-06-25 12:48:09,321 - root - WARNING - Model '' does not appear to be a Llama model. This library is optimized for Llama models.\n",
      "Auto-detected BasicOptimizationStrategy for model: \n",
      "2025-06-25 12:48:09,321 - root - WARNING - Model '<llama_prompt_ops.core.model.DSPyModelAdapter object at 0x10bed1550>' does not appear to be a Llama model. This library is optimized for Llama models and may not work as expected.\n",
      "2025-06-25 12:48:09,323 - root - INFO - Loaded 200 examples from /Users/ytchen/Documents/experimental/Llama4/L6/my-project/data/dataset.json\n",
      "2025-06-25 12:48:09,323 - root - INFO - Created dataset splits:\n",
      "2025-06-25 12:48:09,323 - root - INFO -   - Training:   50 examples (25.0% of total)\n",
      "2025-06-25 12:48:09,323 - root - INFO -   - Validation: 50 examples (25.0% of total)\n",
      "2025-06-25 12:48:09,323 - root - INFO -   - Testing:    100 examples (50.0% of total)\n",
      "Loaded prompt from file: /Users/ytchen/Documents/experimental/Llama4/L6/my-project/prompts/prompt.txt\n",
      "Using 'system_prompt' from config\n",
      "Using config filename as output prefix: config\n",
      "Starting prompt optimization...\n",
      "2025-06-25 12:48:09,324 - root - INFO - Applying BasicOptimizationStrategy to optimize prompt\n",
      "2025-06-25 12:48:09,324 - root - INFO - Training set size: 50\n",
      "2025-06-25 12:48:09,324 - root - INFO - Validation set size: 50\n",
      "2025-06-25 12:48:09,324 - root - INFO - Optimization strategy using 5 labeled demos, 4 bootstrapped demos with 18 threads\n",
      "2025-06-25 12:48:09,324 - root - INFO - Compiling program with 50 training examples and 50 validation examples\n",
      "2025-06-25 12:48:09,324 - root - WARNING - Debug module not available, continuing without enhanced debugging\n",
      "2025-06-25 12:48:09,324 - root - INFO - Starting DSPy optimization with enhanced debugging\n",
      "2025-06-25 12:48:09,324 - root - INFO - Program type: <class 'dspy.predict.predict.Predict'>\n",
      "2025-06-25 12:48:09,324 - root - INFO - Trainset size: 50\n",
      "2025-06-25 12:48:09,324 - root - INFO - Valset size: 50\n",
      "2025-06-25 12:48:09,324 - root - INFO - First trainset example structure: <class 'dspy.primitives.example.Example'>\n",
      "2025-06-25 12:48:09,325 - root - WARNING - Example missing required 'inputs' or 'outputs' attributes\n",
      "2025-06-25 12:48:09,325 - root - WARNING - Example attributes: ['__class__', '__contains__', '__delattr__', '__delitem__', '__dict__', '__dir__', '__doc__', '__eq__', '__firstlineno__', '__format__', '__ge__', '__getattr__', '__getattribute__', '__getitem__', '__getstate__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__iter__', '__le__', '__len__', '__lt__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__setitem__', '__sizeof__', '__static_attributes__', '__str__', '__subclasshook__', '__weakref__', '_demos', '_input_keys', '_output_keys', '_store', 'copy', 'get', 'inputs', 'items', 'keys', 'labels', 'toDict', 'values', 'with_inputs', 'without']\n",
      "2025-06-25 12:48:09,325 - root - INFO - Calling optimizer.compile\n",
      "2025/06/25 12:48:09 INFO dspy.teleprompt.mipro_optimizer_v2: \n",
      "RUNNING WITH THE FOLLOWING LIGHT AUTO RUN SETTINGS:\n",
      "num_trials: 7\n",
      "minibatch: False\n",
      "num_candidates: 5\n",
      "valset size: 50\n",
      "\n",
      "2025/06/25 12:48:09 INFO dspy.teleprompt.mipro_optimizer_v2: \n",
      "==> STEP 1: BOOTSTRAP FEWSHOT EXAMPLES <==\n",
      "2025/06/25 12:48:09 INFO dspy.teleprompt.mipro_optimizer_v2: These will be used as few-shot example candidates for our program and for creating instructions.\n",
      "\n",
      "2025/06/25 12:48:09 INFO dspy.teleprompt.mipro_optimizer_v2: Bootstrapping N=5 sets of demonstrations...\n",
      "Bootstrapping set 1/5\n",
      "Bootstrapping set 2/5\n",
      "Bootstrapping set 3/5\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[92m12:48:09 - LiteLLM:INFO\u001b[0m: utils.py:3119 - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.3-70B-Instruct-Turbo; provider = together_ai\n",
      "2025-06-25 12:48:09,345 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.3-70B-Instruct-Turbo; provider = together_ai\n",
      "2025-06-25 12:48:14,253 - httpx - INFO - HTTP Request: POST https://api.together.xyz/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m12:48:14 - LiteLLM:INFO\u001b[0m: utils.py:1215 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-25 12:48:14,275 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m12:48:14 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "2025-06-25 12:48:14,276 - LiteLLM - INFO - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "\u001b[92m12:48:14 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "2025-06-25 12:48:14,277 - LiteLLM - INFO - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "/Users/ytchen/Documents/experimental/Llama4/env/lib/python3.13/site-packages/pydantic/main.py:463: UserWarning: Pydantic serializer warnings:\n",
      "  PydanticSerializationUnexpectedValue(Expected 9 fields but got 5: Expected `Message` - serialized value may not be as expected [input_value=Message(content='[[ ## an...ields={'refusal': None}), input_type=Message])\n",
      "  PydanticSerializationUnexpectedValue(Expected `StreamingChoices` - serialized value may not be as expected [input_value=Choices(finish_reason='st...': 8489731715923927000}), input_type=Choices])\n",
      "  return self.__pydantic_serializer__.to_python(\n",
      "\u001b[92m12:48:14 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "2025-06-25 12:48:14,297 - LiteLLM - INFO - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "2025-06-25 12:48:21,545 - datasets - INFO - PyTorch version 2.7.0 available.\n",
      "  2%|‚ñâ                                           | 1/50 [00:14<11:47, 14.44s/it]\u001b[92m12:48:23 - LiteLLM:INFO\u001b[0m: utils.py:3119 - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.3-70B-Instruct-Turbo; provider = together_ai\n",
      "2025-06-25 12:48:23,780 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.3-70B-Instruct-Turbo; provider = together_ai\n",
      "2025-06-25 12:48:28,010 - httpx - INFO - HTTP Request: POST https://api.together.xyz/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m12:48:28 - LiteLLM:INFO\u001b[0m: utils.py:1215 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-25 12:48:28,015 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m12:48:28 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "2025-06-25 12:48:28,015 - LiteLLM - INFO - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "/Users/ytchen/Documents/experimental/Llama4/env/lib/python3.13/site-packages/pydantic/main.py:463: UserWarning: Pydantic serializer warnings:\n",
      "  PydanticSerializationUnexpectedValue(Expected 9 fields but got 5: Expected `Message` - serialized value may not be as expected [input_value=Message(content='[[ ## an...ields={'refusal': None}), input_type=Message])\n",
      "  PydanticSerializationUnexpectedValue(Expected `StreamingChoices` - serialized value may not be as expected [input_value=Choices(finish_reason='st...: 16560187157731758000}), input_type=Choices])\n",
      "  return self.__pydantic_serializer__.to_python(\n",
      "  4%|‚ñà‚ñä                                          | 2/50 [00:18<06:45,  8.44s/it]\u001b[92m12:48:28 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "2025-06-25 12:48:28,043 - LiteLLM - INFO - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "\u001b[92m12:48:28 - LiteLLM:INFO\u001b[0m: utils.py:3119 - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.3-70B-Instruct-Turbo; provider = together_ai\n",
      "2025-06-25 12:48:28,050 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.3-70B-Instruct-Turbo; provider = together_ai\n",
      "2025-06-25 12:48:34,391 - httpx - INFO - HTTP Request: POST https://api.together.xyz/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m12:48:34 - LiteLLM:INFO\u001b[0m: utils.py:1215 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-25 12:48:34,395 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m12:48:34 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "2025-06-25 12:48:34,395 - LiteLLM - INFO - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "/Users/ytchen/Documents/experimental/Llama4/env/lib/python3.13/site-packages/pydantic/main.py:463: UserWarning: Pydantic serializer warnings:\n",
      "  PydanticSerializationUnexpectedValue(Expected 9 fields but got 5: Expected `Message` - serialized value may not be as expected [input_value=Message(content='[[ ## an...ields={'refusal': None}), input_type=Message])\n",
      "  PydanticSerializationUnexpectedValue(Expected `StreamingChoices` - serialized value may not be as expected [input_value=Choices(finish_reason='st...': 5176076479437323000}), input_type=Choices])\n",
      "  return self.__pydantic_serializer__.to_python(\n",
      "  6%|‚ñà‚ñà‚ñã                                         | 3/50 [00:25<05:52,  7.50s/it]\u001b[92m12:48:34 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "2025-06-25 12:48:34,416 - LiteLLM - INFO - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "\u001b[92m12:48:34 - LiteLLM:INFO\u001b[0m: utils.py:3119 - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.3-70B-Instruct-Turbo; provider = together_ai\n",
      "2025-06-25 12:48:34,435 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.3-70B-Instruct-Turbo; provider = together_ai\n",
      "2025-06-25 12:48:40,892 - httpx - INFO - HTTP Request: POST https://api.together.xyz/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m12:48:40 - LiteLLM:INFO\u001b[0m: utils.py:1215 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-25 12:48:40,895 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m12:48:40 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "2025-06-25 12:48:40,896 - LiteLLM - INFO - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "  8%|‚ñà‚ñà‚ñà‚ñå                                        | 4/50 [00:31<06:03,  7.89s/it]\n",
      "Bootstrapped 4 full traces after 4 examples for up to 1 rounds, amounting to 4 attempts.\n",
      "Bootstrapping set 4/5\n",
      "/Users/ytchen/Documents/experimental/Llama4/env/lib/python3.13/site-packages/pydantic/main.py:463: UserWarning: Pydantic serializer warnings:\n",
      "  PydanticSerializationUnexpectedValue(Expected 9 fields but got 5: Expected `Message` - serialized value may not be as expected [input_value=Message(content='[[ ## an...ields={'refusal': None}), input_type=Message])\n",
      "  PydanticSerializationUnexpectedValue(Expected `StreamingChoices` - serialized value may not be as expected [input_value=Choices(finish_reason='st...: 14022644857222120000}), input_type=Choices])\n",
      "  return self.__pydantic_serializer__.to_python(\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[92m12:48:40 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "2025-06-25 12:48:40,937 - LiteLLM - INFO - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "\u001b[92m12:48:40 - LiteLLM:INFO\u001b[0m: utils.py:3119 - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.3-70B-Instruct-Turbo; provider = together_ai\n",
      "2025-06-25 12:48:40,939 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.3-70B-Instruct-Turbo; provider = together_ai\n",
      "2025-06-25 12:48:47,137 - httpx - INFO - HTTP Request: POST https://api.together.xyz/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m12:48:47 - LiteLLM:INFO\u001b[0m: utils.py:1215 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-25 12:48:47,142 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m12:48:47 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "2025-06-25 12:48:47,142 - LiteLLM - INFO - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "/Users/ytchen/Documents/experimental/Llama4/env/lib/python3.13/site-packages/pydantic/main.py:463: UserWarning: Pydantic serializer warnings:\n",
      "  PydanticSerializationUnexpectedValue(Expected 9 fields but got 5: Expected `Message` - serialized value may not be as expected [input_value=Message(content='[[ ## an...ields={'refusal': None}), input_type=Message])\n",
      "  PydanticSerializationUnexpectedValue(Expected `StreamingChoices` - serialized value may not be as expected [input_value=Choices(finish_reason='st...': 1599562238093676500}), input_type=Choices])\n",
      "  return self.__pydantic_serializer__.to_python(\n",
      "  2%|‚ñâ                                           | 1/50 [00:06<05:05,  6.23s/it]\u001b[92m12:48:47 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "2025-06-25 12:48:47,181 - LiteLLM - INFO - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "\u001b[92m12:48:47 - LiteLLM:INFO\u001b[0m: utils.py:3119 - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.3-70B-Instruct-Turbo; provider = together_ai\n",
      "2025-06-25 12:48:47,197 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.3-70B-Instruct-Turbo; provider = together_ai\n",
      "2025-06-25 12:48:50,324 - httpx - INFO - HTTP Request: POST https://api.together.xyz/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m12:48:50 - LiteLLM:INFO\u001b[0m: utils.py:1215 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-25 12:48:50,329 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m12:48:50 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "2025-06-25 12:48:50,329 - LiteLLM - INFO - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "/Users/ytchen/Documents/experimental/Llama4/env/lib/python3.13/site-packages/pydantic/main.py:463: UserWarning: Pydantic serializer warnings:\n",
      "  PydanticSerializationUnexpectedValue(Expected 9 fields but got 5: Expected `Message` - serialized value may not be as expected [input_value=Message(content='[[ ## an...ields={'refusal': None}), input_type=Message])\n",
      "  PydanticSerializationUnexpectedValue(Expected `StreamingChoices` - serialized value may not be as expected [input_value=Choices(finish_reason='st...': 7822475053783569000}), input_type=Choices])\n",
      "  return self.__pydantic_serializer__.to_python(\n",
      "  4%|‚ñà‚ñä                                          | 2/50 [00:09<03:33,  4.44s/it]\u001b[92m12:48:50 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "2025-06-25 12:48:50,362 - LiteLLM - INFO - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "\u001b[92m12:48:50 - LiteLLM:INFO\u001b[0m: utils.py:3119 - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.3-70B-Instruct-Turbo; provider = together_ai\n",
      "2025-06-25 12:48:50,372 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.3-70B-Instruct-Turbo; provider = together_ai\n",
      "2025-06-25 12:48:52,875 - httpx - INFO - HTTP Request: POST https://api.together.xyz/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m12:48:52 - LiteLLM:INFO\u001b[0m: utils.py:1215 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-25 12:48:52,880 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m12:48:52 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "2025-06-25 12:48:52,881 - LiteLLM - INFO - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "\u001b[92m12:48:52 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "2025-06-25 12:48:52,881 - LiteLLM - INFO - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "  6%|‚ñà‚ñà‚ñã                                         | 3/50 [00:11<03:07,  3.99s/it]\n",
      "Bootstrapped 3 full traces after 3 examples for up to 1 rounds, amounting to 3 attempts.\n",
      "Bootstrapping set 5/5\n",
      "/Users/ytchen/Documents/experimental/Llama4/env/lib/python3.13/site-packages/pydantic/main.py:463: UserWarning: Pydantic serializer warnings:\n",
      "  PydanticSerializationUnexpectedValue(Expected 9 fields but got 5: Expected `Message` - serialized value may not be as expected [input_value=Message(content='[[ ## an...ields={'refusal': None}), input_type=Message])\n",
      "  PydanticSerializationUnexpectedValue(Expected `StreamingChoices` - serialized value may not be as expected [input_value=Choices(finish_reason='st...: 18055046433942422000}), input_type=Choices])\n",
      "  return self.__pydantic_serializer__.to_python(\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[92m12:48:52 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "2025-06-25 12:48:52,918 - LiteLLM - INFO - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "\u001b[92m12:48:52 - LiteLLM:INFO\u001b[0m: utils.py:3119 - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.3-70B-Instruct-Turbo; provider = together_ai\n",
      "2025-06-25 12:48:52,921 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.3-70B-Instruct-Turbo; provider = together_ai\n",
      "2025-06-25 12:48:54,063 - httpx - INFO - HTTP Request: POST https://api.together.xyz/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m12:48:54 - LiteLLM:INFO\u001b[0m: utils.py:1215 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-25 12:48:54,067 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m12:48:54 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "2025-06-25 12:48:54,068 - LiteLLM - INFO - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "/Users/ytchen/Documents/experimental/Llama4/env/lib/python3.13/site-packages/pydantic/main.py:463: UserWarning: Pydantic serializer warnings:\n",
      "  PydanticSerializationUnexpectedValue(Expected 9 fields but got 5: Expected `Message` - serialized value may not be as expected [input_value=Message(content='[[ ## an...ields={'refusal': None}), input_type=Message])\n",
      "  PydanticSerializationUnexpectedValue(Expected `StreamingChoices` - serialized value may not be as expected [input_value=Choices(finish_reason='st...': 1195771460238044200}), input_type=Choices])\n",
      "  return self.__pydantic_serializer__.to_python(\n",
      "  2%|‚ñâ                                           | 1/50 [00:01<00:57,  1.18s/it]\u001b[92m12:48:54 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "2025-06-25 12:48:54,107 - LiteLLM - INFO - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "\u001b[92m12:48:54 - LiteLLM:INFO\u001b[0m: utils.py:3119 - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.3-70B-Instruct-Turbo; provider = together_ai\n",
      "2025-06-25 12:48:54,117 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.3-70B-Instruct-Turbo; provider = together_ai\n",
      "2025-06-25 12:48:56,970 - httpx - INFO - HTTP Request: POST https://api.together.xyz/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m12:48:56 - LiteLLM:INFO\u001b[0m: utils.py:1215 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-25 12:48:56,975 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m12:48:56 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "2025-06-25 12:48:56,976 - LiteLLM - INFO - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "/Users/ytchen/Documents/experimental/Llama4/env/lib/python3.13/site-packages/pydantic/main.py:463: UserWarning: Pydantic serializer warnings:\n",
      "  PydanticSerializationUnexpectedValue(Expected 9 fields but got 5: Expected `Message` - serialized value may not be as expected [input_value=Message(content='[[ ## an...ields={'refusal': None}), input_type=Message])\n",
      "  PydanticSerializationUnexpectedValue(Expected `StreamingChoices` - serialized value may not be as expected [input_value=Choices(finish_reason='st...: 12340258781511740000}), input_type=Choices])\n",
      "  return self.__pydantic_serializer__.to_python(\n",
      "  4%|‚ñà‚ñä                                          | 2/50 [00:04<01:38,  2.05s/it]\n",
      "Bootstrapped 2 full traces after 2 examples for up to 1 rounds, amounting to 2 attempts.\n",
      "\u001b[92m12:48:56 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "2025-06-25 12:48:56,999 - LiteLLM - INFO - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "2025/06/25 12:48:57 INFO dspy.teleprompt.mipro_optimizer_v2: \n",
      "==> STEP 2: PROPOSE INSTRUCTION CANDIDATES <==\n",
      "2025/06/25 12:48:57 INFO dspy.teleprompt.mipro_optimizer_v2: We will use the few-shot examples from the previous step, a generated dataset summary, a summary of the program code, and a randomly selected prompting tip to propose instructions.\n",
      "\u001b[92m12:48:57 - LiteLLM:INFO\u001b[0m: utils.py:3119 - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.3-70B-Instruct-Turbo; provider = together_ai\n",
      "2025-06-25 12:48:57,027 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.3-70B-Instruct-Turbo; provider = together_ai\n",
      "2025-06-25 12:49:23,693 - httpx - INFO - HTTP Request: POST https://api.together.xyz/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m12:49:23 - LiteLLM:INFO\u001b[0m: utils.py:1215 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-25 12:49:23,698 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m12:49:23 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "2025-06-25 12:49:23,698 - LiteLLM - INFO - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "/Users/ytchen/Documents/experimental/Llama4/env/lib/python3.13/site-packages/pydantic/main.py:463: UserWarning: Pydantic serializer warnings:\n",
      "  PydanticSerializationUnexpectedValue(Expected 9 fields but got 5: Expected `Message` - serialized value may not be as expected [input_value=Message(content=\"[[ ## ob...ields={'refusal': None}), input_type=Message])\n",
      "  PydanticSerializationUnexpectedValue(Expected `StreamingChoices` - serialized value may not be as expected [input_value=Choices(finish_reason='st...': 6806278142079849000}), input_type=Choices])\n",
      "  return self.__pydantic_serializer__.to_python(\n",
      "\u001b[92m12:49:23 - LiteLLM:INFO\u001b[0m: utils.py:3119 - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.3-70B-Instruct-Turbo; provider = together_ai\n",
      "2025-06-25 12:49:23,735 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.3-70B-Instruct-Turbo; provider = together_ai\n",
      "\u001b[92m12:49:23 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "2025-06-25 12:49:23,738 - LiteLLM - INFO - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "2025-06-25 12:49:54,762 - httpx - INFO - HTTP Request: POST https://api.together.xyz/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m12:49:54 - LiteLLM:INFO\u001b[0m: utils.py:1215 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-25 12:49:54,768 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m12:49:54 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "2025-06-25 12:49:54,768 - LiteLLM - INFO - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "/Users/ytchen/Documents/experimental/Llama4/env/lib/python3.13/site-packages/pydantic/main.py:463: UserWarning: Pydantic serializer warnings:\n",
      "  PydanticSerializationUnexpectedValue(Expected 9 fields but got 5: Expected `Message` - serialized value may not be as expected [input_value=Message(content=\"[[ ## ob...ields={'refusal': None}), input_type=Message])\n",
      "  PydanticSerializationUnexpectedValue(Expected `StreamingChoices` - serialized value may not be as expected [input_value=Choices(finish_reason='st...': 5222081746555367000}), input_type=Choices])\n",
      "  return self.__pydantic_serializer__.to_python(\n",
      "\u001b[92m12:49:54 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "2025-06-25 12:49:54,808 - LiteLLM - INFO - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "\u001b[92m12:49:54 - LiteLLM:INFO\u001b[0m: utils.py:3119 - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.3-70B-Instruct-Turbo; provider = together_ai\n",
      "2025-06-25 12:49:54,811 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.3-70B-Instruct-Turbo; provider = together_ai\n",
      "2025-06-25 12:50:02,900 - httpx - INFO - HTTP Request: POST https://api.together.xyz/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m12:50:02 - LiteLLM:INFO\u001b[0m: utils.py:1215 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-25 12:50:02,905 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m12:50:02 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "2025-06-25 12:50:02,905 - LiteLLM - INFO - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "/Users/ytchen/Documents/experimental/Llama4/env/lib/python3.13/site-packages/pydantic/main.py:463: UserWarning: Pydantic serializer warnings:\n",
      "  PydanticSerializationUnexpectedValue(Expected 9 fields but got 5: Expected `Message` - serialized value may not be as expected [input_value=Message(content=\"[[ ## ob...ields={'refusal': None}), input_type=Message])\n",
      "  PydanticSerializationUnexpectedValue(Expected `StreamingChoices` - serialized value may not be as expected [input_value=Choices(finish_reason='st...: 17935999655033027000}), input_type=Choices])\n",
      "  return self.__pydantic_serializer__.to_python(\n",
      "\u001b[92m12:50:02 - LiteLLM:INFO\u001b[0m: utils.py:3119 - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.3-70B-Instruct-Turbo; provider = together_ai\n",
      "2025-06-25 12:50:02,933 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.3-70B-Instruct-Turbo; provider = together_ai\n",
      "\u001b[92m12:50:02 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "2025-06-25 12:50:02,953 - LiteLLM - INFO - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "2025-06-25 12:50:12,681 - httpx - INFO - HTTP Request: POST https://api.together.xyz/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m12:50:12 - LiteLLM:INFO\u001b[0m: utils.py:1215 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-25 12:50:12,686 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m12:50:12 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "2025-06-25 12:50:12,686 - LiteLLM - INFO - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "/Users/ytchen/Documents/experimental/Llama4/env/lib/python3.13/site-packages/pydantic/main.py:463: UserWarning: Pydantic serializer warnings:\n",
      "  PydanticSerializationUnexpectedValue(Expected 9 fields but got 5: Expected `Message` - serialized value may not be as expected [input_value=Message(content=\"[[ ## ob...ields={'refusal': None}), input_type=Message])\n",
      "  PydanticSerializationUnexpectedValue(Expected `StreamingChoices` - serialized value may not be as expected [input_value=Choices(finish_reason='st...': 6025340009801101000}), input_type=Choices])\n",
      "  return self.__pydantic_serializer__.to_python(\n",
      "\u001b[92m12:50:12 - LiteLLM:INFO\u001b[0m: utils.py:3119 - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.3-70B-Instruct-Turbo; provider = together_ai\n",
      "2025-06-25 12:50:12,715 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.3-70B-Instruct-Turbo; provider = together_ai\n",
      "\u001b[92m12:50:12 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "2025-06-25 12:50:12,729 - LiteLLM - INFO - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "2025-06-25 12:50:17,357 - httpx - INFO - HTTP Request: POST https://api.together.xyz/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m12:50:17 - LiteLLM:INFO\u001b[0m: utils.py:1215 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-25 12:50:17,362 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m12:50:17 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "2025-06-25 12:50:17,362 - LiteLLM - INFO - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "/Users/ytchen/Documents/experimental/Llama4/env/lib/python3.13/site-packages/pydantic/main.py:463: UserWarning: Pydantic serializer warnings:\n",
      "  PydanticSerializationUnexpectedValue(Expected 9 fields but got 5: Expected `Message` - serialized value may not be as expected [input_value=Message(content=\"[[ ## ob...ields={'refusal': None}), input_type=Message])\n",
      "  PydanticSerializationUnexpectedValue(Expected `StreamingChoices` - serialized value may not be as expected [input_value=Choices(finish_reason='st...: 11140080501440920000}), input_type=Choices])\n",
      "  return self.__pydantic_serializer__.to_python(\n",
      "\u001b[92m12:50:17 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "2025-06-25 12:50:17,395 - LiteLLM - INFO - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "\u001b[92m12:50:17 - LiteLLM:INFO\u001b[0m: utils.py:3119 - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.3-70B-Instruct-Turbo; provider = together_ai\n",
      "2025-06-25 12:50:17,407 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.3-70B-Instruct-Turbo; provider = together_ai\n",
      "2025-06-25 12:50:20,653 - httpx - INFO - HTTP Request: POST https://api.together.xyz/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m12:50:20 - LiteLLM:INFO\u001b[0m: utils.py:1215 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-25 12:50:20,657 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m12:50:20 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "2025-06-25 12:50:20,658 - LiteLLM - INFO - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "2025/06/25 12:50:20 INFO dspy.teleprompt.mipro_optimizer_v2: \n",
      "Proposing instructions...\n",
      "\n",
      "/Users/ytchen/Documents/experimental/Llama4/env/lib/python3.13/site-packages/pydantic/main.py:463: UserWarning: Pydantic serializer warnings:\n",
      "  PydanticSerializationUnexpectedValue(Expected 9 fields but got 5: Expected `Message` - serialized value may not be as expected [input_value=Message(content='[[ ## su...ields={'refusal': None}), input_type=Message])\n",
      "  PydanticSerializationUnexpectedValue(Expected `StreamingChoices` - serialized value may not be as expected [input_value=Choices(finish_reason='st...d': 282432961601646850}), input_type=Choices])\n",
      "  return self.__pydantic_serializer__.to_python(\n",
      "\u001b[92m12:50:20 - LiteLLM:INFO\u001b[0m: utils.py:3119 - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.3-70B-Instruct-Turbo; provider = together_ai\n",
      "2025-06-25 12:50:20,695 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.3-70B-Instruct-Turbo; provider = together_ai\n",
      "\u001b[92m12:50:20 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "2025-06-25 12:50:20,709 - LiteLLM - INFO - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "2025-06-25 12:50:36,607 - httpx - INFO - HTTP Request: POST https://api.together.xyz/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m12:50:36 - LiteLLM:INFO\u001b[0m: utils.py:1215 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-25 12:50:36,611 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m12:50:36 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "2025-06-25 12:50:36,612 - LiteLLM - INFO - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "/Users/ytchen/Documents/experimental/Llama4/env/lib/python3.13/site-packages/pydantic/main.py:463: UserWarning: Pydantic serializer warnings:\n",
      "  PydanticSerializationUnexpectedValue(Expected 9 fields but got 5: Expected `Message` - serialized value may not be as expected [input_value=Message(content=\"[[ ## pr...ields={'refusal': None}), input_type=Message])\n",
      "  PydanticSerializationUnexpectedValue(Expected `StreamingChoices` - serialized value may not be as expected [input_value=Choices(finish_reason='st...: 18297103055542870000}), input_type=Choices])\n",
      "  return self.__pydantic_serializer__.to_python(\n",
      "\u001b[92m12:50:36 - LiteLLM:INFO\u001b[0m: utils.py:3119 - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.3-70B-Instruct-Turbo; provider = together_ai\n",
      "2025-06-25 12:50:36,641 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.3-70B-Instruct-Turbo; provider = together_ai\n",
      "\u001b[92m12:50:36 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "2025-06-25 12:50:36,649 - LiteLLM - INFO - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "2025-06-25 12:50:43,672 - httpx - INFO - HTTP Request: POST https://api.together.xyz/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m12:50:43 - LiteLLM:INFO\u001b[0m: utils.py:1215 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-25 12:50:43,676 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m12:50:43 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "2025-06-25 12:50:43,677 - LiteLLM - INFO - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "/Users/ytchen/Documents/experimental/Llama4/env/lib/python3.13/site-packages/pydantic/main.py:463: UserWarning: Pydantic serializer warnings:\n",
      "  PydanticSerializationUnexpectedValue(Expected 9 fields but got 5: Expected `Message` - serialized value may not be as expected [input_value=Message(content=\"[[ ## mo...ields={'refusal': None}), input_type=Message])\n",
      "  PydanticSerializationUnexpectedValue(Expected `StreamingChoices` - serialized value may not be as expected [input_value=Choices(finish_reason='st...d': 251396977809090180}), input_type=Choices])\n",
      "  return self.__pydantic_serializer__.to_python(\n",
      "\u001b[92m12:50:43 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "2025-06-25 12:50:43,717 - LiteLLM - INFO - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "\u001b[92m12:50:43 - LiteLLM:INFO\u001b[0m: utils.py:3119 - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.3-70B-Instruct-Turbo; provider = together_ai\n",
      "2025-06-25 12:50:43,719 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.3-70B-Instruct-Turbo; provider = together_ai\n",
      "2025-06-25 12:50:45,620 - httpx - INFO - HTTP Request: POST https://api.together.xyz/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m12:50:45 - LiteLLM:INFO\u001b[0m: utils.py:1215 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-25 12:50:45,624 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m12:50:45 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "2025-06-25 12:50:45,624 - LiteLLM - INFO - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "/Users/ytchen/Documents/experimental/Llama4/env/lib/python3.13/site-packages/pydantic/main.py:463: UserWarning: Pydantic serializer warnings:\n",
      "  PydanticSerializationUnexpectedValue(Expected 9 fields but got 5: Expected `Message` - serialized value may not be as expected [input_value=Message(content='[[ ## pr...ields={'refusal': None}), input_type=Message])\n",
      "  PydanticSerializationUnexpectedValue(Expected `StreamingChoices` - serialized value may not be as expected [input_value=Choices(finish_reason='st...: 14806924662852380000}), input_type=Choices])\n",
      "  return self.__pydantic_serializer__.to_python(\n",
      "\u001b[92m12:50:45 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "2025-06-25 12:50:45,667 - LiteLLM - INFO - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "\u001b[92m12:50:45 - LiteLLM:INFO\u001b[0m: utils.py:3119 - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.3-70B-Instruct-Turbo; provider = together_ai\n",
      "2025-06-25 12:50:45,669 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.3-70B-Instruct-Turbo; provider = together_ai\n",
      "2025-06-25 12:50:47,152 - httpx - INFO - HTTP Request: POST https://api.together.xyz/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m12:50:47 - LiteLLM:INFO\u001b[0m: utils.py:1215 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-25 12:50:47,159 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m12:50:47 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "2025-06-25 12:50:47,159 - LiteLLM - INFO - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "/Users/ytchen/Documents/experimental/Llama4/env/lib/python3.13/site-packages/pydantic/main.py:463: UserWarning: Pydantic serializer warnings:\n",
      "  PydanticSerializationUnexpectedValue(Expected 9 fields but got 5: Expected `Message` - serialized value may not be as expected [input_value=Message(content=\"[[ ## pr...ields={'refusal': None}), input_type=Message])\n",
      "  PydanticSerializationUnexpectedValue(Expected `StreamingChoices` - serialized value may not be as expected [input_value=Choices(finish_reason='st...: 10570611742639237000}), input_type=Choices])\n",
      "  return self.__pydantic_serializer__.to_python(\n",
      "\u001b[92m12:50:47 - LiteLLM:INFO\u001b[0m: utils.py:3119 - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.3-70B-Instruct-Turbo; provider = together_ai\n",
      "2025-06-25 12:50:47,206 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.3-70B-Instruct-Turbo; provider = together_ai\n",
      "\u001b[92m12:50:47 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "2025-06-25 12:50:47,208 - LiteLLM - INFO - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "2025-06-25 12:50:50,738 - httpx - INFO - HTTP Request: POST https://api.together.xyz/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m12:50:50 - LiteLLM:INFO\u001b[0m: utils.py:1215 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-25 12:50:50,742 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m12:50:50 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "2025-06-25 12:50:50,743 - LiteLLM - INFO - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "/Users/ytchen/Documents/experimental/Llama4/env/lib/python3.13/site-packages/pydantic/main.py:463: UserWarning: Pydantic serializer warnings:\n",
      "  PydanticSerializationUnexpectedValue(Expected 9 fields but got 5: Expected `Message` - serialized value may not be as expected [input_value=Message(content=\"[[ ## mo...ields={'refusal': None}), input_type=Message])\n",
      "  PydanticSerializationUnexpectedValue(Expected `StreamingChoices` - serialized value may not be as expected [input_value=Choices(finish_reason='st...: 11076808013778110000}), input_type=Choices])\n",
      "  return self.__pydantic_serializer__.to_python(\n",
      "\u001b[92m12:50:50 - LiteLLM:INFO\u001b[0m: utils.py:3119 - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.3-70B-Instruct-Turbo; provider = together_ai\n",
      "2025-06-25 12:50:50,777 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.3-70B-Instruct-Turbo; provider = together_ai\n",
      "\u001b[92m12:50:50 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "2025-06-25 12:50:50,782 - LiteLLM - INFO - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "2025-06-25 12:50:53,086 - httpx - INFO - HTTP Request: POST https://api.together.xyz/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m12:50:53 - LiteLLM:INFO\u001b[0m: utils.py:1215 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-25 12:50:53,092 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m12:50:53 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "2025-06-25 12:50:53,093 - LiteLLM - INFO - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "/Users/ytchen/Documents/experimental/Llama4/env/lib/python3.13/site-packages/pydantic/main.py:463: UserWarning: Pydantic serializer warnings:\n",
      "  PydanticSerializationUnexpectedValue(Expected 9 fields but got 5: Expected `Message` - serialized value may not be as expected [input_value=Message(content='[[ ## pr...ields={'refusal': None}), input_type=Message])\n",
      "  PydanticSerializationUnexpectedValue(Expected `StreamingChoices` - serialized value may not be as expected [input_value=Choices(finish_reason='st...': 2423184634678072300}), input_type=Choices])\n",
      "  return self.__pydantic_serializer__.to_python(\n",
      "\u001b[92m12:50:53 - LiteLLM:INFO\u001b[0m: utils.py:3119 - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.3-70B-Instruct-Turbo; provider = together_ai\n",
      "2025-06-25 12:50:53,123 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.3-70B-Instruct-Turbo; provider = together_ai\n",
      "\u001b[92m12:50:53 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "2025-06-25 12:50:53,131 - LiteLLM - INFO - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "2025-06-25 12:50:55,436 - httpx - INFO - HTTP Request: POST https://api.together.xyz/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m12:50:55 - LiteLLM:INFO\u001b[0m: utils.py:1215 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-25 12:50:55,440 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m12:50:55 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "2025-06-25 12:50:55,441 - LiteLLM - INFO - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "/Users/ytchen/Documents/experimental/Llama4/env/lib/python3.13/site-packages/pydantic/main.py:463: UserWarning: Pydantic serializer warnings:\n",
      "  PydanticSerializationUnexpectedValue(Expected 9 fields but got 5: Expected `Message` - serialized value may not be as expected [input_value=Message(content=\"[[ ## pr...ields={'refusal': None}), input_type=Message])\n",
      "  PydanticSerializationUnexpectedValue(Expected `StreamingChoices` - serialized value may not be as expected [input_value=Choices(finish_reason='st...': 8025919692465367000}), input_type=Choices])\n",
      "  return self.__pydantic_serializer__.to_python(\n",
      "\u001b[92m12:50:55 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "2025-06-25 12:50:55,480 - LiteLLM - INFO - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "\u001b[92m12:50:55 - LiteLLM:INFO\u001b[0m: utils.py:3119 - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.3-70B-Instruct-Turbo; provider = together_ai\n",
      "2025-06-25 12:50:55,489 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.3-70B-Instruct-Turbo; provider = together_ai\n",
      "2025-06-25 12:50:59,126 - httpx - INFO - HTTP Request: POST https://api.together.xyz/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m12:50:59 - LiteLLM:INFO\u001b[0m: utils.py:1215 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-25 12:50:59,131 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m12:50:59 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "2025-06-25 12:50:59,132 - LiteLLM - INFO - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "/Users/ytchen/Documents/experimental/Llama4/env/lib/python3.13/site-packages/pydantic/main.py:463: UserWarning: Pydantic serializer warnings:\n",
      "  PydanticSerializationUnexpectedValue(Expected 9 fields but got 5: Expected `Message` - serialized value may not be as expected [input_value=Message(content=\"[[ ## mo...ields={'refusal': None}), input_type=Message])\n",
      "  PydanticSerializationUnexpectedValue(Expected `StreamingChoices` - serialized value may not be as expected [input_value=Choices(finish_reason='st...: 16221415461274960000}), input_type=Choices])\n",
      "  return self.__pydantic_serializer__.to_python(\n",
      "\u001b[92m12:50:59 - LiteLLM:INFO\u001b[0m: utils.py:3119 - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.3-70B-Instruct-Turbo; provider = together_ai\n",
      "2025-06-25 12:50:59,168 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.3-70B-Instruct-Turbo; provider = together_ai\n",
      "\u001b[92m12:50:59 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "2025-06-25 12:50:59,169 - LiteLLM - INFO - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "2025-06-25 12:51:00,284 - httpx - INFO - HTTP Request: POST https://api.together.xyz/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m12:51:00 - LiteLLM:INFO\u001b[0m: utils.py:1215 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-25 12:51:00,288 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m12:51:00 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "2025-06-25 12:51:00,289 - LiteLLM - INFO - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "/Users/ytchen/Documents/experimental/Llama4/env/lib/python3.13/site-packages/pydantic/main.py:463: UserWarning: Pydantic serializer warnings:\n",
      "  PydanticSerializationUnexpectedValue(Expected 9 fields but got 5: Expected `Message` - serialized value may not be as expected [input_value=Message(content='[[ ## pr...ields={'refusal': None}), input_type=Message])\n",
      "  PydanticSerializationUnexpectedValue(Expected `StreamingChoices` - serialized value may not be as expected [input_value=Choices(finish_reason='st...': 8485215469864036000}), input_type=Choices])\n",
      "  return self.__pydantic_serializer__.to_python(\n",
      "\u001b[92m12:51:00 - LiteLLM:INFO\u001b[0m: utils.py:3119 - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.3-70B-Instruct-Turbo; provider = together_ai\n",
      "2025-06-25 12:51:00,322 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.3-70B-Instruct-Turbo; provider = together_ai\n",
      "\u001b[92m12:51:00 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "2025-06-25 12:51:00,325 - LiteLLM - INFO - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "2025-06-25 12:51:02,173 - httpx - INFO - HTTP Request: POST https://api.together.xyz/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m12:51:02 - LiteLLM:INFO\u001b[0m: utils.py:1215 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-25 12:51:02,178 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m12:51:02 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "2025-06-25 12:51:02,179 - LiteLLM - INFO - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "/Users/ytchen/Documents/experimental/Llama4/env/lib/python3.13/site-packages/pydantic/main.py:463: UserWarning: Pydantic serializer warnings:\n",
      "  PydanticSerializationUnexpectedValue(Expected 9 fields but got 5: Expected `Message` - serialized value may not be as expected [input_value=Message(content=\"[[ ## pr...ields={'refusal': None}), input_type=Message])\n",
      "  PydanticSerializationUnexpectedValue(Expected `StreamingChoices` - serialized value may not be as expected [input_value=Choices(finish_reason='st...: 16259183412738100000}), input_type=Choices])\n",
      "  return self.__pydantic_serializer__.to_python(\n",
      "\u001b[92m12:51:02 - LiteLLM:INFO\u001b[0m: utils.py:3119 - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.3-70B-Instruct-Turbo; provider = together_ai\n",
      "2025-06-25 12:51:02,218 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.3-70B-Instruct-Turbo; provider = together_ai\n",
      "\u001b[92m12:51:02 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "2025-06-25 12:51:02,225 - LiteLLM - INFO - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "2025-06-25 12:51:05,867 - httpx - INFO - HTTP Request: POST https://api.together.xyz/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m12:51:05 - LiteLLM:INFO\u001b[0m: utils.py:1215 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-25 12:51:05,872 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m12:51:05 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "2025-06-25 12:51:05,873 - LiteLLM - INFO - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "/Users/ytchen/Documents/experimental/Llama4/env/lib/python3.13/site-packages/pydantic/main.py:463: UserWarning: Pydantic serializer warnings:\n",
      "  PydanticSerializationUnexpectedValue(Expected 9 fields but got 5: Expected `Message` - serialized value may not be as expected [input_value=Message(content=\"[[ ## mo...ields={'refusal': None}), input_type=Message])\n",
      "  PydanticSerializationUnexpectedValue(Expected `StreamingChoices` - serialized value may not be as expected [input_value=Choices(finish_reason='st...': 5748266267379407000}), input_type=Choices])\n",
      "  return self.__pydantic_serializer__.to_python(\n",
      "\u001b[92m12:51:05 - LiteLLM:INFO\u001b[0m: utils.py:3119 - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.3-70B-Instruct-Turbo; provider = together_ai\n",
      "2025-06-25 12:51:05,908 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.3-70B-Instruct-Turbo; provider = together_ai\n",
      "\u001b[92m12:51:05 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "2025-06-25 12:51:05,912 - LiteLLM - INFO - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "2025-06-25 12:51:08,019 - httpx - INFO - HTTP Request: POST https://api.together.xyz/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m12:51:08 - LiteLLM:INFO\u001b[0m: utils.py:1215 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-25 12:51:08,024 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m12:51:08 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "2025-06-25 12:51:08,024 - LiteLLM - INFO - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "/Users/ytchen/Documents/experimental/Llama4/env/lib/python3.13/site-packages/pydantic/main.py:463: UserWarning: Pydantic serializer warnings:\n",
      "  PydanticSerializationUnexpectedValue(Expected 9 fields but got 5: Expected `Message` - serialized value may not be as expected [input_value=Message(content='[[ ## pr...ields={'refusal': None}), input_type=Message])\n",
      "  PydanticSerializationUnexpectedValue(Expected `StreamingChoices` - serialized value may not be as expected [input_value=Choices(finish_reason='st...: 15614025542720790000}), input_type=Choices])\n",
      "  return self.__pydantic_serializer__.to_python(\n",
      "\u001b[92m12:51:08 - LiteLLM:INFO\u001b[0m: utils.py:3119 - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.3-70B-Instruct-Turbo; provider = together_ai\n",
      "2025-06-25 12:51:08,065 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.3-70B-Instruct-Turbo; provider = together_ai\n",
      "\u001b[92m12:51:08 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "2025-06-25 12:51:08,062 - LiteLLM - INFO - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "2025-06-25 12:51:10,399 - httpx - INFO - HTTP Request: POST https://api.together.xyz/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m12:51:10 - LiteLLM:INFO\u001b[0m: utils.py:1215 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-25 12:51:10,404 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m12:51:10 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "2025-06-25 12:51:10,405 - LiteLLM - INFO - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "/Users/ytchen/Documents/experimental/Llama4/env/lib/python3.13/site-packages/pydantic/main.py:463: UserWarning: Pydantic serializer warnings:\n",
      "  PydanticSerializationUnexpectedValue(Expected 9 fields but got 5: Expected `Message` - serialized value may not be as expected [input_value=Message(content=\"[[ ## pr...ields={'refusal': None}), input_type=Message])\n",
      "  PydanticSerializationUnexpectedValue(Expected `StreamingChoices` - serialized value may not be as expected [input_value=Choices(finish_reason='st...': 5512816616939162000}), input_type=Choices])\n",
      "  return self.__pydantic_serializer__.to_python(\n",
      "\u001b[92m12:51:10 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "2025-06-25 12:51:10,444 - LiteLLM - INFO - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "\u001b[92m12:51:10 - LiteLLM:INFO\u001b[0m: utils.py:3119 - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.3-70B-Instruct-Turbo; provider = together_ai\n",
      "2025-06-25 12:51:10,448 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.3-70B-Instruct-Turbo; provider = together_ai\n",
      "2025-06-25 12:51:13,574 - httpx - INFO - HTTP Request: POST https://api.together.xyz/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m12:51:13 - LiteLLM:INFO\u001b[0m: utils.py:1215 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-25 12:51:13,579 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m12:51:13 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "2025-06-25 12:51:13,579 - LiteLLM - INFO - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "/Users/ytchen/Documents/experimental/Llama4/env/lib/python3.13/site-packages/pydantic/main.py:463: UserWarning: Pydantic serializer warnings:\n",
      "  PydanticSerializationUnexpectedValue(Expected 9 fields but got 5: Expected `Message` - serialized value may not be as expected [input_value=Message(content=\"[[ ## mo...ields={'refusal': None}), input_type=Message])\n",
      "  PydanticSerializationUnexpectedValue(Expected `StreamingChoices` - serialized value may not be as expected [input_value=Choices(finish_reason='st...: 11915438024612934000}), input_type=Choices])\n",
      "  return self.__pydantic_serializer__.to_python(\n",
      "\u001b[92m12:51:13 - LiteLLM:INFO\u001b[0m: utils.py:3119 - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.3-70B-Instruct-Turbo; provider = together_ai\n",
      "2025-06-25 12:51:13,608 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.3-70B-Instruct-Turbo; provider = together_ai\n",
      "\u001b[92m12:51:13 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "2025-06-25 12:51:13,623 - LiteLLM - INFO - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "2025-06-25 12:51:16,133 - httpx - INFO - HTTP Request: POST https://api.together.xyz/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m12:51:16 - LiteLLM:INFO\u001b[0m: utils.py:1215 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-25 12:51:16,136 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m12:51:16 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "2025-06-25 12:51:16,137 - LiteLLM - INFO - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "\u001b[92m12:51:16 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "2025/06/25 12:51:16 INFO dspy.teleprompt.mipro_optimizer_v2: Proposed Instructions for Predictor 0:\n",
      "\n",
      "2025-06-25 12:51:16,137 - LiteLLM - INFO - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "/Users/ytchen/Documents/experimental/Llama4/env/lib/python3.13/site-packages/pydantic/main.py:463: UserWarning: Pydantic serializer warnings:\n",
      "  PydanticSerializationUnexpectedValue(Expected 9 fields but got 5: Expected `Message` - serialized value may not be as expected [input_value=Message(content='[[ ## pr...ields={'refusal': None}), input_type=Message])\n",
      "  PydanticSerializationUnexpectedValue(Expected `StreamingChoices` - serialized value may not be as expected [input_value=Choices(finish_reason='st...': 4156192253103189500}), input_type=Choices])\n",
      "  return self.__pydantic_serializer__.to_python(\n",
      "2025/06/25 12:51:16 INFO dspy.teleprompt.mipro_optimizer_v2: 0: You are a helpful assistant. Extract and return a json with the following keys and values:\n",
      "- \"urgency\" as one of `high`, `medium`, `low`\n",
      "- \"sentiment\" as one of `negative`, `neutral`, `positive`\n",
      "- \"categories\" Create a dictionary with categories as keys and boolean values (True/False), where the value indicates whether the category is one of the best matching support category tags from: `emergency_repair_services`, `routine_maintenance_requests`, `quality_and_safety_concerns`, `specialized_cleaning_services`, `general_inquiries`, `sustainability_and_environmental_practices`, `training_and_support_requests`, `cleaning_services_scheduling`, `customer_feedback_and_complaints`, `facility_management_issues`\n",
      "Your complete message should be a valid json string that can be read directly and only contain the keys mentioned in the list above. Never enclose it in ```json...```, no newlines, no unnessacary whitespaces.\n",
      "\n",
      "2025/06/25 12:51:16 INFO dspy.teleprompt.mipro_optimizer_v2: 1: You are a customer support specialist for ProCare Facility Solutions, tasked with categorizing and analyzing customer inquiries related to facility management and maintenance services. Extract and return a json with the following keys and values: - \"urgency\" as one of `high`, `medium`, `low` - \"sentiment\" as one of `negative`, `neutral`, `positive` - \"categories\" Create a dictionary with categories as keys and boolean values (True/False), where the value indicates whether the category is one of the best matching support category tags from: `emergency_repair_services`, `routine_maintenance_requests`, `quality_and_safety_concerns`, `specialized_cleaning_services`, `general_inquiries`, `sustainability_and_environmental_practices`, `training_and_support_requests`, `cleaning_services_scheduling`, `customer_feedback_and_complaints`, `facility_management_issues`. Your complete message should be a valid json string that can be read directly and only contain the keys mentioned in the list above.\n",
      "\n",
      "2025/06/25 12:51:16 INFO dspy.teleprompt.mipro_optimizer_v2: 2: Classify the customer's email into categories and assess its sentiment and urgency. Return a json string with \"urgency\" as one of \"high\", \"medium\", \"low\", \"sentiment\" as one of \"negative\", \"neutral\", \"positive\", and \"categories\" as a dictionary with category names as keys and boolean values indicating whether each category is a best match. Ensure the json string is valid, concise, and contains only the specified keys.\n",
      "\n",
      "\u001b[92m12:51:16 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "2025/06/25 12:51:16 INFO dspy.teleprompt.mipro_optimizer_v2: 3: Given a customer inquiry, analyze the content to determine the urgency level as high, medium, or low, and the sentiment as negative, neutral, or positive. Then, categorize the inquiry into relevant support categories such as emergency repair services, routine maintenance requests, quality and safety concerns, specialized cleaning services, general inquiries, sustainability and environmental practices, training and support requests, cleaning services scheduling, customer feedback and complaints, and facility management issues, marking each category as true or false based on relevance. Return the analysis as a json string with the keys \"urgency\", \"sentiment\", and \"categories\", ensuring the json is valid, directly readable, and without unnecessary whitespace or newlines.\n",
      "\n",
      "2025-06-25 12:51:16,162 - LiteLLM - INFO - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "2025/06/25 12:51:16 INFO dspy.teleprompt.mipro_optimizer_v2: 4: Given a customer email, analyze its content to determine the urgency level, sentiment, and relevant support categories, and return a JSON object with the following keys: \"urgency\" (high, medium, low), \"sentiment\" (negative, neutral, positive), and \"categories\" (a dictionary with support category tags as keys and boolean values indicating the best matching categories).\n",
      "\n",
      "2025/06/25 12:51:16 INFO dspy.teleprompt.mipro_optimizer_v2: \n",
      "\n",
      "2025/06/25 12:51:16 INFO dspy.teleprompt.mipro_optimizer_v2: ==> STEP 3: FINDING OPTIMAL PROMPT PARAMETERS <==\n",
      "2025/06/25 12:51:16 INFO dspy.teleprompt.mipro_optimizer_v2: We will evaluate the program over a series of trials with different combinations of instructions and few-shot examples to find the optimal combination using Bayesian Optimization.\n",
      "\n",
      "2025/06/25 12:51:16 INFO dspy.teleprompt.mipro_optimizer_v2: == Trial 1 / 7 - Full Evaluation of Default Program ==\n",
      "\u001b[92m12:51:16 - LiteLLM:INFO\u001b[0m: utils.py:3119 - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.3-70B-Instruct-Turbo; provider = together_ai\n",
      "2025-06-25 12:51:16,198 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.3-70B-Instruct-Turbo; provider = together_ai\n",
      "\u001b[92m12:51:16 - LiteLLM:INFO\u001b[0m: utils.py:3119 - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.3-70B-Instruct-Turbo; provider = together_ai\n",
      "2025-06-25 12:51:16,208 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.3-70B-Instruct-Turbo; provider = together_ai\n",
      "\u001b[92m12:51:16 - LiteLLM:INFO\u001b[0m: utils.py:3119 - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.3-70B-Instruct-Turbo; provider = together_ai\n",
      "2025-06-25 12:51:16,228 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.3-70B-Instruct-Turbo; provider = together_ai\n",
      "\u001b[92m12:51:16 - LiteLLM:INFO\u001b[0m: utils.py:3119 - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.3-70B-Instruct-Turbo; provider = together_ai\n",
      "2025-06-25 12:51:16,261 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.3-70B-Instruct-Turbo; provider = together_ai\n",
      "\u001b[92m12:51:16 - LiteLLM:INFO\u001b[0m: utils.py:3119 - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.3-70B-Instruct-Turbo; provider = together_ai\n",
      "2025-06-25 12:51:16,267 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.3-70B-Instruct-Turbo; provider = together_ai\n",
      "\u001b[92m12:51:16 - LiteLLM:INFO\u001b[0m: utils.py:3119 - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.3-70B-Instruct-Turbo; provider = together_ai\n",
      "2025-06-25 12:51:16,287 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.3-70B-Instruct-Turbo; provider = together_ai\n",
      "\u001b[92m12:51:16 - LiteLLM:INFO\u001b[0m: utils.py:3119 - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.3-70B-Instruct-Turbo; provider = together_ai\n",
      "2025-06-25 12:51:16,294 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.3-70B-Instruct-Turbo; provider = together_ai\n",
      "\u001b[92m12:51:16 - LiteLLM:INFO\u001b[0m: utils.py:3119 - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.3-70B-Instruct-Turbo; provider = together_ai\n",
      "2025-06-25 12:51:16,299 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.3-70B-Instruct-Turbo; provider = together_ai\n",
      "\u001b[92m12:51:16 - LiteLLM:INFO\u001b[0m: utils.py:3119 - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.3-70B-Instruct-Turbo; provider = together_ai\n",
      "2025-06-25 12:51:16,319 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.3-70B-Instruct-Turbo; provider = together_ai\n",
      "\u001b[92m12:51:16 - LiteLLM:INFO\u001b[0m: utils.py:3119 - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.3-70B-Instruct-Turbo; provider = together_ai\n",
      "2025-06-25 12:51:16,330 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.3-70B-Instruct-Turbo; provider = together_ai\n",
      "\u001b[92m12:51:16 - LiteLLM:INFO\u001b[0m: utils.py:3119 - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.3-70B-Instruct-Turbo; provider = together_ai\n",
      "2025-06-25 12:51:16,342 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.3-70B-Instruct-Turbo; provider = together_ai\n",
      "\u001b[92m12:51:16 - LiteLLM:INFO\u001b[0m: utils.py:3119 - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.3-70B-Instruct-Turbo; provider = together_ai\n",
      "2025-06-25 12:51:16,352 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.3-70B-Instruct-Turbo; provider = together_ai\n",
      "\u001b[92m12:51:16 - LiteLLM:INFO\u001b[0m: utils.py:3119 - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.3-70B-Instruct-Turbo; provider = together_ai\n",
      "2025-06-25 12:51:16,375 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.3-70B-Instruct-Turbo; provider = together_ai\n",
      "\u001b[92m12:51:16 - LiteLLM:INFO\u001b[0m: utils.py:3119 - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.3-70B-Instruct-Turbo; provider = together_ai\n",
      "2025-06-25 12:51:16,395 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.3-70B-Instruct-Turbo; provider = together_ai\n",
      "\u001b[92m12:51:16 - LiteLLM:INFO\u001b[0m: utils.py:3119 - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.3-70B-Instruct-Turbo; provider = together_ai\n",
      "2025-06-25 12:51:16,407 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.3-70B-Instruct-Turbo; provider = together_ai\n",
      "\u001b[92m12:51:16 - LiteLLM:INFO\u001b[0m: utils.py:3119 - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.3-70B-Instruct-Turbo; provider = together_ai\n",
      "2025-06-25 12:51:16,418 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.3-70B-Instruct-Turbo; provider = together_ai\n",
      "\u001b[92m12:51:16 - LiteLLM:INFO\u001b[0m: utils.py:3119 - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.3-70B-Instruct-Turbo; provider = together_ai\n",
      "2025-06-25 12:51:16,431 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.3-70B-Instruct-Turbo; provider = together_ai\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[92m12:51:16 - LiteLLM:INFO\u001b[0m: utils.py:3119 - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.3-70B-Instruct-Turbo; provider = together_ai\n",
      "2025-06-25 12:51:16,441 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.3-70B-Instruct-Turbo; provider = together_ai\n",
      "2025-06-25 12:51:17,136 - httpx - INFO - HTTP Request: POST https://api.together.xyz/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m12:51:17 - LiteLLM:INFO\u001b[0m: utils.py:1215 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-25 12:51:17,140 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m12:51:17 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "2025-06-25 12:51:17,140 - LiteLLM - INFO - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "/Users/ytchen/Documents/experimental/Llama4/env/lib/python3.13/site-packages/pydantic/main.py:463: UserWarning: Pydantic serializer warnings:\n",
      "  PydanticSerializationUnexpectedValue(Expected 9 fields but got 5: Expected `Message` - serialized value may not be as expected [input_value=Message(content='[[ ## an...ields={'refusal': None}), input_type=Message])\n",
      "  PydanticSerializationUnexpectedValue(Expected `StreamingChoices` - serialized value may not be as expected [input_value=Choices(finish_reason='st...: 14558640827571315000}), input_type=Choices])\n",
      "  return self.__pydantic_serializer__.to_python(\n",
      "Average Metric: 0.63 / 1 (63.3%):   0%|                  | 0/50 [00:00<?, ?it/s]\u001b[92m12:51:17 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "Average Metric: 0.63 / 1 (63.3%):   2%|‚ñè         | 1/50 [00:00<00:35,  1.39it/s]2025-06-25 12:51:17,170 - LiteLLM - INFO - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "\u001b[92m12:51:17 - LiteLLM:INFO\u001b[0m: utils.py:3119 - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.3-70B-Instruct-Turbo; provider = together_ai\n",
      "2025-06-25 12:51:17,193 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.3-70B-Instruct-Turbo; provider = together_ai\n",
      "2025-06-25 12:51:17,465 - httpx - INFO - HTTP Request: POST https://api.together.xyz/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-06-25 12:51:17,467 - httpx - INFO - HTTP Request: POST https://api.together.xyz/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m12:51:17 - LiteLLM:INFO\u001b[0m: utils.py:1215 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-25 12:51:17,472 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m12:51:17 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "2025-06-25 12:51:17,473 - LiteLLM - INFO - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "/Users/ytchen/Documents/experimental/Llama4/env/lib/python3.13/site-packages/pydantic/main.py:463: UserWarning: Pydantic serializer warnings:\n",
      "  PydanticSerializationUnexpectedValue(Expected 9 fields but got 5: Expected `Message` - serialized value may not be as expected [input_value=Message(content='[[ ## an...ields={'refusal': None}), input_type=Message])\n",
      "  PydanticSerializationUnexpectedValue(Expected `StreamingChoices` - serialized value may not be as expected [input_value=Choices(finish_reason='st...: 14310728755686250000}), input_type=Choices])\n",
      "  return self.__pydantic_serializer__.to_python(\n",
      "Average Metric: 1.60 / 2 (80.0%):   4%|‚ñç         | 2/50 [00:01<00:23,  2.05it/s]\u001b[92m12:51:17 - LiteLLM:INFO\u001b[0m: utils.py:3119 - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.3-70B-Instruct-Turbo; provider = together_ai\n",
      "2025-06-25 12:51:17,494 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.3-70B-Instruct-Turbo; provider = together_ai\n",
      "\u001b[92m12:51:17 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "2025-06-25 12:51:17,506 - LiteLLM - INFO - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "\u001b[92m12:51:17 - LiteLLM:INFO\u001b[0m: utils.py:1215 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-25 12:51:17,531 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m12:51:17 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "2025-06-25 12:51:17,531 - LiteLLM - INFO - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "Average Metric: 2.57 / 3 (85.6%):   4%|‚ñç         | 2/50 [00:01<00:23,  2.05it/s]/Users/ytchen/Documents/experimental/Llama4/env/lib/python3.13/site-packages/pydantic/main.py:463: UserWarning: Pydantic serializer warnings:\n",
      "  PydanticSerializationUnexpectedValue(Expected 9 fields but got 5: Expected `Message` - serialized value may not be as expected [input_value=Message(content='[[ ## an...ields={'refusal': None}), input_type=Message])\n",
      "  PydanticSerializationUnexpectedValue(Expected `StreamingChoices` - serialized value may not be as expected [input_value=Choices(finish_reason='st...': 3966844615658441700}), input_type=Choices])\n",
      "  return self.__pydantic_serializer__.to_python(\n",
      "\u001b[92m12:51:17 - LiteLLM:INFO\u001b[0m: utils.py:3119 - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.3-70B-Instruct-Turbo; provider = together_ai\n",
      "2025-06-25 12:51:17,547 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.3-70B-Instruct-Turbo; provider = together_ai\n",
      "\u001b[92m12:51:17 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "2025-06-25 12:51:17,549 - LiteLLM - INFO - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "2025-06-25 12:51:17,614 - httpx - INFO - HTTP Request: POST https://api.together.xyz/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m12:51:17 - LiteLLM:INFO\u001b[0m: utils.py:1215 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-25 12:51:17,615 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m12:51:17 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "2025-06-25 12:51:17,615 - LiteLLM - INFO - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "/Users/ytchen/Documents/experimental/Llama4/env/lib/python3.13/site-packages/pydantic/main.py:463: UserWarning: Pydantic serializer warnings:\n",
      "  PydanticSerializationUnexpectedValue(Expected 9 fields but got 5: Expected `Message` - serialized value may not be as expected [input_value=Message(content='[[ ## an...ields={'refusal': None}), input_type=Message])\n",
      "  PydanticSerializationUnexpectedValue(Expected `StreamingChoices` - serialized value may not be as expected [input_value=Choices(finish_reason='st...: 14269879928813607000}), input_type=Choices])\n",
      "  return self.__pydantic_serializer__.to_python(\n",
      "Average Metric: 3.20 / 4 (80.0%):   6%|‚ñå         | 3/50 [00:01<00:22,  2.05it/s]\u001b[92m12:51:17 - LiteLLM:INFO\u001b[0m: utils.py:3119 - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.3-70B-Instruct-Turbo; provider = together_ai\n",
      "2025-06-25 12:51:17,629 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.3-70B-Instruct-Turbo; provider = together_ai\n",
      "\u001b[92m12:51:17 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "2025-06-25 12:51:17,637 - LiteLLM - INFO - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "2025-06-25 12:51:17,645 - httpx - INFO - HTTP Request: POST https://api.together.xyz/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Average Metric: 3.20 / 4 (80.0%):   8%|‚ñä         | 4/50 [00:01<00:10,  4.41it/s]2025-06-25 12:51:17,645 - httpx - INFO - HTTP Request: POST https://api.together.xyz/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m12:51:17 - LiteLLM:INFO\u001b[0m: utils.py:1215 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-25 12:51:17,646 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m12:51:17 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "2025-06-25 12:51:17,646 - LiteLLM - INFO - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "/Users/ytchen/Documents/experimental/Llama4/env/lib/python3.13/site-packages/pydantic/main.py:463: UserWarning: Pydantic serializer warnings:\n",
      "  PydanticSerializationUnexpectedValue(Expected 9 fields but got 5: Expected `Message` - serialized value may not be as expected [input_value=Message(content='[[ ## an...ields={'refusal': None}), input_type=Message])\n",
      "  PydanticSerializationUnexpectedValue(Expected `StreamingChoices` - serialized value may not be as expected [input_value=Choices(finish_reason='st...': 6505595386736293000}), input_type=Choices])\n",
      "  return self.__pydantic_serializer__.to_python(\n",
      "Average Metric: 3.87 / 5 (77.3%):   8%|‚ñä         | 4/50 [00:01<00:10,  4.41it/s]\u001b[92m12:51:17 - LiteLLM:INFO\u001b[0m: utils.py:3119 - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.3-70B-Instruct-Turbo; provider = together_ai\n",
      "2025-06-25 12:51:17,660 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.3-70B-Instruct-Turbo; provider = together_ai\n",
      "\u001b[92m12:51:17 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "2025-06-25 12:51:17,666 - LiteLLM - INFO - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "\u001b[92m12:51:17 - LiteLLM:INFO\u001b[0m: utils.py:1215 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-25 12:51:17,672 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m12:51:17 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "2025-06-25 12:51:17,672 - LiteLLM - INFO - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "/Users/ytchen/Documents/experimental/Llama4/env/lib/python3.13/site-packages/pydantic/main.py:463: UserWarning: Pydantic serializer warnings:\n",
      "  PydanticSerializationUnexpectedValue(Expected 9 fields but got 5: Expected `Message` - serialized value may not be as expected [input_value=Message(content='[[ ## an...ields={'refusal': None}), input_type=Message])\n",
      "  PydanticSerializationUnexpectedValue(Expected `StreamingChoices` - serialized value may not be as expected [input_value=Choices(finish_reason='st...: 14229887159081677000}), input_type=Choices])\n",
      "  return self.__pydantic_serializer__.to_python(\n",
      "\u001b[92m12:51:17 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "2025-06-25 12:51:17,679 - LiteLLM - INFO - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "Average Metric: 4.87 / 6 (81.1%):  10%|‚ñà         | 5/50 [00:01<00:10,  4.41it/s]\u001b[92m12:51:17 - LiteLLM:INFO\u001b[0m: utils.py:3119 - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.3-70B-Instruct-Turbo; provider = together_ai\n",
      "2025-06-25 12:51:17,686 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.3-70B-Instruct-Turbo; provider = together_ai\n",
      "\u001b[92m12:51:17 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "2025-06-25 12:51:17,693 - LiteLLM - INFO - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "2025-06-25 12:51:17,699 - httpx - INFO - HTTP Request: POST https://api.together.xyz/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-06-25 12:51:17,702 - httpx - INFO - HTTP Request: POST https://api.together.xyz/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m12:51:17 - LiteLLM:INFO\u001b[0m: utils.py:1215 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-25 12:51:17,703 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m12:51:17 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "2025-06-25 12:51:17,704 - LiteLLM - INFO - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "/Users/ytchen/Documents/experimental/Llama4/env/lib/python3.13/site-packages/pydantic/main.py:463: UserWarning: Pydantic serializer warnings:\n",
      "  PydanticSerializationUnexpectedValue(Expected 9 fields but got 5: Expected `Message` - serialized value may not be as expected [input_value=Message(content='[[ ## an...ields={'refusal': None}), input_type=Message])\n",
      "  PydanticSerializationUnexpectedValue(Expected `StreamingChoices` - serialized value may not be as expected [input_value=Choices(finish_reason='st...: 16681627617509827000}), input_type=Choices])\n",
      "  return self.__pydantic_serializer__.to_python(\n",
      "Average Metric: 5.80 / 7 (82.9%):  12%|‚ñà‚ñè        | 6/50 [00:01<00:09,  4.41it/s]\u001b[92m12:51:17 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "2025-06-25 12:51:17,717 - LiteLLM - INFO - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "\u001b[92m12:51:17 - LiteLLM:INFO\u001b[0m: utils.py:3119 - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.3-70B-Instruct-Turbo; provider = together_ai\n",
      "2025-06-25 12:51:17,723 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.3-70B-Instruct-Turbo; provider = together_ai\n",
      "\u001b[92m12:51:17 - LiteLLM:INFO\u001b[0m: utils.py:1215 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-25 12:51:17,731 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m12:51:17 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "2025-06-25 12:51:17,731 - LiteLLM - INFO - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "/Users/ytchen/Documents/experimental/Llama4/env/lib/python3.13/site-packages/pydantic/main.py:463: UserWarning: Pydantic serializer warnings:\n",
      "  PydanticSerializationUnexpectedValue(Expected 9 fields but got 5: Expected `Message` - serialized value may not be as expected [input_value=Message(content='[[ ## an...ields={'refusal': None}), input_type=Message])\n",
      "  PydanticSerializationUnexpectedValue(Expected `StreamingChoices` - serialized value may not be as expected [input_value=Choices(finish_reason='st...': 6325843651465035000}), input_type=Choices])\n",
      "  return self.__pydantic_serializer__.to_python(\n",
      "\u001b[92m12:51:17 - LiteLLM:INFO\u001b[0m: utils.py:3119 - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.3-70B-Instruct-Turbo; provider = together_ai\n",
      "2025-06-25 12:51:17,745 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.3-70B-Instruct-Turbo; provider = together_ai\n",
      "Average Metric: 6.70 / 8 (83.8%):  16%|‚ñà‚ñå        | 8/50 [00:01<00:04,  9.90it/s]\u001b[92m12:51:17 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "2025-06-25 12:51:17,753 - LiteLLM - INFO - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "2025-06-25 12:51:17,762 - httpx - INFO - HTTP Request: POST https://api.together.xyz/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m12:51:17 - LiteLLM:INFO\u001b[0m: utils.py:1215 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-25 12:51:17,763 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m12:51:17 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "2025-06-25 12:51:17,763 - LiteLLM - INFO - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "/Users/ytchen/Documents/experimental/Llama4/env/lib/python3.13/site-packages/pydantic/main.py:463: UserWarning: Pydantic serializer warnings:\n",
      "  PydanticSerializationUnexpectedValue(Expected 9 fields but got 5: Expected `Message` - serialized value may not be as expected [input_value=Message(content='[[ ## an...ields={'refusal': None}), input_type=Message])\n",
      "  PydanticSerializationUnexpectedValue(Expected `StreamingChoices` - serialized value may not be as expected [input_value=Choices(finish_reason='st...: 10578211297533510000}), input_type=Choices])\n",
      "  return self.__pydantic_serializer__.to_python(\n",
      "Average Metric: 7.30 / 9 (81.1%):  16%|‚ñà‚ñå        | 8/50 [00:01<00:04,  9.90it/s]\u001b[92m12:51:17 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "2025-06-25 12:51:17,776 - LiteLLM - INFO - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "\u001b[92m12:51:17 - LiteLLM:INFO\u001b[0m: utils.py:3119 - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.3-70B-Instruct-Turbo; provider = together_ai\n",
      "2025-06-25 12:51:17,783 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.3-70B-Instruct-Turbo; provider = together_ai\n",
      "2025-06-25 12:51:17,797 - httpx - INFO - HTTP Request: POST https://api.together.xyz/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m12:51:17 - LiteLLM:INFO\u001b[0m: utils.py:1215 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-25 12:51:17,798 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m12:51:17 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "2025-06-25 12:51:17,798 - LiteLLM - INFO - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "/Users/ytchen/Documents/experimental/Llama4/env/lib/python3.13/site-packages/pydantic/main.py:463: UserWarning: Pydantic serializer warnings:\n",
      "  PydanticSerializationUnexpectedValue(Expected 9 fields but got 5: Expected `Message` - serialized value may not be as expected [input_value=Message(content='[[ ## an...ields={'refusal': None}), input_type=Message])\n",
      "  PydanticSerializationUnexpectedValue(Expected `StreamingChoices` - serialized value may not be as expected [input_value=Choices(finish_reason='st...': 7238307331489192000}), input_type=Choices])\n",
      "  return self.__pydantic_serializer__.to_python(\n",
      "Average Metric: 8.27 / 10 (82.7%):  18%|‚ñà‚ñå       | 9/50 [00:01<00:04,  9.90it/s]\u001b[92m12:51:17 - LiteLLM:INFO\u001b[0m: utils.py:3119 - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.3-70B-Instruct-Turbo; provider = together_ai\n",
      "2025-06-25 12:51:17,812 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.3-70B-Instruct-Turbo; provider = together_ai\n",
      "\u001b[92m12:51:17 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "2025-06-25 12:51:17,819 - LiteLLM - INFO - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "2025-06-25 12:51:17,826 - httpx - INFO - HTTP Request: POST https://api.together.xyz/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "\u001b[92m12:51:17 - LiteLLM:INFO\u001b[0m: utils.py:3119 - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.3-70B-Instruct-Turbo; provider = together_ai\n",
      "2025-06-25 12:51:17,842 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.3-70B-Instruct-Turbo; provider = together_ai\n",
      "2025-06-25 12:51:17,860 - httpx - INFO - HTTP Request: POST https://api.together.xyz/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "\u001b[92m12:51:17 - LiteLLM:INFO\u001b[0m: utils.py:3119 - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.3-70B-Instruct-Turbo; provider = together_ai\n",
      "2025-06-25 12:51:17,867 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.3-70B-Instruct-Turbo; provider = together_ai\n",
      "2025-06-25 12:51:17,874 - httpx - INFO - HTTP Request: POST https://api.together.xyz/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-25 12:51:17,874 - httpx - INFO - HTTP Request: POST https://api.together.xyz/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "\u001b[92m12:51:17 - LiteLLM:INFO\u001b[0m: utils.py:3119 - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.3-70B-Instruct-Turbo; provider = together_ai\n",
      "2025-06-25 12:51:17,882 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.3-70B-Instruct-Turbo; provider = together_ai\n",
      "\u001b[92m12:51:17 - LiteLLM:INFO\u001b[0m: utils.py:3119 - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.3-70B-Instruct-Turbo; provider = together_ai\n",
      "2025-06-25 12:51:17,896 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.3-70B-Instruct-Turbo; provider = together_ai\n",
      "2025-06-25 12:51:17,906 - httpx - INFO - HTTP Request: POST https://api.together.xyz/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "\u001b[92m12:51:17 - LiteLLM:INFO\u001b[0m: utils.py:3119 - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.3-70B-Instruct-Turbo; provider = together_ai\n",
      "2025-06-25 12:51:17,913 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.3-70B-Instruct-Turbo; provider = together_ai\n",
      "2025-06-25 12:51:17,960 - httpx - INFO - HTTP Request: POST https://api.together.xyz/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "\u001b[92m12:51:17 - LiteLLM:INFO\u001b[0m: utils.py:3119 - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.3-70B-Instruct-Turbo; provider = together_ai\n",
      "2025-06-25 12:51:17,984 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.3-70B-Instruct-Turbo; provider = together_ai\n",
      "2025-06-25 12:51:17,992 - httpx - INFO - HTTP Request: POST https://api.together.xyz/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "\u001b[92m12:51:17 - LiteLLM:INFO\u001b[0m: utils.py:3119 - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.3-70B-Instruct-Turbo; provider = together_ai\n",
      "2025-06-25 12:51:17,999 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.3-70B-Instruct-Turbo; provider = together_ai\n",
      "2025-06-25 12:51:18,008 - httpx - INFO - HTTP Request: POST https://api.together.xyz/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "\u001b[92m12:51:18 - LiteLLM:INFO\u001b[0m: utils.py:3119 - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.3-70B-Instruct-Turbo; provider = together_ai\n",
      "2025-06-25 12:51:18,022 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.3-70B-Instruct-Turbo; provider = together_ai\n",
      "2025-06-25 12:51:18,039 - httpx - INFO - HTTP Request: POST https://api.together.xyz/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "\u001b[92m12:51:18 - LiteLLM:INFO\u001b[0m: utils.py:3119 - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.3-70B-Instruct-Turbo; provider = together_ai\n",
      "2025-06-25 12:51:18,046 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.3-70B-Instruct-Turbo; provider = together_ai\n",
      "2025-06-25 12:51:18,113 - httpx - INFO - HTTP Request: POST https://api.together.xyz/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-25 12:51:18,113 - httpx - INFO - HTTP Request: POST https://api.together.xyz/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-25 12:51:18,123 - httpx - INFO - HTTP Request: POST https://api.together.xyz/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-25 12:51:18,124 - httpx - INFO - HTTP Request: POST https://api.together.xyz/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "\u001b[92m12:51:18 - LiteLLM:INFO\u001b[0m: utils.py:3119 - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.3-70B-Instruct-Turbo; provider = together_ai\n",
      "2025-06-25 12:51:18,139 - httpx - INFO - HTTP Request: POST https://api.together.xyz/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "\u001b[92m12:51:18 - LiteLLM:INFO\u001b[0m: utils.py:3119 - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.3-70B-Instruct-Turbo; provider = together_ai\n",
      "2025-06-25 12:51:18,136 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.3-70B-Instruct-Turbo; provider = together_ai\n",
      "2025-06-25 12:51:18,147 - httpx - INFO - HTTP Request: POST https://api.together.xyz/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-25 12:51:18,146 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.3-70B-Instruct-Turbo; provider = together_ai\n",
      "\u001b[92m12:51:18 - LiteLLM:INFO\u001b[0m: utils.py:3119 - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.3-70B-Instruct-Turbo; provider = together_ai\n",
      "2025-06-25 12:51:18,166 - httpx - INFO - HTTP Request: POST https://api.together.xyz/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-25 12:51:18,166 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.3-70B-Instruct-Turbo; provider = together_ai\n",
      "\u001b[92m12:51:18 - LiteLLM:INFO\u001b[0m: utils.py:3119 - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.3-70B-Instruct-Turbo; provider = together_ai\n",
      "2025-06-25 12:51:18,195 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.3-70B-Instruct-Turbo; provider = together_ai\n",
      "\u001b[92m12:51:18 - LiteLLM:INFO\u001b[0m: utils.py:3119 - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.3-70B-Instruct-Turbo; provider = together_ai\n",
      "2025-06-25 12:51:18,214 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.3-70B-Instruct-Turbo; provider = together_ai\n",
      "2025-06-25 12:51:18,241 - httpx - INFO - HTTP Request: POST https://api.together.xyz/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-06-25 12:51:18,248 - httpx - INFO - HTTP Request: POST https://api.together.xyz/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-06-25 12:51:18,249 - httpx - INFO - HTTP Request: POST https://api.together.xyz/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m12:51:18 - LiteLLM:INFO\u001b[0m: utils.py:3119 - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.3-70B-Instruct-Turbo; provider = together_ai\n",
      "2025-06-25 12:51:18,268 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.3-70B-Instruct-Turbo; provider = together_ai\n",
      "\u001b[92m12:51:18 - LiteLLM:INFO\u001b[0m: utils.py:3119 - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.3-70B-Instruct-Turbo; provider = together_ai\n",
      "2025-06-25 12:51:18,290 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.3-70B-Instruct-Turbo; provider = together_ai\n",
      "\u001b[92m12:51:18 - LiteLLM:INFO\u001b[0m: utils.py:1215 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-25 12:51:18,297 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m12:51:18 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "2025-06-25 12:51:18,298 - LiteLLM - INFO - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "/Users/ytchen/Documents/experimental/Llama4/env/lib/python3.13/site-packages/pydantic/main.py:463: UserWarning: Pydantic serializer warnings:\n",
      "  PydanticSerializationUnexpectedValue(Expected 9 fields but got 5: Expected `Message` - serialized value may not be as expected [input_value=Message(content='[[ ## an...ields={'refusal': None}), input_type=Message])\n",
      "  PydanticSerializationUnexpectedValue(Expected `StreamingChoices` - serialized value may not be as expected [input_value=Choices(finish_reason='st...: 11677464396240548000}), input_type=Choices])\n",
      "  return self.__pydantic_serializer__.to_python(\n",
      "\u001b[92m12:51:18 - LiteLLM:INFO\u001b[0m: utils.py:3119 - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.3-70B-Instruct-Turbo; provider = together_ai\n",
      "2025-06-25 12:51:18,311 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.3-70B-Instruct-Turbo; provider = together_ai\n",
      "\u001b[92m12:51:18 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "2025-06-25 12:51:18,317 - LiteLLM - INFO - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "Average Metric: 8.90 / 11 (80.9%):  22%|‚ñà‚ñä      | 11/50 [00:01<00:05,  7.25it/s]\u001b[92m12:51:18 - LiteLLM:INFO\u001b[0m: utils.py:1215 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-25 12:51:18,326 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m12:51:18 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "2025-06-25 12:51:18,326 - LiteLLM - INFO - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "/Users/ytchen/Documents/experimental/Llama4/env/lib/python3.13/site-packages/pydantic/main.py:463: UserWarning: Pydantic serializer warnings:\n",
      "  PydanticSerializationUnexpectedValue(Expected 9 fields but got 5: Expected `Message` - serialized value may not be as expected [input_value=Message(content='[[ ## an...ields={'refusal': None}), input_type=Message])\n",
      "  PydanticSerializationUnexpectedValue(Expected `StreamingChoices` - serialized value may not be as expected [input_value=Choices(finish_reason='st...: 10415309719982305000}), input_type=Choices])\n",
      "  return self.__pydantic_serializer__.to_python(\n",
      "\u001b[92m12:51:18 - LiteLLM:INFO\u001b[0m: utils.py:3119 - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.3-70B-Instruct-Turbo; provider = together_ai\n",
      "2025-06-25 12:51:18,340 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.3-70B-Instruct-Turbo; provider = together_ai\n",
      "Average Metric: 9.83 / 12 (81.9%):  22%|‚ñà‚ñä      | 11/50 [00:01<00:05,  7.25it/s]\u001b[92m12:51:18 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "2025-06-25 12:51:18,347 - LiteLLM - INFO - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "2025-06-25 12:51:18,354 - httpx - INFO - HTTP Request: POST https://api.together.xyz/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m12:51:18 - LiteLLM:INFO\u001b[0m: utils.py:1215 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-25 12:51:18,355 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m12:51:18 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "2025-06-25 12:51:18,355 - LiteLLM - INFO - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "Average Metric: 10.50 / 13 (80.8%):  24%|‚ñà‚ñã     | 12/50 [00:01<00:05,  7.25it/s]/Users/ytchen/Documents/experimental/Llama4/env/lib/python3.13/site-packages/pydantic/main.py:463: UserWarning: Pydantic serializer warnings:\n",
      "  PydanticSerializationUnexpectedValue(Expected 9 fields but got 5: Expected `Message` - serialized value may not be as expected [input_value=Message(content='[[ ## an...ields={'refusal': None}), input_type=Message])\n",
      "  PydanticSerializationUnexpectedValue(Expected `StreamingChoices` - serialized value may not be as expected [input_value=Choices(finish_reason='st...: 15402050854531120000}), input_type=Choices])\n",
      "  return self.__pydantic_serializer__.to_python(\n",
      "\u001b[92m12:51:18 - LiteLLM:INFO\u001b[0m: utils.py:3119 - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.3-70B-Instruct-Turbo; provider = together_ai\n",
      "2025-06-25 12:51:18,368 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.3-70B-Instruct-Turbo; provider = together_ai\n",
      "\u001b[92m12:51:18 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "2025-06-25 12:51:18,369 - LiteLLM - INFO - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "\u001b[92m12:51:18 - LiteLLM:INFO\u001b[0m: utils.py:1215 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-25 12:51:18,384 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m12:51:18 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "2025-06-25 12:51:18,384 - LiteLLM - INFO - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "Average Metric: 11.43 / 14 (81.7%):  26%|‚ñà‚ñä     | 13/50 [00:01<00:05,  7.25it/s]\u001b[92m12:51:18 - LiteLLM:INFO\u001b[0m: utils.py:3119 - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.3-70B-Instruct-Turbo; provider = together_ai\n",
      "2025-06-25 12:51:18,391 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.3-70B-Instruct-Turbo; provider = together_ai\n",
      "/Users/ytchen/Documents/experimental/Llama4/env/lib/python3.13/site-packages/pydantic/main.py:463: UserWarning: Pydantic serializer warnings:\n",
      "  PydanticSerializationUnexpectedValue(Expected 9 fields but got 5: Expected `Message` - serialized value may not be as expected [input_value=Message(content='[[ ## an...ields={'refusal': None}), input_type=Message])\n",
      "  PydanticSerializationUnexpectedValue(Expected `StreamingChoices` - serialized value may not be as expected [input_value=Choices(finish_reason='st...: 14125520216790925000}), input_type=Choices])\n",
      "  return self.__pydantic_serializer__.to_python(\n",
      "\u001b[92m12:51:18 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "2025-06-25 12:51:18,407 - LiteLLM - INFO - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "2025-06-25 12:51:18,478 - httpx - INFO - HTTP Request: POST https://api.together.xyz/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "\u001b[92m12:51:18 - LiteLLM:INFO\u001b[0m: utils.py:3119 - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.3-70B-Instruct-Turbo; provider = together_ai\n",
      "2025-06-25 12:51:18,493 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.3-70B-Instruct-Turbo; provider = together_ai\n",
      "2025-06-25 12:51:18,502 - httpx - INFO - HTTP Request: POST https://api.together.xyz/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m12:51:18 - LiteLLM:INFO\u001b[0m: utils.py:1215 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-25 12:51:18,503 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m12:51:18 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "2025-06-25 12:51:18,503 - LiteLLM - INFO - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "/Users/ytchen/Documents/experimental/Llama4/env/lib/python3.13/site-packages/pydantic/main.py:463: UserWarning: Pydantic serializer warnings:\n",
      "  PydanticSerializationUnexpectedValue(Expected 9 fields but got 5: Expected `Message` - serialized value may not be as expected [input_value=Message(content='[[ ## an...ields={'refusal': None}), input_type=Message])\n",
      "  PydanticSerializationUnexpectedValue(Expected `StreamingChoices` - serialized value may not be as expected [input_value=Choices(finish_reason='st...': 5372768386634630000}), input_type=Choices])\n",
      "  return self.__pydantic_serializer__.to_python(\n",
      "Average Metric: 12.07 / 15 (80.4%):  30%|‚ñà‚ñà     | 15/50 [00:02<00:03, 10.23it/s]\u001b[92m12:51:18 - LiteLLM:INFO\u001b[0m: utils.py:3119 - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.3-70B-Instruct-Turbo; provider = together_ai\n",
      "2025-06-25 12:51:18,511 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.3-70B-Instruct-Turbo; provider = together_ai\n",
      "\u001b[92m12:51:18 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "2025-06-25 12:51:18,524 - LiteLLM - INFO - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "2025-06-25 12:51:18,569 - httpx - INFO - HTTP Request: POST https://api.together.xyz/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m12:51:18 - LiteLLM:INFO\u001b[0m: utils.py:1215 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-25 12:51:18,570 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m12:51:18 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "2025-06-25 12:51:18,570 - LiteLLM - INFO - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "/Users/ytchen/Documents/experimental/Llama4/env/lib/python3.13/site-packages/pydantic/main.py:463: UserWarning: Pydantic serializer warnings:\n",
      "  PydanticSerializationUnexpectedValue(Expected 9 fields but got 5: Expected `Message` - serialized value may not be as expected [input_value=Message(content='[[ ## an...ields={'refusal': None}), input_type=Message])\n",
      "  PydanticSerializationUnexpectedValue(Expected `StreamingChoices` - serialized value may not be as expected [input_value=Choices(finish_reason='st...': 3230154147858388000}), input_type=Choices])\n",
      "  return self.__pydantic_serializer__.to_python(\n",
      "Average Metric: 13.03 / 16 (81.5%):  30%|‚ñà‚ñà     | 15/50 [00:02<00:03, 10.23it/s]\u001b[92m12:51:18 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "2025-06-25 12:51:18,584 - LiteLLM - INFO - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "\u001b[92m12:51:18 - LiteLLM:INFO\u001b[0m: utils.py:3119 - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.3-70B-Instruct-Turbo; provider = together_ai\n",
      "2025-06-25 12:51:18,591 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.3-70B-Instruct-Turbo; provider = together_ai\n",
      "2025-06-25 12:51:18,833 - httpx - INFO - HTTP Request: POST https://api.together.xyz/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m12:51:18 - LiteLLM:INFO\u001b[0m: utils.py:1215 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-25 12:51:18,837 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m12:51:18 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "2025-06-25 12:51:18,838 - LiteLLM - INFO - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "/Users/ytchen/Documents/experimental/Llama4/env/lib/python3.13/site-packages/pydantic/main.py:463: UserWarning: Pydantic serializer warnings:\n",
      "  PydanticSerializationUnexpectedValue(Expected 9 fields but got 5: Expected `Message` - serialized value may not be as expected [input_value=Message(content='[[ ## an...ields={'refusal': None}), input_type=Message])\n",
      "  PydanticSerializationUnexpectedValue(Expected `StreamingChoices` - serialized value may not be as expected [input_value=Choices(finish_reason='st...': 9824504311507165000}), input_type=Choices])\n",
      "  return self.__pydantic_serializer__.to_python(\n",
      "Average Metric: 13.37 / 17 (78.6%):  34%|‚ñà‚ñà‚ñç    | 17/50 [00:02<00:03,  8.57it/s]\u001b[92m12:51:18 - LiteLLM:INFO\u001b[0m: utils.py:3119 - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.3-70B-Instruct-Turbo; provider = together_ai\n",
      "2025-06-25 12:51:18,858 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.3-70B-Instruct-Turbo; provider = together_ai\n",
      "\u001b[92m12:51:18 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "2025-06-25 12:51:18,885 - LiteLLM - INFO - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "2025-06-25 12:51:18,963 - httpx - INFO - HTTP Request: POST https://api.together.xyz/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m12:51:18 - LiteLLM:INFO\u001b[0m: utils.py:1215 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-25 12:51:18,963 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m12:51:18 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "2025-06-25 12:51:18,964 - LiteLLM - INFO - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "/Users/ytchen/Documents/experimental/Llama4/env/lib/python3.13/site-packages/pydantic/main.py:463: UserWarning: Pydantic serializer warnings:\n",
      "  PydanticSerializationUnexpectedValue(Expected 9 fields but got 5: Expected `Message` - serialized value may not be as expected [input_value=Message(content='[[ ## an...ields={'refusal': None}), input_type=Message])\n",
      "  PydanticSerializationUnexpectedValue(Expected `StreamingChoices` - serialized value may not be as expected [input_value=Choices(finish_reason='st...: 10466289934934616000}), input_type=Choices])\n",
      "  return self.__pydantic_serializer__.to_python(\n",
      "Average Metric: 14.30 / 18 (79.4%):  34%|‚ñà‚ñà‚ñç    | 17/50 [00:02<00:03,  8.57it/s]\u001b[92m12:51:18 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "2025-06-25 12:51:18,979 - LiteLLM - INFO - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "\u001b[92m12:51:18 - LiteLLM:INFO\u001b[0m: utils.py:3119 - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.3-70B-Instruct-Turbo; provider = together_ai\n",
      "2025-06-25 12:51:18,989 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.3-70B-Instruct-Turbo; provider = together_ai\n",
      "2025-06-25 12:51:19,084 - httpx - INFO - HTTP Request: POST https://api.together.xyz/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m12:51:19 - LiteLLM:INFO\u001b[0m: utils.py:1215 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-25 12:51:19,085 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m12:51:19 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "2025-06-25 12:51:19,085 - LiteLLM - INFO - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "/Users/ytchen/Documents/experimental/Llama4/env/lib/python3.13/site-packages/pydantic/main.py:463: UserWarning: Pydantic serializer warnings:\n",
      "  PydanticSerializationUnexpectedValue(Expected 9 fields but got 5: Expected `Message` - serialized value may not be as expected [input_value=Message(content='[[ ## an...ields={'refusal': None}), input_type=Message])\n",
      "  PydanticSerializationUnexpectedValue(Expected `StreamingChoices` - serialized value may not be as expected [input_value=Choices(finish_reason='st...: 13463488085397602000}), input_type=Choices])\n",
      "  return self.__pydantic_serializer__.to_python(\n",
      "\u001b[92m12:51:19 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "2025-06-25 12:51:19,102 - LiteLLM - INFO - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "Average Metric: 15.23 / 19 (80.2%):  38%|‚ñà‚ñà‚ñã    | 19/50 [00:02<00:03,  8.47it/s]2025-06-25 12:51:19,121 - httpx - INFO - HTTP Request: POST https://api.together.xyz/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m12:51:19 - LiteLLM:INFO\u001b[0m: utils.py:3119 - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.3-70B-Instruct-Turbo; provider = together_ai\n",
      "2025-06-25 12:51:19,124 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.3-70B-Instruct-Turbo; provider = together_ai\n",
      "\u001b[92m12:51:19 - LiteLLM:INFO\u001b[0m: utils.py:1215 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-25 12:51:19,147 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m12:51:19 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "2025-06-25 12:51:19,147 - LiteLLM - INFO - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "/Users/ytchen/Documents/experimental/Llama4/env/lib/python3.13/site-packages/pydantic/main.py:463: UserWarning: Pydantic serializer warnings:\n",
      "  PydanticSerializationUnexpectedValue(Expected 9 fields but got 5: Expected `Message` - serialized value may not be as expected [input_value=Message(content='[[ ## an...ields={'refusal': None}), input_type=Message])\n",
      "  PydanticSerializationUnexpectedValue(Expected `StreamingChoices` - serialized value may not be as expected [input_value=Choices(finish_reason='st...': 4375610281032240600}), input_type=Choices])\n",
      "  return self.__pydantic_serializer__.to_python(\n",
      "\u001b[92m12:51:19 - LiteLLM:INFO\u001b[0m: utils.py:3119 - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.3-70B-Instruct-Turbo; provider = together_ai\n",
      "2025-06-25 12:51:19,162 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.3-70B-Instruct-Turbo; provider = together_ai\n",
      "\u001b[92m12:51:19 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "2025-06-25 12:51:19,163 - LiteLLM - INFO - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "Average Metric: 16.17 / 20 (80.8%):  38%|‚ñà‚ñà‚ñã    | 19/50 [00:02<00:03,  8.47it/s]2025-06-25 12:51:19,251 - httpx - INFO - HTTP Request: POST https://api.together.xyz/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m12:51:19 - LiteLLM:INFO\u001b[0m: utils.py:1215 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-25 12:51:19,251 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m12:51:19 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "2025-06-25 12:51:19,252 - LiteLLM - INFO - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "/Users/ytchen/Documents/experimental/Llama4/env/lib/python3.13/site-packages/pydantic/main.py:463: UserWarning: Pydantic serializer warnings:\n",
      "  PydanticSerializationUnexpectedValue(Expected 9 fields but got 5: Expected `Message` - serialized value may not be as expected [input_value=Message(content='[[ ## an...ields={'refusal': None}), input_type=Message])\n",
      "  PydanticSerializationUnexpectedValue(Expected `StreamingChoices` - serialized value may not be as expected [input_value=Choices(finish_reason='st...': 1578671178022914600}), input_type=Choices])\n",
      "  return self.__pydantic_serializer__.to_python(\n",
      "Average Metric: 16.80 / 21 (80.0%):  42%|‚ñà‚ñà‚ñâ    | 21/50 [00:02<00:03,  9.31it/s]\u001b[92m12:51:19 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "2025-06-25 12:51:19,266 - LiteLLM - INFO - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "\u001b[92m12:51:19 - LiteLLM:INFO\u001b[0m: utils.py:3119 - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.3-70B-Instruct-Turbo; provider = together_ai\n",
      "2025-06-25 12:51:19,274 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.3-70B-Instruct-Turbo; provider = together_ai\n",
      "2025-06-25 12:51:19,401 - httpx - INFO - HTTP Request: POST https://api.together.xyz/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m12:51:19 - LiteLLM:INFO\u001b[0m: utils.py:1215 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-25 12:51:19,403 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m12:51:19 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "2025-06-25 12:51:19,403 - LiteLLM - INFO - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "/Users/ytchen/Documents/experimental/Llama4/env/lib/python3.13/site-packages/pydantic/main.py:463: UserWarning: Pydantic serializer warnings:\n",
      "  PydanticSerializationUnexpectedValue(Expected 9 fields but got 5: Expected `Message` - serialized value may not be as expected [input_value=Message(content='[[ ## an...ields={'refusal': None}), input_type=Message])\n",
      "  PydanticSerializationUnexpectedValue(Expected `StreamingChoices` - serialized value may not be as expected [input_value=Choices(finish_reason='st...: 13910551477816308000}), input_type=Choices])\n",
      "  return self.__pydantic_serializer__.to_python(\n",
      "Average Metric: 17.13 / 22 (77.9%):  42%|‚ñà‚ñà‚ñâ    | 21/50 [00:02<00:03,  9.31it/s]\u001b[92m12:51:19 - LiteLLM:INFO\u001b[0m: utils.py:3119 - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.3-70B-Instruct-Turbo; provider = together_ai\n",
      "2025-06-25 12:51:19,416 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.3-70B-Instruct-Turbo; provider = together_ai\n",
      "\u001b[92m12:51:19 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "2025-06-25 12:51:19,432 - LiteLLM - INFO - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "2025-06-25 12:51:19,473 - httpx - INFO - HTTP Request: POST https://api.together.xyz/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m12:51:19 - LiteLLM:INFO\u001b[0m: utils.py:1215 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-25 12:51:19,474 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m12:51:19 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "2025-06-25 12:51:19,474 - LiteLLM - INFO - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "/Users/ytchen/Documents/experimental/Llama4/env/lib/python3.13/site-packages/pydantic/main.py:463: UserWarning: Pydantic serializer warnings:\n",
      "  PydanticSerializationUnexpectedValue(Expected 9 fields but got 5: Expected `Message` - serialized value may not be as expected [input_value=Message(content='[[ ## an...ields={'refusal': None}), input_type=Message])\n",
      "  PydanticSerializationUnexpectedValue(Expected `StreamingChoices` - serialized value may not be as expected [input_value=Choices(finish_reason='st...: 10455309426522620000}), input_type=Choices])\n",
      "  return self.__pydantic_serializer__.to_python(\n",
      "Average Metric: 18.10 / 23 (78.7%):  44%|‚ñà‚ñà‚ñà    | 22/50 [00:03<00:03,  9.31it/s]\u001b[92m12:51:19 - LiteLLM:INFO\u001b[0m: utils.py:3119 - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.3-70B-Instruct-Turbo; provider = together_ai\n",
      "2025-06-25 12:51:19,489 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.3-70B-Instruct-Turbo; provider = together_ai\n",
      "Average Metric: 18.10 / 23 (78.7%):  46%|‚ñà‚ñà‚ñà‚ñè   | 23/50 [00:03<00:02,  9.23it/s]\u001b[92m12:51:19 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "2025-06-25 12:51:19,498 - LiteLLM - INFO - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "2025-06-25 12:51:19,506 - httpx - INFO - HTTP Request: POST https://api.together.xyz/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m12:51:19 - LiteLLM:INFO\u001b[0m: utils.py:1215 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-25 12:51:19,506 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m12:51:19 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "2025-06-25 12:51:19,506 - LiteLLM - INFO - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "/Users/ytchen/Documents/experimental/Llama4/env/lib/python3.13/site-packages/pydantic/main.py:463: UserWarning: Pydantic serializer warnings:\n",
      "  PydanticSerializationUnexpectedValue(Expected 9 fields but got 5: Expected `Message` - serialized value may not be as expected [input_value=Message(content='[[ ## an...ields={'refusal': None}), input_type=Message])\n",
      "  PydanticSerializationUnexpectedValue(Expected `StreamingChoices` - serialized value may not be as expected [input_value=Choices(finish_reason='st...: 10112799266261301000}), input_type=Choices])\n",
      "  return self.__pydantic_serializer__.to_python(\n",
      "\u001b[92m12:51:19 - LiteLLM:INFO\u001b[0m: utils.py:3119 - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.3-70B-Instruct-Turbo; provider = together_ai\n",
      "Average Metric: 18.43 / 24 (76.8%):  46%|‚ñà‚ñà‚ñà‚ñè   | 23/50 [00:03<00:02,  9.23it/s]2025-06-25 12:51:19,520 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.3-70B-Instruct-Turbo; provider = together_ai\n",
      "\u001b[92m12:51:19 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "2025-06-25 12:51:19,521 - LiteLLM - INFO - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "2025-06-25 12:51:19,534 - httpx - INFO - HTTP Request: POST https://api.together.xyz/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m12:51:19 - LiteLLM:INFO\u001b[0m: utils.py:1215 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-25 12:51:19,539 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m12:51:19 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "2025-06-25 12:51:19,539 - LiteLLM - INFO - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "/Users/ytchen/Documents/experimental/Llama4/env/lib/python3.13/site-packages/pydantic/main.py:463: UserWarning: Pydantic serializer warnings:\n",
      "  PydanticSerializationUnexpectedValue(Expected 9 fields but got 5: Expected `Message` - serialized value may not be as expected [input_value=Message(content='[[ ## an...ields={'refusal': None}), input_type=Message])\n",
      "  PydanticSerializationUnexpectedValue(Expected `StreamingChoices` - serialized value may not be as expected [input_value=Choices(finish_reason='st...': 1495049302597541600}), input_type=Choices])\n",
      "  return self.__pydantic_serializer__.to_python(\n",
      "Average Metric: 19.37 / 25 (77.5%):  48%|‚ñà‚ñà‚ñà‚ñé   | 24/50 [00:03<00:02,  9.23it/s]\u001b[92m12:51:19 - LiteLLM:INFO\u001b[0m: utils.py:3119 - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.3-70B-Instruct-Turbo; provider = together_ai\n",
      "2025-06-25 12:51:19,546 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.3-70B-Instruct-Turbo; provider = together_ai\n",
      "\u001b[92m12:51:19 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "2025-06-25 12:51:19,559 - LiteLLM - INFO - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "2025-06-25 12:51:19,613 - httpx - INFO - HTTP Request: POST https://api.together.xyz/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m12:51:19 - LiteLLM:INFO\u001b[0m: utils.py:1215 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-25 12:51:19,614 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m12:51:19 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "2025-06-25 12:51:19,614 - LiteLLM - INFO - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "/Users/ytchen/Documents/experimental/Llama4/env/lib/python3.13/site-packages/pydantic/main.py:463: UserWarning: Pydantic serializer warnings:\n",
      "  PydanticSerializationUnexpectedValue(Expected 9 fields but got 5: Expected `Message` - serialized value may not be as expected [input_value=Message(content='[[ ## an...ields={'refusal': None}), input_type=Message])\n",
      "  PydanticSerializationUnexpectedValue(Expected `StreamingChoices` - serialized value may not be as expected [input_value=Choices(finish_reason='st...': 2243726059903382800}), input_type=Choices])\n",
      "  return self.__pydantic_serializer__.to_python(\n",
      "Average Metric: 20.33 / 26 (78.2%):  52%|‚ñà‚ñà‚ñà‚ñã   | 26/50 [00:03<00:02, 11.69it/s]\u001b[92m12:51:19 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "2025-06-25 12:51:19,629 - LiteLLM - INFO - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "\u001b[92m12:51:19 - LiteLLM:INFO\u001b[0m: utils.py:3119 - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.3-70B-Instruct-Turbo; provider = together_ai\n",
      "2025-06-25 12:51:19,636 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.3-70B-Instruct-Turbo; provider = together_ai\n",
      "2025-06-25 12:51:19,743 - httpx - INFO - HTTP Request: POST https://api.together.xyz/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m12:51:19 - LiteLLM:INFO\u001b[0m: utils.py:1215 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-25 12:51:19,744 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m12:51:19 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "2025-06-25 12:51:19,744 - LiteLLM - INFO - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "/Users/ytchen/Documents/experimental/Llama4/env/lib/python3.13/site-packages/pydantic/main.py:463: UserWarning: Pydantic serializer warnings:\n",
      "  PydanticSerializationUnexpectedValue(Expected 9 fields but got 5: Expected `Message` - serialized value may not be as expected [input_value=Message(content='[[ ## an...ields={'refusal': None}), input_type=Message])\n",
      "  PydanticSerializationUnexpectedValue(Expected `StreamingChoices` - serialized value may not be as expected [input_value=Choices(finish_reason='st...': 9280793731928148000}), input_type=Choices])\n",
      "  return self.__pydantic_serializer__.to_python(\n",
      "Average Metric: 20.63 / 27 (76.4%):  52%|‚ñà‚ñà‚ñà‚ñã   | 26/50 [00:03<00:02, 11.69it/s]\u001b[92m12:51:19 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "2025-06-25 12:51:19,761 - LiteLLM - INFO - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "\u001b[92m12:51:19 - LiteLLM:INFO\u001b[0m: utils.py:3119 - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.3-70B-Instruct-Turbo; provider = together_ai\n",
      "2025-06-25 12:51:19,769 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.3-70B-Instruct-Turbo; provider = together_ai\n",
      "2025-06-25 12:51:19,831 - httpx - INFO - HTTP Request: POST https://api.together.xyz/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m12:51:19 - LiteLLM:INFO\u001b[0m: utils.py:1215 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-25 12:51:19,832 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m12:51:19 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "2025-06-25 12:51:19,832 - LiteLLM - INFO - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "/Users/ytchen/Documents/experimental/Llama4/env/lib/python3.13/site-packages/pydantic/main.py:463: UserWarning: Pydantic serializer warnings:\n",
      "  PydanticSerializationUnexpectedValue(Expected 9 fields but got 5: Expected `Message` - serialized value may not be as expected [input_value=Message(content='[[ ## an...ields={'refusal': None}), input_type=Message])\n",
      "  PydanticSerializationUnexpectedValue(Expected `StreamingChoices` - serialized value may not be as expected [input_value=Choices(finish_reason='st...: 11841456956978299000}), input_type=Choices])\n",
      "  return self.__pydantic_serializer__.to_python(\n",
      "Average Metric: 21.57 / 28 (77.0%):  56%|‚ñà‚ñà‚ñà‚ñâ   | 28/50 [00:03<00:02, 10.92it/s]\u001b[92m12:51:19 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "2025-06-25 12:51:19,847 - LiteLLM - INFO - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "\u001b[92m12:51:19 - LiteLLM:INFO\u001b[0m: utils.py:3119 - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.3-70B-Instruct-Turbo; provider = together_ai\n",
      "2025-06-25 12:51:19,856 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.3-70B-Instruct-Turbo; provider = together_ai\n",
      "2025-06-25 12:51:19,879 - httpx - INFO - HTTP Request: POST https://api.together.xyz/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m12:51:19 - LiteLLM:INFO\u001b[0m: utils.py:1215 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-25 12:51:19,880 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m12:51:19 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "2025-06-25 12:51:19,880 - LiteLLM - INFO - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "/Users/ytchen/Documents/experimental/Llama4/env/lib/python3.13/site-packages/pydantic/main.py:463: UserWarning: Pydantic serializer warnings:\n",
      "  PydanticSerializationUnexpectedValue(Expected 9 fields but got 5: Expected `Message` - serialized value may not be as expected [input_value=Message(content='[[ ## an...ields={'refusal': None}), input_type=Message])\n",
      "  PydanticSerializationUnexpectedValue(Expected `StreamingChoices` - serialized value may not be as expected [input_value=Choices(finish_reason='st...: 14500027025967280000}), input_type=Choices])\n",
      "  return self.__pydantic_serializer__.to_python(\n",
      "Average Metric: 22.53 / 29 (77.7%):  56%|‚ñà‚ñà‚ñà‚ñâ   | 28/50 [00:03<00:02, 10.92it/s]\u001b[92m12:51:19 - LiteLLM:INFO\u001b[0m: utils.py:3119 - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.3-70B-Instruct-Turbo; provider = together_ai\n",
      "2025-06-25 12:51:19,894 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.3-70B-Instruct-Turbo; provider = together_ai\n",
      "\u001b[92m12:51:19 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "2025-06-25 12:51:19,900 - LiteLLM - INFO - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "2025-06-25 12:51:19,938 - httpx - INFO - HTTP Request: POST https://api.together.xyz/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m12:51:19 - LiteLLM:INFO\u001b[0m: utils.py:1215 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-25 12:51:19,938 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m12:51:19 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "2025-06-25 12:51:19,939 - LiteLLM - INFO - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "/Users/ytchen/Documents/experimental/Llama4/env/lib/python3.13/site-packages/pydantic/main.py:463: UserWarning: Pydantic serializer warnings:\n",
      "  PydanticSerializationUnexpectedValue(Expected 9 fields but got 5: Expected `Message` - serialized value may not be as expected [input_value=Message(content='[[ ## an...ields={'refusal': None}), input_type=Message])\n",
      "  PydanticSerializationUnexpectedValue(Expected `StreamingChoices` - serialized value may not be as expected [input_value=Choices(finish_reason='st...': 2467134694076685000}), input_type=Choices])\n",
      "  return self.__pydantic_serializer__.to_python(\n",
      "Average Metric: 23.53 / 30 (78.4%):  60%|‚ñà‚ñà‚ñà‚ñà‚ñè  | 30/50 [00:03<00:01, 12.30it/s]\u001b[92m12:51:19 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "2025-06-25 12:51:19,953 - LiteLLM - INFO - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "\u001b[92m12:51:19 - LiteLLM:INFO\u001b[0m: utils.py:3119 - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.3-70B-Instruct-Turbo; provider = together_ai\n",
      "2025-06-25 12:51:19,960 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.3-70B-Instruct-Turbo; provider = together_ai\n",
      "2025-06-25 12:51:20,074 - httpx - INFO - HTTP Request: POST https://api.together.xyz/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m12:51:20 - LiteLLM:INFO\u001b[0m: utils.py:1215 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-25 12:51:20,075 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m12:51:20 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "2025-06-25 12:51:20,076 - LiteLLM - INFO - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "/Users/ytchen/Documents/experimental/Llama4/env/lib/python3.13/site-packages/pydantic/main.py:463: UserWarning: Pydantic serializer warnings:\n",
      "  PydanticSerializationUnexpectedValue(Expected 9 fields but got 5: Expected `Message` - serialized value may not be as expected [input_value=Message(content='[[ ## an...ields={'refusal': None}), input_type=Message])\n",
      "  PydanticSerializationUnexpectedValue(Expected `StreamingChoices` - serialized value may not be as expected [input_value=Choices(finish_reason='st...: 17017652254857607000}), input_type=Choices])\n",
      "  return self.__pydantic_serializer__.to_python(\n",
      "Average Metric: 24.47 / 31 (78.9%):  60%|‚ñà‚ñà‚ñà‚ñà‚ñè  | 30/50 [00:03<00:01, 12.30it/s]\u001b[92m12:51:20 - LiteLLM:INFO\u001b[0m: utils.py:3119 - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.3-70B-Instruct-Turbo; provider = together_ai\n",
      "2025-06-25 12:51:20,093 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.3-70B-Instruct-Turbo; provider = together_ai\n",
      "\u001b[92m12:51:20 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "2025-06-25 12:51:20,102 - LiteLLM - INFO - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "2025-06-25 12:51:20,114 - httpx - INFO - HTTP Request: POST https://api.together.xyz/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "\u001b[92m12:51:20 - LiteLLM:INFO\u001b[0m: utils.py:3119 - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.3-70B-Instruct-Turbo; provider = together_ai\n",
      "2025-06-25 12:51:20,122 - httpx - INFO - HTTP Request: POST https://api.together.xyz/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-25 12:51:20,122 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.3-70B-Instruct-Turbo; provider = together_ai\n",
      "\u001b[92m12:51:20 - LiteLLM:INFO\u001b[0m: utils.py:3119 - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.3-70B-Instruct-Turbo; provider = together_ai\n",
      "2025-06-25 12:51:20,136 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.3-70B-Instruct-Turbo; provider = together_ai\n",
      "2025-06-25 12:51:20,244 - httpx - INFO - HTTP Request: POST https://api.together.xyz/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "\u001b[92m12:51:20 - LiteLLM:INFO\u001b[0m: utils.py:3119 - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.3-70B-Instruct-Turbo; provider = together_ai\n",
      "2025-06-25 12:51:20,252 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.3-70B-Instruct-Turbo; provider = together_ai\n",
      "2025-06-25 12:51:20,263 - httpx - INFO - HTTP Request: POST https://api.together.xyz/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m12:51:20 - LiteLLM:INFO\u001b[0m: utils.py:1215 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-25 12:51:20,265 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m12:51:20 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "2025-06-25 12:51:20,266 - LiteLLM - INFO - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "/Users/ytchen/Documents/experimental/Llama4/env/lib/python3.13/site-packages/pydantic/main.py:463: UserWarning: Pydantic serializer warnings:\n",
      "  PydanticSerializationUnexpectedValue(Expected 9 fields but got 5: Expected `Message` - serialized value may not be as expected [input_value=Message(content='[[ ## an...ields={'refusal': None}), input_type=Message])\n",
      "  PydanticSerializationUnexpectedValue(Expected `StreamingChoices` - serialized value may not be as expected [input_value=Choices(finish_reason='st...': 8560024585473913000}), input_type=Choices])\n",
      "  return self.__pydantic_serializer__.to_python(\n",
      "\u001b[92m12:51:20 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "2025-06-25 12:51:20,274 - LiteLLM - INFO - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "2025-06-25 12:51:20,289 - httpx - INFO - HTTP Request: POST https://api.together.xyz/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "\u001b[92m12:51:20 - LiteLLM:INFO\u001b[0m: utils.py:3119 - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.3-70B-Instruct-Turbo; provider = together_ai\n",
      "2025-06-25 12:51:20,291 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.3-70B-Instruct-Turbo; provider = together_ai\n",
      "2025-06-25 12:51:20,295 - httpx - INFO - HTTP Request: POST https://api.together.xyz/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "Average Metric: 25.10 / 32 (78.4%):  64%|‚ñà‚ñà‚ñà‚ñà‚ñç  | 32/50 [00:03<00:01,  9.38it/s]\u001b[92m12:51:20 - LiteLLM:INFO\u001b[0m: utils.py:3119 - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.3-70B-Instruct-Turbo; provider = together_ai\n",
      "2025-06-25 12:51:20,329 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.3-70B-Instruct-Turbo; provider = together_ai\n",
      "\u001b[92m12:51:20 - LiteLLM:INFO\u001b[0m: utils.py:3119 - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.3-70B-Instruct-Turbo; provider = together_ai\n",
      "2025-06-25 12:51:20,335 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.3-70B-Instruct-Turbo; provider = together_ai\n",
      "2025-06-25 12:51:20,363 - httpx - INFO - HTTP Request: POST https://api.together.xyz/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "\u001b[92m12:51:20 - LiteLLM:INFO\u001b[0m: utils.py:3119 - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.3-70B-Instruct-Turbo; provider = together_ai\n",
      "2025-06-25 12:51:20,379 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.3-70B-Instruct-Turbo; provider = together_ai\n",
      "2025-06-25 12:51:20,431 - httpx - INFO - HTTP Request: POST https://api.together.xyz/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-06-25 12:51:20,434 - httpx - INFO - HTTP Request: POST https://api.together.xyz/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m12:51:20 - LiteLLM:INFO\u001b[0m: utils.py:1215 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-25 12:51:20,435 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m12:51:20 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "2025-06-25 12:51:20,435 - LiteLLM - INFO - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "Average Metric: 25.77 / 33 (78.1%):  64%|‚ñà‚ñà‚ñà‚ñà‚ñç  | 32/50 [00:03<00:01,  9.38it/s]/Users/ytchen/Documents/experimental/Llama4/env/lib/python3.13/site-packages/pydantic/main.py:463: UserWarning: Pydantic serializer warnings:\n",
      "  PydanticSerializationUnexpectedValue(Expected 9 fields but got 5: Expected `Message` - serialized value may not be as expected [input_value=Message(content='[[ ## an...ields={'refusal': None}), input_type=Message])\n",
      "  PydanticSerializationUnexpectedValue(Expected `StreamingChoices` - serialized value may not be as expected [input_value=Choices(finish_reason='st...: 14832064845926083000}), input_type=Choices])\n",
      "  return self.__pydantic_serializer__.to_python(\n",
      "\u001b[92m12:51:20 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "2025-06-25 12:51:20,443 - LiteLLM - INFO - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "\u001b[92m12:51:20 - LiteLLM:INFO\u001b[0m: utils.py:1215 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-25 12:51:20,450 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m12:51:20 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "2025-06-25 12:51:20,450 - LiteLLM - INFO - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "Average Metric: 26.43 / 34 (77.7%):  68%|‚ñà‚ñà‚ñà‚ñà‚ñä  | 34/50 [00:04<00:01, 10.17it/s]/Users/ytchen/Documents/experimental/Llama4/env/lib/python3.13/site-packages/pydantic/main.py:463: UserWarning: Pydantic serializer warnings:\n",
      "  PydanticSerializationUnexpectedValue(Expected 9 fields but got 5: Expected `Message` - serialized value may not be as expected [input_value=Message(content='[[ ## an...ields={'refusal': None}), input_type=Message])\n",
      "  PydanticSerializationUnexpectedValue(Expected `StreamingChoices` - serialized value may not be as expected [input_value=Choices(finish_reason='st...: 13695271686338648000}), input_type=Choices])\n",
      "  return self.__pydantic_serializer__.to_python(\n",
      "\u001b[92m12:51:20 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "2025-06-25 12:51:20,458 - LiteLLM - INFO - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "2025-06-25 12:51:20,465 - httpx - INFO - HTTP Request: POST https://api.together.xyz/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-25 12:51:20,465 - httpx - INFO - HTTP Request: POST https://api.together.xyz/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "\u001b[92m12:51:20 - LiteLLM:INFO\u001b[0m: utils.py:3119 - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.3-70B-Instruct-Turbo; provider = together_ai\n",
      "2025-06-25 12:51:20,481 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.3-70B-Instruct-Turbo; provider = together_ai\n",
      "2025-06-25 12:51:20,489 - httpx - INFO - HTTP Request: POST https://api.together.xyz/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "\u001b[92m12:51:20 - LiteLLM:INFO\u001b[0m: utils.py:3119 - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.3-70B-Instruct-Turbo; provider = together_ai\n",
      "2025-06-25 12:51:20,501 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.3-70B-Instruct-Turbo; provider = together_ai\n",
      "2025-06-25 12:51:20,508 - httpx - INFO - HTTP Request: POST https://api.together.xyz/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "\u001b[92m12:51:20 - LiteLLM:INFO\u001b[0m: utils.py:3119 - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.3-70B-Instruct-Turbo; provider = together_ai\n",
      "2025-06-25 12:51:20,515 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.3-70B-Instruct-Turbo; provider = together_ai\n",
      "\u001b[92m12:51:20 - LiteLLM:INFO\u001b[0m: utils.py:3119 - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.3-70B-Instruct-Turbo; provider = together_ai\n",
      "2025-06-25 12:51:20,535 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.3-70B-Instruct-Turbo; provider = together_ai\n",
      "2025-06-25 12:51:20,608 - httpx - INFO - HTTP Request: POST https://api.together.xyz/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-06-25 12:51:20,609 - httpx - INFO - HTTP Request: POST https://api.together.xyz/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m12:51:20 - LiteLLM:INFO\u001b[0m: utils.py:1215 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-25 12:51:20,611 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m12:51:20 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "2025-06-25 12:51:20,611 - LiteLLM - INFO - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "Average Metric: 27.40 / 35 (78.3%):  68%|‚ñà‚ñà‚ñà‚ñà‚ñä  | 34/50 [00:04<00:01, 10.17it/s]/Users/ytchen/Documents/experimental/Llama4/env/lib/python3.13/site-packages/pydantic/main.py:463: UserWarning: Pydantic serializer warnings:\n",
      "  PydanticSerializationUnexpectedValue(Expected 9 fields but got 5: Expected `Message` - serialized value may not be as expected [input_value=Message(content='[[ ## an...ields={'refusal': None}), input_type=Message])\n",
      "  PydanticSerializationUnexpectedValue(Expected `StreamingChoices` - serialized value may not be as expected [input_value=Choices(finish_reason='st...': 2904531318763725000}), input_type=Choices])\n",
      "  return self.__pydantic_serializer__.to_python(\n",
      "\u001b[92m12:51:20 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "2025-06-25 12:51:20,625 - LiteLLM - INFO - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "\u001b[92m12:51:20 - LiteLLM:INFO\u001b[0m: utils.py:1215 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-25 12:51:20,632 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m12:51:20 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "2025-06-25 12:51:20,632 - LiteLLM - INFO - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "\u001b[92m12:51:20 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "Average Metric: 28.07 / 36 (78.0%):  72%|‚ñà‚ñà‚ñà‚ñà‚ñà  | 36/50 [00:04<00:01, 10.40it/s]2025-06-25 12:51:20,632 - LiteLLM - INFO - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "/Users/ytchen/Documents/experimental/Llama4/env/lib/python3.13/site-packages/pydantic/main.py:463: UserWarning: Pydantic serializer warnings:\n",
      "  PydanticSerializationUnexpectedValue(Expected 9 fields but got 5: Expected `Message` - serialized value may not be as expected [input_value=Message(content='[[ ## an...ields={'refusal': None}), input_type=Message])\n",
      "  PydanticSerializationUnexpectedValue(Expected `StreamingChoices` - serialized value may not be as expected [input_value=Choices(finish_reason='st...: 10212770042602314000}), input_type=Choices])\n",
      "  return self.__pydantic_serializer__.to_python(\n",
      "\u001b[92m12:51:20 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "2025-06-25 12:51:20,640 - LiteLLM - INFO - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "2025-06-25 12:51:20,660 - httpx - INFO - HTTP Request: POST https://api.together.xyz/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-06-25 12:51:20,661 - httpx - INFO - HTTP Request: POST https://api.together.xyz/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m12:51:20 - LiteLLM:INFO\u001b[0m: utils.py:1215 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-25 12:51:20,662 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m12:51:20 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "2025-06-25 12:51:20,662 - LiteLLM - INFO - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "Average Metric: 29.03 / 37 (78.5%):  72%|‚ñà‚ñà‚ñà‚ñà‚ñà  | 36/50 [00:04<00:01, 10.40it/s]/Users/ytchen/Documents/experimental/Llama4/env/lib/python3.13/site-packages/pydantic/main.py:463: UserWarning: Pydantic serializer warnings:\n",
      "  PydanticSerializationUnexpectedValue(Expected 9 fields but got 5: Expected `Message` - serialized value may not be as expected [input_value=Message(content='[[ ## an...ields={'refusal': None}), input_type=Message])\n",
      "  PydanticSerializationUnexpectedValue(Expected `StreamingChoices` - serialized value may not be as expected [input_value=Choices(finish_reason='st...: 10192148984860060000}), input_type=Choices])\n",
      "  return self.__pydantic_serializer__.to_python(\n",
      "\u001b[92m12:51:20 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "2025-06-25 12:51:20,670 - LiteLLM - INFO - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "\u001b[92m12:51:20 - LiteLLM:INFO\u001b[0m: utils.py:1215 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-25 12:51:20,678 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m12:51:20 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "2025-06-25 12:51:20,678 - LiteLLM - INFO - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "Average Metric: 29.63 / 38 (78.0%):  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 37/50 [00:04<00:01, 10.40it/s]/Users/ytchen/Documents/experimental/Llama4/env/lib/python3.13/site-packages/pydantic/main.py:463: UserWarning: Pydantic serializer warnings:\n",
      "  PydanticSerializationUnexpectedValue(Expected 9 fields but got 5: Expected `Message` - serialized value may not be as expected [input_value=Message(content='[[ ## an...ields={'refusal': None}), input_type=Message])\n",
      "  PydanticSerializationUnexpectedValue(Expected `StreamingChoices` - serialized value may not be as expected [input_value=Choices(finish_reason='st...: 14155255808516063000}), input_type=Choices])\n",
      "  return self.__pydantic_serializer__.to_python(\n",
      "\u001b[92m12:51:20 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "2025-06-25 12:51:20,685 - LiteLLM - INFO - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "2025-06-25 12:51:20,697 - httpx - INFO - HTTP Request: POST https://api.together.xyz/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m12:51:20 - LiteLLM:INFO\u001b[0m: utils.py:1215 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-25 12:51:20,698 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m12:51:20 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "2025-06-25 12:51:20,698 - LiteLLM - INFO - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "Average Metric: 30.27 / 39 (77.6%):  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 38/50 [00:04<00:01, 10.40it/s]/Users/ytchen/Documents/experimental/Llama4/env/lib/python3.13/site-packages/pydantic/main.py:463: UserWarning: Pydantic serializer warnings:\n",
      "  PydanticSerializationUnexpectedValue(Expected 9 fields but got 5: Expected `Message` - serialized value may not be as expected [input_value=Message(content='[[ ## an...ields={'refusal': None}), input_type=Message])\n",
      "  PydanticSerializationUnexpectedValue(Expected `StreamingChoices` - serialized value may not be as expected [input_value=Choices(finish_reason='st...': 8268823920455162000}), input_type=Choices])\n",
      "  return self.__pydantic_serializer__.to_python(\n",
      "\u001b[92m12:51:20 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "2025-06-25 12:51:20,705 - LiteLLM - INFO - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "2025-06-25 12:51:20,762 - httpx - INFO - HTTP Request: POST https://api.together.xyz/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m12:51:20 - LiteLLM:INFO\u001b[0m: utils.py:1215 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-25 12:51:20,763 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m12:51:20 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "2025-06-25 12:51:20,763 - LiteLLM - INFO - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "Average Metric: 30.90 / 40 (77.2%):  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 40/50 [00:04<00:00, 14.83it/s]/Users/ytchen/Documents/experimental/Llama4/env/lib/python3.13/site-packages/pydantic/main.py:463: UserWarning: Pydantic serializer warnings:\n",
      "  PydanticSerializationUnexpectedValue(Expected 9 fields but got 5: Expected `Message` - serialized value may not be as expected [input_value=Message(content='[[ ## an...ields={'refusal': None}), input_type=Message])\n",
      "  PydanticSerializationUnexpectedValue(Expected `StreamingChoices` - serialized value may not be as expected [input_value=Choices(finish_reason='st...: 12543243782088395000}), input_type=Choices])\n",
      "  return self.__pydantic_serializer__.to_python(\n",
      "\u001b[92m12:51:20 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "2025-06-25 12:51:20,770 - LiteLLM - INFO - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "2025-06-25 12:51:20,944 - httpx - INFO - HTTP Request: POST https://api.together.xyz/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m12:51:20 - LiteLLM:INFO\u001b[0m: utils.py:1215 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-25 12:51:20,947 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m12:51:20 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "2025-06-25 12:51:20,948 - LiteLLM - INFO - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "/Users/ytchen/Documents/experimental/Llama4/env/lib/python3.13/site-packages/pydantic/main.py:463: UserWarning: Pydantic serializer warnings:\n",
      "  PydanticSerializationUnexpectedValue(Expected 9 fields but got 5: Expected `Message` - serialized value may not be as expected [input_value=Message(content='[[ ## an...ields={'refusal': None}), input_type=Message])\n",
      "  PydanticSerializationUnexpectedValue(Expected `StreamingChoices` - serialized value may not be as expected [input_value=Choices(finish_reason='st...d': 195692215469261020}), input_type=Choices])\n",
      "  return self.__pydantic_serializer__.to_python(\n",
      "Average Metric: 31.87 / 41 (77.7%):  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 40/50 [00:04<00:00, 14.83it/s]\u001b[92m12:51:20 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "2025-06-25 12:51:20,966 - LiteLLM - INFO - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "2025-06-25 12:51:20,978 - httpx - INFO - HTTP Request: POST https://api.together.xyz/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m12:51:20 - LiteLLM:INFO\u001b[0m: utils.py:1215 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-25 12:51:20,979 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m12:51:20 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "2025-06-25 12:51:20,979 - LiteLLM - INFO - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "Average Metric: 32.83 / 42 (78.2%):  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 42/50 [00:04<00:00, 12.91it/s]/Users/ytchen/Documents/experimental/Llama4/env/lib/python3.13/site-packages/pydantic/main.py:463: UserWarning: Pydantic serializer warnings:\n",
      "  PydanticSerializationUnexpectedValue(Expected 9 fields but got 5: Expected `Message` - serialized value may not be as expected [input_value=Message(content='[[ ## an...ields={'refusal': None}), input_type=Message])\n",
      "  PydanticSerializationUnexpectedValue(Expected `StreamingChoices` - serialized value may not be as expected [input_value=Choices(finish_reason='st...: 13575679654650366000}), input_type=Choices])\n",
      "  return self.__pydantic_serializer__.to_python(\n",
      "\u001b[92m12:51:20 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "2025-06-25 12:51:20,991 - LiteLLM - INFO - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "2025-06-25 12:51:21,167 - httpx - INFO - HTTP Request: POST https://api.together.xyz/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m12:51:21 - LiteLLM:INFO\u001b[0m: utils.py:1215 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-25 12:51:21,170 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m12:51:21 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "2025-06-25 12:51:21,171 - LiteLLM - INFO - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "Average Metric: 33.77 / 43 (78.5%):  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 42/50 [00:04<00:00, 12.91it/s]/Users/ytchen/Documents/experimental/Llama4/env/lib/python3.13/site-packages/pydantic/main.py:463: UserWarning: Pydantic serializer warnings:\n",
      "  PydanticSerializationUnexpectedValue(Expected 9 fields but got 5: Expected `Message` - serialized value may not be as expected [input_value=Message(content='[[ ## an...ields={'refusal': None}), input_type=Message])\n",
      "  PydanticSerializationUnexpectedValue(Expected `StreamingChoices` - serialized value may not be as expected [input_value=Choices(finish_reason='st...: 17483264249849706000}), input_type=Choices])\n",
      "  return self.__pydantic_serializer__.to_python(\n",
      "\u001b[92m12:51:21 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "2025-06-25 12:51:21,195 - LiteLLM - INFO - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "2025-06-25 12:51:21,275 - httpx - INFO - HTTP Request: POST https://api.together.xyz/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m12:51:21 - LiteLLM:INFO\u001b[0m: utils.py:1215 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-25 12:51:21,278 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m12:51:21 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "2025-06-25 12:51:21,279 - LiteLLM - INFO - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "\u001b[92m12:51:21 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "2025-06-25 12:51:21,279 - LiteLLM - INFO - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "/Users/ytchen/Documents/experimental/Llama4/env/lib/python3.13/site-packages/pydantic/main.py:463: UserWarning: Pydantic serializer warnings:\n",
      "  PydanticSerializationUnexpectedValue(Expected 9 fields but got 5: Expected `Message` - serialized value may not be as expected [input_value=Message(content='[[ ## an...ields={'refusal': None}), input_type=Message])\n",
      "  PydanticSerializationUnexpectedValue(Expected `StreamingChoices` - serialized value may not be as expected [input_value=Choices(finish_reason='st...d': 179386747397334700}), input_type=Choices])\n",
      "  return self.__pydantic_serializer__.to_python(\n",
      "Average Metric: 34.70 / 44 (78.9%):  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 44/50 [00:04<00:00, 10.31it/s]\u001b[92m12:51:21 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "2025-06-25 12:51:21,295 - LiteLLM - INFO - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "2025-06-25 12:51:21,341 - httpx - INFO - HTTP Request: POST https://api.together.xyz/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m12:51:21 - LiteLLM:INFO\u001b[0m: utils.py:1215 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-25 12:51:21,343 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m12:51:21 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "2025-06-25 12:51:21,343 - LiteLLM - INFO - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "\u001b[92m12:51:21 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "2025-06-25 12:51:21,343 - LiteLLM - INFO - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "/Users/ytchen/Documents/experimental/Llama4/env/lib/python3.13/site-packages/pydantic/main.py:463: UserWarning: Pydantic serializer warnings:\n",
      "  PydanticSerializationUnexpectedValue(Expected 9 fields but got 5: Expected `Message` - serialized value may not be as expected [input_value=Message(content='[[ ## an...ields={'refusal': None}), input_type=Message])\n",
      "  PydanticSerializationUnexpectedValue(Expected `StreamingChoices` - serialized value may not be as expected [input_value=Choices(finish_reason='st...: 13553385325166483000}), input_type=Choices])\n",
      "  return self.__pydantic_serializer__.to_python(\n",
      "\u001b[92m12:51:21 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "Average Metric: 35.70 / 45 (79.3%):  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 44/50 [00:04<00:00, 10.31it/s]2025-06-25 12:51:21,353 - LiteLLM - INFO - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "2025-06-25 12:51:21,374 - httpx - INFO - HTTP Request: POST https://api.together.xyz/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m12:51:21 - LiteLLM:INFO\u001b[0m: utils.py:1215 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-25 12:51:21,375 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m12:51:21 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "2025-06-25 12:51:21,375 - LiteLLM - INFO - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "Average Metric: 36.67 / 46 (79.7%):  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 45/50 [00:04<00:00, 10.31it/s]/Users/ytchen/Documents/experimental/Llama4/env/lib/python3.13/site-packages/pydantic/main.py:463: UserWarning: Pydantic serializer warnings:\n",
      "  PydanticSerializationUnexpectedValue(Expected 9 fields but got 5: Expected `Message` - serialized value may not be as expected [input_value=Message(content='[[ ## an...ields={'refusal': None}), input_type=Message])\n",
      "  PydanticSerializationUnexpectedValue(Expected `StreamingChoices` - serialized value may not be as expected [input_value=Choices(finish_reason='st...d': 387205243151686800}), input_type=Choices])\n",
      "  return self.__pydantic_serializer__.to_python(\n",
      "\u001b[92m12:51:21 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "2025-06-25 12:51:21,384 - LiteLLM - INFO - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "2025-06-25 12:51:21,507 - httpx - INFO - HTTP Request: POST https://api.together.xyz/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m12:51:21 - LiteLLM:INFO\u001b[0m: utils.py:1215 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-25 12:51:21,511 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m12:51:21 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "2025-06-25 12:51:21,511 - LiteLLM - INFO - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "/Users/ytchen/Documents/experimental/Llama4/env/lib/python3.13/site-packages/pydantic/main.py:463: UserWarning: Pydantic serializer warnings:\n",
      "  PydanticSerializationUnexpectedValue(Expected 9 fields but got 5: Expected `Message` - serialized value may not be as expected [input_value=Message(content='[[ ## an...ields={'refusal': None}), input_type=Message])\n",
      "  PydanticSerializationUnexpectedValue(Expected `StreamingChoices` - serialized value may not be as expected [input_value=Choices(finish_reason='st...': 7200407251047123000}), input_type=Choices])\n",
      "  return self.__pydantic_serializer__.to_python(\n",
      "Average Metric: 37.30 / 47 (79.4%):  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 47/50 [00:05<00:00, 11.19it/s]\u001b[92m12:51:21 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "2025-06-25 12:51:21,534 - LiteLLM - INFO - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "2025-06-25 12:51:21,688 - httpx - INFO - HTTP Request: POST https://api.together.xyz/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m12:51:21 - LiteLLM:INFO\u001b[0m: utils.py:1215 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-25 12:51:21,692 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m12:51:21 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "2025-06-25 12:51:21,692 - LiteLLM - INFO - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "/Users/ytchen/Documents/experimental/Llama4/env/lib/python3.13/site-packages/pydantic/main.py:463: UserWarning: Pydantic serializer warnings:\n",
      "  PydanticSerializationUnexpectedValue(Expected 9 fields but got 5: Expected `Message` - serialized value may not be as expected [input_value=Message(content='[[ ## an...ields={'refusal': None}), input_type=Message])\n",
      "  PydanticSerializationUnexpectedValue(Expected `StreamingChoices` - serialized value may not be as expected [input_value=Choices(finish_reason='st...: 13360060795467608000}), input_type=Choices])\n",
      "  return self.__pydantic_serializer__.to_python(\n",
      "Average Metric: 38.27 / 48 (79.7%):  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 47/50 [00:05<00:00, 11.19it/s]\u001b[92m12:51:21 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "2025-06-25 12:51:21,713 - LiteLLM - INFO - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "2025-06-25 12:51:22,074 - httpx - INFO - HTTP Request: POST https://api.together.xyz/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m12:51:22 - LiteLLM:INFO\u001b[0m: utils.py:1215 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-25 12:51:22,081 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m12:51:22 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "2025-06-25 12:51:22,081 - LiteLLM - INFO - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "/Users/ytchen/Documents/experimental/Llama4/env/lib/python3.13/site-packages/pydantic/main.py:463: UserWarning: Pydantic serializer warnings:\n",
      "  PydanticSerializationUnexpectedValue(Expected 9 fields but got 5: Expected `Message` - serialized value may not be as expected [input_value=Message(content='[[ ## an...ields={'refusal': None}), input_type=Message])\n",
      "  PydanticSerializationUnexpectedValue(Expected `StreamingChoices` - serialized value may not be as expected [input_value=Choices(finish_reason='st...d': 682876762352399900}), input_type=Choices])\n",
      "  return self.__pydantic_serializer__.to_python(\n",
      "Average Metric: 38.90 / 49 (79.4%):  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 49/50 [00:05<00:00,  7.12it/s]\u001b[92m12:51:22 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "2025-06-25 12:51:22,119 - LiteLLM - INFO - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "2025-06-25 12:51:22,480 - httpx - INFO - HTTP Request: POST https://api.together.xyz/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m12:51:22 - LiteLLM:INFO\u001b[0m: utils.py:1215 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-25 12:51:22,486 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m12:51:22 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "2025-06-25 12:51:22,487 - LiteLLM - INFO - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "/Users/ytchen/Documents/experimental/Llama4/env/lib/python3.13/site-packages/pydantic/main.py:463: UserWarning: Pydantic serializer warnings:\n",
      "  PydanticSerializationUnexpectedValue(Expected 9 fields but got 5: Expected `Message` - serialized value may not be as expected [input_value=Message(content='[[ ## an...ields={'refusal': None}), input_type=Message])\n",
      "  PydanticSerializationUnexpectedValue(Expected `StreamingChoices` - serialized value may not be as expected [input_value=Choices(finish_reason='st...': 2443481919589219000}), input_type=Choices])\n",
      "  return self.__pydantic_serializer__.to_python(\n",
      "Average Metric: 39.90 / 50 (79.8%): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 50/50 [00:06<00:00,  8.25it/s]\n",
      "2025/06/25 12:51:22 INFO dspy.evaluate.evaluate: Average Metric: 39.9 / 50 (79.8%)\n",
      "\u001b[92m12:51:22 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "2025-06-25 12:51:22,520 - LiteLLM - INFO - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "2025/06/25 12:51:22 INFO dspy.teleprompt.mipro_optimizer_v2: Default program score: 79.8\n",
      "\n",
      "/Users/ytchen/Documents/experimental/Llama4/env/lib/python3.13/site-packages/optuna/_experimental.py:32: ExperimentalWarning: Argument ``multivariate`` is an experimental feature. The interface can change in the future.\n",
      "  warnings.warn(\n",
      "2025/06/25 12:51:22 INFO dspy.teleprompt.mipro_optimizer_v2: ===== Trial 2 / 7 =====\n",
      "\u001b[92m12:51:22 - LiteLLM:INFO\u001b[0m: utils.py:3119 - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.3-70B-Instruct-Turbo; provider = together_ai\n",
      "2025-06-25 12:51:22,558 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.3-70B-Instruct-Turbo; provider = together_ai\n",
      "\u001b[92m12:51:22 - LiteLLM:INFO\u001b[0m: utils.py:3119 - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.3-70B-Instruct-Turbo; provider = together_ai\n",
      "2025-06-25 12:51:22,579 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.3-70B-Instruct-Turbo; provider = together_ai\n",
      "\u001b[92m12:51:22 - LiteLLM:INFO\u001b[0m: utils.py:3119 - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.3-70B-Instruct-Turbo; provider = together_ai\n",
      "2025-06-25 12:51:22,600 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.3-70B-Instruct-Turbo; provider = together_ai\n",
      "\u001b[92m12:51:22 - LiteLLM:INFO\u001b[0m: utils.py:3119 - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.3-70B-Instruct-Turbo; provider = together_ai\n",
      "2025-06-25 12:51:22,627 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.3-70B-Instruct-Turbo; provider = together_ai\n",
      "\u001b[92m12:51:22 - LiteLLM:INFO\u001b[0m: utils.py:3119 - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.3-70B-Instruct-Turbo; provider = together_ai\n",
      "2025-06-25 12:51:22,637 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.3-70B-Instruct-Turbo; provider = together_ai\n",
      "\u001b[92m12:51:22 - LiteLLM:INFO\u001b[0m: utils.py:3119 - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.3-70B-Instruct-Turbo; provider = together_ai\n",
      "2025-06-25 12:51:22,651 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.3-70B-Instruct-Turbo; provider = together_ai\n",
      "\u001b[92m12:51:22 - LiteLLM:INFO\u001b[0m: utils.py:3119 - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.3-70B-Instruct-Turbo; provider = together_ai\n",
      "2025-06-25 12:51:22,651 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.3-70B-Instruct-Turbo; provider = together_ai\n",
      "\u001b[92m12:51:22 - LiteLLM:INFO\u001b[0m: utils.py:3119 - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.3-70B-Instruct-Turbo; provider = together_ai\n",
      "2025-06-25 12:51:22,664 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.3-70B-Instruct-Turbo; provider = together_ai\n",
      "\u001b[92m12:51:22 - LiteLLM:INFO\u001b[0m: utils.py:3119 - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.3-70B-Instruct-Turbo; provider = together_ai\n",
      "2025-06-25 12:51:22,720 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.3-70B-Instruct-Turbo; provider = together_ai\n",
      "\u001b[92m12:51:22 - LiteLLM:INFO\u001b[0m: utils.py:3119 - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.3-70B-Instruct-Turbo; provider = together_ai\n",
      "2025-06-25 12:51:22,727 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.3-70B-Instruct-Turbo; provider = together_ai\n",
      "\u001b[92m12:51:22 - LiteLLM:INFO\u001b[0m: utils.py:3119 - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.3-70B-Instruct-Turbo; provider = together_ai\n",
      "2025-06-25 12:51:22,742 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.3-70B-Instruct-Turbo; provider = together_ai\n",
      "\u001b[92m12:51:22 - LiteLLM:INFO\u001b[0m: utils.py:3119 - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.3-70B-Instruct-Turbo; provider = together_ai\n",
      "2025-06-25 12:51:22,778 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.3-70B-Instruct-Turbo; provider = together_ai\n",
      "\u001b[92m12:51:22 - LiteLLM:INFO\u001b[0m: utils.py:3119 - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.3-70B-Instruct-Turbo; provider = together_ai\n",
      "2025-06-25 12:51:22,785 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.3-70B-Instruct-Turbo; provider = together_ai\n",
      "\u001b[92m12:51:22 - LiteLLM:INFO\u001b[0m: utils.py:3119 - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.3-70B-Instruct-Turbo; provider = together_ai\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]2025-06-25 12:51:22,786 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.3-70B-Instruct-Turbo; provider = together_ai\n",
      "\u001b[92m12:51:22 - LiteLLM:INFO\u001b[0m: utils.py:3119 - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.3-70B-Instruct-Turbo; provider = together_ai\n",
      "2025-06-25 12:51:22,800 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.3-70B-Instruct-Turbo; provider = together_ai\n",
      "\u001b[92m12:51:22 - LiteLLM:INFO\u001b[0m: utils.py:3119 - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.3-70B-Instruct-Turbo; provider = together_ai\n",
      "2025-06-25 12:51:22,809 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.3-70B-Instruct-Turbo; provider = together_ai\n",
      "\u001b[92m12:51:22 - LiteLLM:INFO\u001b[0m: utils.py:3119 - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.3-70B-Instruct-Turbo; provider = together_ai\n",
      "2025-06-25 12:51:22,810 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.3-70B-Instruct-Turbo; provider = together_ai\n",
      "\u001b[92m12:51:22 - LiteLLM:INFO\u001b[0m: utils.py:3119 - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.3-70B-Instruct-Turbo; provider = together_ai\n",
      "2025-06-25 12:51:22,811 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.3-70B-Instruct-Turbo; provider = together_ai\n",
      "2025-06-25 12:51:23,713 - httpx - INFO - HTTP Request: POST https://api.together.xyz/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m12:51:23 - LiteLLM:INFO\u001b[0m: utils.py:1215 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-25 12:51:23,716 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m12:51:23 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "2025-06-25 12:51:23,717 - LiteLLM - INFO - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "/Users/ytchen/Documents/experimental/Llama4/env/lib/python3.13/site-packages/pydantic/main.py:463: UserWarning: Pydantic serializer warnings:\n",
      "  PydanticSerializationUnexpectedValue(Expected 9 fields but got 5: Expected `Message` - serialized value may not be as expected [input_value=Message(content='[[ ## an...ields={'refusal': None}), input_type=Message])\n",
      "  PydanticSerializationUnexpectedValue(Expected `StreamingChoices` - serialized value may not be as expected [input_value=Choices(finish_reason='st...: 16036816691548013000}), input_type=Choices])\n",
      "  return self.__pydantic_serializer__.to_python(\n",
      "Average Metric: 0.93 / 1 (93.3%):   2%|‚ñè         | 1/50 [00:00<00:45,  1.08it/s]\u001b[92m12:51:23 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "2025-06-25 12:51:23,749 - LiteLLM - INFO - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "\u001b[92m12:51:23 - LiteLLM:INFO\u001b[0m: utils.py:3119 - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.3-70B-Instruct-Turbo; provider = together_ai\n",
      "2025-06-25 12:51:23,765 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.3-70B-Instruct-Turbo; provider = together_ai\n",
      "2025-06-25 12:51:23,976 - httpx - INFO - HTTP Request: POST https://api.together.xyz/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m12:51:23 - LiteLLM:INFO\u001b[0m: utils.py:1215 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-25 12:51:23,980 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m12:51:23 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "2025-06-25 12:51:23,981 - LiteLLM - INFO - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "/Users/ytchen/Documents/experimental/Llama4/env/lib/python3.13/site-packages/pydantic/main.py:463: UserWarning: Pydantic serializer warnings:\n",
      "  PydanticSerializationUnexpectedValue(Expected 9 fields but got 5: Expected `Message` - serialized value may not be as expected [input_value=Message(content='[[ ## an...ields={'refusal': None}), input_type=Message])\n",
      "  PydanticSerializationUnexpectedValue(Expected `StreamingChoices` - serialized value may not be as expected [input_value=Choices(finish_reason='st...': 2644059934946790000}), input_type=Choices])\n",
      "  return self.__pydantic_serializer__.to_python(\n",
      "Average Metric: 1.93 / 2 (96.7%):   2%|‚ñè         | 1/50 [00:01<00:45,  1.08it/s]\u001b[92m12:51:24 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "2025-06-25 12:51:24,020 - LiteLLM - INFO - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "\u001b[92m12:51:24 - LiteLLM:INFO\u001b[0m: utils.py:3119 - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.3-70B-Instruct-Turbo; provider = together_ai\n",
      "2025-06-25 12:51:24,022 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.3-70B-Instruct-Turbo; provider = together_ai\n",
      "Average Metric: 1.93 / 2 (96.7%):   4%|‚ñç         | 2/50 [00:01<00:26,  1.84it/s]2025-06-25 12:51:24,262 - httpx - INFO - HTTP Request: POST https://api.together.xyz/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-06-25 12:51:24,264 - httpx - INFO - HTTP Request: POST https://api.together.xyz/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-06-25 12:51:24,265 - httpx - INFO - HTTP Request: POST https://api.together.xyz/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m12:51:24 - LiteLLM:INFO\u001b[0m: utils.py:1215 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-25 12:51:24,267 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m12:51:24 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "2025-06-25 12:51:24,268 - LiteLLM - INFO - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "/Users/ytchen/Documents/experimental/Llama4/env/lib/python3.13/site-packages/pydantic/main.py:463: UserWarning: Pydantic serializer warnings:\n",
      "  PydanticSerializationUnexpectedValue(Expected 9 fields but got 5: Expected `Message` - serialized value may not be as expected [input_value=Message(content='[[ ## an...ields={'refusal': None}), input_type=Message])\n",
      "  PydanticSerializationUnexpectedValue(Expected `StreamingChoices` - serialized value may not be as expected [input_value=Choices(finish_reason='st...: 18429168820844670000}), input_type=Choices])\n",
      "  return self.__pydantic_serializer__.to_python(\n",
      "Average Metric: 2.93 / 3 (97.8%):   6%|‚ñå         | 3/50 [00:01<00:19,  2.40it/s]\u001b[92m12:51:24 - LiteLLM:INFO\u001b[0m: utils.py:1215 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-25 12:51:24,306 - httpx - INFO - HTTP Request: POST https://api.together.xyz/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m12:51:24 - LiteLLM:INFO\u001b[0m: utils.py:3119 - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.3-70B-Instruct-Turbo; provider = together_ai\n",
      "2025-06-25 12:51:24,295 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m12:51:24 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "2025-06-25 12:51:24,299 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.3-70B-Instruct-Turbo; provider = together_ai\n",
      "\u001b[92m12:51:24 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "2025-06-25 12:51:24,308 - LiteLLM - INFO - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "\u001b[92m12:51:24 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "2025-06-25 12:51:24,309 - LiteLLM - INFO - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "Average Metric: 3.60 / 4 (90.0%):   6%|‚ñå         | 3/50 [00:01<00:19,  2.40it/s]\u001b[92m12:51:24 - LiteLLM:INFO\u001b[0m: utils.py:1215 - Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m12:51:24 - LiteLLM:INFO\u001b[0m: utils.py:1215 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-25 12:51:24,315 - LiteLLM - INFO - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "/Users/ytchen/Documents/experimental/Llama4/env/lib/python3.13/site-packages/pydantic/main.py:463: UserWarning: Pydantic serializer warnings:\n",
      "  PydanticSerializationUnexpectedValue(Expected 9 fields but got 5: Expected `Message` - serialized value may not be as expected [input_value=Message(content='[[ ## an...ields={'refusal': None}), input_type=Message])\n",
      "  PydanticSerializationUnexpectedValue(Expected `StreamingChoices` - serialized value may not be as expected [input_value=Choices(finish_reason='st...': 5202564380449300000}), input_type=Choices])\n",
      "  return self.__pydantic_serializer__.to_python(\n",
      "\u001b[92m12:51:24 - LiteLLM:INFO\u001b[0m: utils.py:3119 - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.3-70B-Instruct-Turbo; provider = together_ai\n",
      "\u001b[92m12:51:24 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "2025-06-25 12:51:24,316 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m12:51:24 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "\u001b[92m12:51:24 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "2025-06-25 12:51:24,332 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m12:51:24 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "2025-06-25 12:51:24,342 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.3-70B-Instruct-Turbo; provider = together_ai\n",
      "2025-06-25 12:51:24,349 - LiteLLM - INFO - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "\u001b[92m12:51:24 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "2025-06-25 12:51:24,349 - LiteLLM - INFO - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "Average Metric: 4.57 / 5 (91.3%):   8%|‚ñä         | 4/50 [00:01<00:19,  2.40it/s]2025-06-25 12:51:24,349 - httpx - INFO - HTTP Request: POST https://api.together.xyz/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m12:51:24 - LiteLLM:INFO\u001b[0m: utils.py:3119 - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.3-70B-Instruct-Turbo; provider = together_ai\n",
      "2025-06-25 12:51:24,349 - LiteLLM - INFO - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "/Users/ytchen/Documents/experimental/Llama4/env/lib/python3.13/site-packages/pydantic/main.py:463: UserWarning: Pydantic serializer warnings:\n",
      "  PydanticSerializationUnexpectedValue(Expected 9 fields but got 5: Expected `Message` - serialized value may not be as expected [input_value=Message(content='[[ ## an...ields={'refusal': None}), input_type=Message])\n",
      "  PydanticSerializationUnexpectedValue(Expected `StreamingChoices` - serialized value may not be as expected [input_value=Choices(finish_reason='st...': 4227358882708405000}), input_type=Choices])\n",
      "  return self.__pydantic_serializer__.to_python(\n",
      "\u001b[92m12:51:24 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "2025-06-25 12:51:24,350 - LiteLLM - INFO - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "/Users/ytchen/Documents/experimental/Llama4/env/lib/python3.13/site-packages/pydantic/main.py:463: UserWarning: Pydantic serializer warnings:\n",
      "  PydanticSerializationUnexpectedValue(Expected 9 fields but got 5: Expected `Message` - serialized value may not be as expected [input_value=Message(content='[[ ## an...ields={'refusal': None}), input_type=Message])\n",
      "  PydanticSerializationUnexpectedValue(Expected `StreamingChoices` - serialized value may not be as expected [input_value=Choices(finish_reason='st...': 1714539173882853400}), input_type=Choices])\n",
      "  return self.__pydantic_serializer__.to_python(\n",
      "2025-06-25 12:51:24,362 - LiteLLM - INFO - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "\u001b[92m12:51:24 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "2025-06-25 12:51:24,373 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.3-70B-Instruct-Turbo; provider = together_ai\n",
      "Average Metric: 5.53 / 6 (92.2%):  12%|‚ñà‚ñè        | 6/50 [00:01<00:07,  6.03it/s]\u001b[92m12:51:24 - LiteLLM:INFO\u001b[0m: utils.py:3119 - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.3-70B-Instruct-Turbo; provider = together_ai\n",
      "2025-06-25 12:51:24,379 - LiteLLM - INFO - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "2025-06-25 12:51:24,393 - LiteLLM - INFO - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "\u001b[92m12:51:24 - LiteLLM:INFO\u001b[0m: utils.py:1215 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-25 12:51:24,402 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.3-70B-Instruct-Turbo; provider = together_ai\n",
      "2025-06-25 12:51:24,411 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m12:51:24 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "2025-06-25 12:51:24,424 - LiteLLM - INFO - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "\u001b[92m12:51:24 - LiteLLM:INFO\u001b[0m: utils.py:3119 - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.3-70B-Instruct-Turbo; provider = together_ai\n",
      "2025-06-25 12:51:24,433 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.3-70B-Instruct-Turbo; provider = together_ai\n",
      "/Users/ytchen/Documents/experimental/Llama4/env/lib/python3.13/site-packages/pydantic/main.py:463: UserWarning: Pydantic serializer warnings:\n",
      "  PydanticSerializationUnexpectedValue(Expected 9 fields but got 5: Expected `Message` - serialized value may not be as expected [input_value=Message(content='[[ ## an...ields={'refusal': None}), input_type=Message])\n",
      "  PydanticSerializationUnexpectedValue(Expected `StreamingChoices` - serialized value may not be as expected [input_value=Choices(finish_reason='st...: 16212035675472325000}), input_type=Choices])\n",
      "  return self.__pydantic_serializer__.to_python(\n",
      "Average Metric: 6.50 / 7 (92.9%):  12%|‚ñà‚ñè        | 6/50 [00:01<00:07,  6.03it/s]\u001b[92m12:51:24 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "2025-06-25 12:51:24,448 - LiteLLM - INFO - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "2025-06-25 12:51:24,455 - httpx - INFO - HTTP Request: POST https://api.together.xyz/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-06-25 12:51:24,456 - httpx - INFO - HTTP Request: POST https://api.together.xyz/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m12:51:24 - LiteLLM:INFO\u001b[0m: utils.py:1215 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-25 12:51:24,457 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m12:51:24 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "2025-06-25 12:51:24,457 - LiteLLM - INFO - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "Average Metric: 7.10 / 8 (88.8%):  14%|‚ñà‚ñç        | 7/50 [00:01<00:07,  6.03it/s]\u001b[92m12:51:24 - LiteLLM:INFO\u001b[0m: utils.py:3119 - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.3-70B-Instruct-Turbo; provider = together_ai\n",
      "2025-06-25 12:51:24,464 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.3-70B-Instruct-Turbo; provider = together_ai\n",
      "/Users/ytchen/Documents/experimental/Llama4/env/lib/python3.13/site-packages/pydantic/main.py:463: UserWarning: Pydantic serializer warnings:\n",
      "  PydanticSerializationUnexpectedValue(Expected 9 fields but got 5: Expected `Message` - serialized value may not be as expected [input_value=Message(content='[[ ## an...ields={'refusal': None}), input_type=Message])\n",
      "  PydanticSerializationUnexpectedValue(Expected `StreamingChoices` - serialized value may not be as expected [input_value=Choices(finish_reason='st...': 3996740838920483300}), input_type=Choices])\n",
      "  return self.__pydantic_serializer__.to_python(\n",
      "\u001b[92m12:51:24 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "2025-06-25 12:51:24,477 - LiteLLM - INFO - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "\u001b[92m12:51:24 - LiteLLM:INFO\u001b[0m: utils.py:1215 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-25 12:51:24,486 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m12:51:24 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "2025-06-25 12:51:24,486 - LiteLLM - INFO - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "/Users/ytchen/Documents/experimental/Llama4/env/lib/python3.13/site-packages/pydantic/main.py:463: UserWarning: Pydantic serializer warnings:\n",
      "  PydanticSerializationUnexpectedValue(Expected 9 fields but got 5: Expected `Message` - serialized value may not be as expected [input_value=Message(content='[[ ## an...ields={'refusal': None}), input_type=Message])\n",
      "  PydanticSerializationUnexpectedValue(Expected `StreamingChoices` - serialized value may not be as expected [input_value=Choices(finish_reason='st...: 12505115989686526000}), input_type=Choices])\n",
      "  return self.__pydantic_serializer__.to_python(\n",
      "\u001b[92m12:51:24 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "2025-06-25 12:51:24,499 - LiteLLM - INFO - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "\u001b[92m12:51:24 - LiteLLM:INFO\u001b[0m: utils.py:3119 - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.3-70B-Instruct-Turbo; provider = together_ai\n",
      "2025-06-25 12:51:24,506 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.3-70B-Instruct-Turbo; provider = together_ai\n",
      "Average Metric: 8.07 / 9 (89.6%):  18%|‚ñà‚ñä        | 9/50 [00:01<00:04,  9.53it/s]2025-06-25 12:51:24,539 - httpx - INFO - HTTP Request: POST https://api.together.xyz/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m12:51:24 - LiteLLM:INFO\u001b[0m: utils.py:1215 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-25 12:51:24,549 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m12:51:24 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "2025-06-25 12:51:24,550 - LiteLLM - INFO - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "Average Metric: 9.00 / 10 (90.0%):  18%|‚ñà‚ñå       | 9/50 [00:01<00:04,  9.53it/s]/Users/ytchen/Documents/experimental/Llama4/env/lib/python3.13/site-packages/pydantic/main.py:463: UserWarning: Pydantic serializer warnings:\n",
      "  PydanticSerializationUnexpectedValue(Expected 9 fields but got 5: Expected `Message` - serialized value may not be as expected [input_value=Message(content='[[ ## an...ields={'refusal': None}), input_type=Message])\n",
      "  PydanticSerializationUnexpectedValue(Expected `StreamingChoices` - serialized value may not be as expected [input_value=Choices(finish_reason='st...': 2169111684043358200}), input_type=Choices])\n",
      "  return self.__pydantic_serializer__.to_python(\n",
      "2025-06-25 12:51:24,570 - httpx - INFO - HTTP Request: POST https://api.together.xyz/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m12:51:24 - LiteLLM:INFO\u001b[0m: utils.py:3119 - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.3-70B-Instruct-Turbo; provider = together_ai\n",
      "2025-06-25 12:51:24,575 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.3-70B-Instruct-Turbo; provider = together_ai\n",
      "\u001b[92m12:51:24 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "2025-06-25 12:51:24,575 - httpx - INFO - HTTP Request: POST https://api.together.xyz/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-06-25 12:51:24,581 - httpx - INFO - HTTP Request: POST https://api.together.xyz/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-06-25 12:51:24,589 - httpx - INFO - HTTP Request: POST https://api.together.xyz/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-25 12:51:24,589 - httpx - INFO - HTTP Request: POST https://api.together.xyz/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-06-25 12:51:24,589 - httpx - INFO - HTTP Request: POST https://api.together.xyz/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-25 12:51:24,580 - LiteLLM - INFO - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "\u001b[92m12:51:24 - LiteLLM:INFO\u001b[0m: utils.py:1215 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-25 12:51:24,597 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m12:51:24 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "2025-06-25 12:51:24,598 - LiteLLM - INFO - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "Average Metric: 9.97 / 11 (90.6%):  20%|‚ñà‚ñå      | 10/50 [00:01<00:04,  9.53it/s]/Users/ytchen/Documents/experimental/Llama4/env/lib/python3.13/site-packages/pydantic/main.py:463: UserWarning: Pydantic serializer warnings:\n",
      "  PydanticSerializationUnexpectedValue(Expected 9 fields but got 5: Expected `Message` - serialized value may not be as expected [input_value=Message(content='[[ ## an...ields={'refusal': None}), input_type=Message])\n",
      "  PydanticSerializationUnexpectedValue(Expected `StreamingChoices` - serialized value may not be as expected [input_value=Choices(finish_reason='st...: 16153483829435904000}), input_type=Choices])\n",
      "  return self.__pydantic_serializer__.to_python(\n",
      "\u001b[92m12:51:24 - LiteLLM:INFO\u001b[0m: utils.py:3119 - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.3-70B-Instruct-Turbo; provider = together_ai\n",
      "2025-06-25 12:51:24,612 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.3-70B-Instruct-Turbo; provider = together_ai\n",
      "\u001b[92m12:51:24 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "2025-06-25 12:51:24,622 - LiteLLM - INFO - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "\u001b[92m12:51:24 - LiteLLM:INFO\u001b[0m: utils.py:1215 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-25 12:51:24,629 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m12:51:24 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "2025-06-25 12:51:24,629 - LiteLLM - INFO - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "/Users/ytchen/Documents/experimental/Llama4/env/lib/python3.13/site-packages/pydantic/main.py:463: UserWarning: Pydantic serializer warnings:\n",
      "  PydanticSerializationUnexpectedValue(Expected 9 fields but got 5: Expected `Message` - serialized value may not be as expected [input_value=Message(content='[[ ## an...ields={'refusal': None}), input_type=Message])\n",
      "  PydanticSerializationUnexpectedValue(Expected `StreamingChoices` - serialized value may not be as expected [input_value=Choices(finish_reason='st...: 15938412407166056000}), input_type=Choices])\n",
      "  return self.__pydantic_serializer__.to_python(\n",
      "Average Metric: 10.93 / 12 (91.1%):  24%|‚ñà‚ñã     | 12/50 [00:01<00:03, 12.59it/s]2025-06-25 12:51:24,643 - httpx - INFO - HTTP Request: POST https://api.together.xyz/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "\u001b[92m12:51:24 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "2025-06-25 12:51:24,644 - LiteLLM - INFO - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "\u001b[92m12:51:24 - LiteLLM:INFO\u001b[0m: utils.py:3119 - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.3-70B-Instruct-Turbo; provider = together_ai\n",
      "2025-06-25 12:51:24,650 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.3-70B-Instruct-Turbo; provider = together_ai\n",
      "2025-06-25 12:51:24,657 - httpx - INFO - HTTP Request: POST https://api.together.xyz/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "\u001b[92m12:51:24 - LiteLLM:INFO\u001b[0m: utils.py:3119 - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.3-70B-Instruct-Turbo; provider = together_ai\n",
      "2025-06-25 12:51:24,666 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.3-70B-Instruct-Turbo; provider = together_ai\n",
      "\u001b[92m12:51:24 - LiteLLM:INFO\u001b[0m: utils.py:1215 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-25 12:51:24,674 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m12:51:24 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "2025-06-25 12:51:24,675 - LiteLLM - INFO - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "\u001b[92m12:51:24 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "2025-06-25 12:51:24,675 - LiteLLM - INFO - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "Average Metric: 11.93 / 13 (91.8%):  24%|‚ñà‚ñã     | 12/50 [00:01<00:03, 12.59it/s]/Users/ytchen/Documents/experimental/Llama4/env/lib/python3.13/site-packages/pydantic/main.py:463: UserWarning: Pydantic serializer warnings:\n",
      "  PydanticSerializationUnexpectedValue(Expected 9 fields but got 5: Expected `Message` - serialized value may not be as expected [input_value=Message(content='[[ ## an...ields={'refusal': None}), input_type=Message])\n",
      "  PydanticSerializationUnexpectedValue(Expected `StreamingChoices` - serialized value may not be as expected [input_value=Choices(finish_reason='st...: 10525621495191210000}), input_type=Choices])\n",
      "  return self.__pydantic_serializer__.to_python(\n",
      "\u001b[92m12:51:24 - LiteLLM:INFO\u001b[0m: utils.py:3119 - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.3-70B-Instruct-Turbo; provider = together_ai\n",
      "2025-06-25 12:51:24,688 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.3-70B-Instruct-Turbo; provider = together_ai\n",
      "\u001b[92m12:51:24 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "2025-06-25 12:51:24,689 - LiteLLM - INFO - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "\u001b[92m12:51:24 - LiteLLM:INFO\u001b[0m: utils.py:1215 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-25 12:51:24,697 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m12:51:24 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "2025-06-25 12:51:24,704 - httpx - INFO - HTTP Request: POST https://api.together.xyz/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "\u001b[92m12:51:24 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "2025-06-25 12:51:24,704 - LiteLLM - INFO - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "2025-06-25 12:51:24,704 - LiteLLM - INFO - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "Average Metric: 12.93 / 14 (92.4%):  26%|‚ñà‚ñä     | 13/50 [00:01<00:02, 12.59it/s]\u001b[92m12:51:24 - LiteLLM:INFO\u001b[0m: utils.py:3119 - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.3-70B-Instruct-Turbo; provider = together_ai\n",
      "/Users/ytchen/Documents/experimental/Llama4/env/lib/python3.13/site-packages/pydantic/main.py:463: UserWarning: Pydantic serializer warnings:\n",
      "  PydanticSerializationUnexpectedValue(Expected 9 fields but got 5: Expected `Message` - serialized value may not be as expected [input_value=Message(content='[[ ## an...ields={'refusal': None}), input_type=Message])\n",
      "  PydanticSerializationUnexpectedValue(Expected `StreamingChoices` - serialized value may not be as expected [input_value=Choices(finish_reason='st...: 15533462685725938000}), input_type=Choices])\n",
      "  return self.__pydantic_serializer__.to_python(\n",
      "2025-06-25 12:51:24,712 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.3-70B-Instruct-Turbo; provider = together_ai\n",
      "\u001b[92m12:51:24 - LiteLLM:INFO\u001b[0m: utils.py:3119 - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.3-70B-Instruct-Turbo; provider = together_ai\n",
      "2025-06-25 12:51:24,734 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.3-70B-Instruct-Turbo; provider = together_ai\n",
      "\u001b[92m12:51:24 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "2025-06-25 12:51:24,740 - LiteLLM - INFO - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "2025-06-25 12:51:24,748 - httpx - INFO - HTTP Request: POST https://api.together.xyz/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "\u001b[92m12:51:24 - LiteLLM:INFO\u001b[0m: utils.py:3119 - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.3-70B-Instruct-Turbo; provider = together_ai\n",
      "2025-06-25 12:51:24,756 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.3-70B-Instruct-Turbo; provider = together_ai\n",
      "\u001b[92m12:51:24 - LiteLLM:INFO\u001b[0m: utils.py:3119 - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.3-70B-Instruct-Turbo; provider = together_ai\n",
      "2025-06-25 12:51:24,770 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.3-70B-Instruct-Turbo; provider = together_ai\n",
      "\u001b[92m12:51:24 - LiteLLM:INFO\u001b[0m: utils.py:3119 - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.3-70B-Instruct-Turbo; provider = together_ai\n",
      "2025-06-25 12:51:24,785 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.3-70B-Instruct-Turbo; provider = together_ai\n",
      "\u001b[92m12:51:24 - LiteLLM:INFO\u001b[0m: utils.py:3119 - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.3-70B-Instruct-Turbo; provider = together_ai\n",
      "2025-06-25 12:51:24,801 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.3-70B-Instruct-Turbo; provider = together_ai\n",
      "2025-06-25 12:51:24,888 - httpx - INFO - HTTP Request: POST https://api.together.xyz/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m12:51:24 - LiteLLM:INFO\u001b[0m: utils.py:1215 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-25 12:51:24,889 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m12:51:24 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "2025-06-25 12:51:24,889 - LiteLLM - INFO - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "Average Metric: 13.60 / 15 (90.7%):  30%|‚ñà‚ñà     | 15/50 [00:02<00:02, 12.30it/s]/Users/ytchen/Documents/experimental/Llama4/env/lib/python3.13/site-packages/pydantic/main.py:463: UserWarning: Pydantic serializer warnings:\n",
      "  PydanticSerializationUnexpectedValue(Expected 9 fields but got 5: Expected `Message` - serialized value may not be as expected [input_value=Message(content='[[ ## an...ields={'refusal': None}), input_type=Message])\n",
      "  PydanticSerializationUnexpectedValue(Expected `StreamingChoices` - serialized value may not be as expected [input_value=Choices(finish_reason='st...: 14804508204040806000}), input_type=Choices])\n",
      "  return self.__pydantic_serializer__.to_python(\n",
      "\u001b[92m12:51:24 - LiteLLM:INFO\u001b[0m: utils.py:3119 - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.3-70B-Instruct-Turbo; provider = together_ai\n",
      "2025-06-25 12:51:24,905 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.3-70B-Instruct-Turbo; provider = together_ai\n",
      "\u001b[92m12:51:24 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "2025-06-25 12:51:24,913 - LiteLLM - INFO - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "2025-06-25 12:51:24,921 - httpx - INFO - HTTP Request: POST https://api.together.xyz/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-25 12:51:24,924 - httpx - INFO - HTTP Request: POST https://api.together.xyz/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-25 12:51:24,924 - httpx - INFO - HTTP Request: POST https://api.together.xyz/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-25 12:51:24,924 - httpx - INFO - HTTP Request: POST https://api.together.xyz/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "\u001b[92m12:51:24 - LiteLLM:INFO\u001b[0m: utils.py:3119 - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.3-70B-Instruct-Turbo; provider = together_ai\n",
      "2025-06-25 12:51:24,931 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.3-70B-Instruct-Turbo; provider = together_ai\n",
      "2025-06-25 12:51:24,939 - httpx - INFO - HTTP Request: POST https://api.together.xyz/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-25 12:51:24,940 - httpx - INFO - HTTP Request: POST https://api.together.xyz/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-25 12:51:24,940 - httpx - INFO - HTTP Request: POST https://api.together.xyz/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-25 12:51:24,940 - httpx - INFO - HTTP Request: POST https://api.together.xyz/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "\u001b[92m12:51:24 - LiteLLM:INFO\u001b[0m: utils.py:3119 - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.3-70B-Instruct-Turbo; provider = together_ai\n",
      "2025-06-25 12:51:24,955 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.3-70B-Instruct-Turbo; provider = together_ai\n",
      "2025-06-25 12:51:24,964 - httpx - INFO - HTTP Request: POST https://api.together.xyz/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m12:51:24 - LiteLLM:INFO\u001b[0m: utils.py:3119 - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.3-70B-Instruct-Turbo; provider = together_ai\n",
      "2025-06-25 12:51:24,970 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.3-70B-Instruct-Turbo; provider = together_ai\n",
      "\u001b[92m12:51:24 - LiteLLM:INFO\u001b[0m: utils.py:3119 - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.3-70B-Instruct-Turbo; provider = together_ai\n",
      "2025-06-25 12:51:24,994 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.3-70B-Instruct-Turbo; provider = together_ai\n",
      "\u001b[92m12:51:25 - LiteLLM:INFO\u001b[0m: utils.py:3119 - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.3-70B-Instruct-Turbo; provider = together_ai\n",
      "2025-06-25 12:51:25,018 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.3-70B-Instruct-Turbo; provider = together_ai\n",
      "\u001b[92m12:51:25 - LiteLLM:INFO\u001b[0m: utils.py:3119 - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.3-70B-Instruct-Turbo; provider = together_ai\n",
      "2025-06-25 12:51:25,034 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.3-70B-Instruct-Turbo; provider = together_ai\n",
      "2025-06-25 12:51:25,040 - httpx - INFO - HTTP Request: POST https://api.together.xyz/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m12:51:25 - LiteLLM:INFO\u001b[0m: utils.py:3119 - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.3-70B-Instruct-Turbo; provider = together_ai\n",
      "2025-06-25 12:51:25,057 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.3-70B-Instruct-Turbo; provider = together_ai\n",
      "\u001b[92m12:51:25 - LiteLLM:INFO\u001b[0m: utils.py:3119 - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.3-70B-Instruct-Turbo; provider = together_ai\n",
      "2025-06-25 12:51:25,080 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.3-70B-Instruct-Turbo; provider = together_ai\n",
      "\u001b[92m12:51:25 - LiteLLM:INFO\u001b[0m: utils.py:1215 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-25 12:51:25,088 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m12:51:25 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "2025-06-25 12:51:25,088 - LiteLLM - INFO - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "/Users/ytchen/Documents/experimental/Llama4/env/lib/python3.13/site-packages/pydantic/main.py:463: UserWarning: Pydantic serializer warnings:\n",
      "  PydanticSerializationUnexpectedValue(Expected 9 fields but got 5: Expected `Message` - serialized value may not be as expected [input_value=Message(content='[[ ## an...ields={'refusal': None}), input_type=Message])\n",
      "  PydanticSerializationUnexpectedValue(Expected `StreamingChoices` - serialized value may not be as expected [input_value=Choices(finish_reason='st...: 13534481275662360000}), input_type=Choices])\n",
      "  return self.__pydantic_serializer__.to_python(\n",
      "Average Metric: 14.60 / 16 (91.2%):  30%|‚ñà‚ñà     | 15/50 [00:02<00:02, 12.30it/s]\u001b[92m12:51:25 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "2025-06-25 12:51:25,102 - LiteLLM - INFO - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "\u001b[92m12:51:25 - LiteLLM:INFO\u001b[0m: utils.py:3119 - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.3-70B-Instruct-Turbo; provider = together_ai\n",
      "2025-06-25 12:51:25,109 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.3-70B-Instruct-Turbo; provider = together_ai\n",
      "\u001b[92m12:51:25 - LiteLLM:INFO\u001b[0m: utils.py:1215 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-25 12:51:25,118 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m12:51:25 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "2025-06-25 12:51:25,118 - LiteLLM - INFO - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "/Users/ytchen/Documents/experimental/Llama4/env/lib/python3.13/site-packages/pydantic/main.py:463: UserWarning: Pydantic serializer warnings:\n",
      "  PydanticSerializationUnexpectedValue(Expected 9 fields but got 5: Expected `Message` - serialized value may not be as expected [input_value=Message(content='[[ ## an...ields={'refusal': None}), input_type=Message])\n",
      "  PydanticSerializationUnexpectedValue(Expected `StreamingChoices` - serialized value may not be as expected [input_value=Choices(finish_reason='st...': 1007543644104305300}), input_type=Choices])\n",
      "  return self.__pydantic_serializer__.to_python(\n",
      "Average Metric: 14.93 / 17 (87.8%):  34%|‚ñà‚ñà‚ñç    | 17/50 [00:02<00:02, 11.00it/s]\u001b[92m12:51:25 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "2025-06-25 12:51:25,132 - LiteLLM - INFO - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "\u001b[92m12:51:25 - LiteLLM:INFO\u001b[0m: utils.py:3119 - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.3-70B-Instruct-Turbo; provider = together_ai\n",
      "2025-06-25 12:51:25,132 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.3-70B-Instruct-Turbo; provider = together_ai\n",
      "2025-06-25 12:51:25,200 - httpx - INFO - HTTP Request: POST https://api.together.xyz/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m12:51:25 - LiteLLM:INFO\u001b[0m: utils.py:1215 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-25 12:51:25,200 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m12:51:25 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "2025-06-25 12:51:25,200 - LiteLLM - INFO - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "/Users/ytchen/Documents/experimental/Llama4/env/lib/python3.13/site-packages/pydantic/main.py:463: UserWarning: Pydantic serializer warnings:\n",
      "  PydanticSerializationUnexpectedValue(Expected 9 fields but got 5: Expected `Message` - serialized value may not be as expected [input_value=Message(content='[[ ## an...ields={'refusal': None}), input_type=Message])\n",
      "  PydanticSerializationUnexpectedValue(Expected `StreamingChoices` - serialized value may not be as expected [input_value=Choices(finish_reason='st...': 4143605985980137000}), input_type=Choices])\n",
      "  return self.__pydantic_serializer__.to_python(\n",
      "Average Metric: 15.87 / 18 (88.1%):  34%|‚ñà‚ñà‚ñç    | 17/50 [00:02<00:02, 11.00it/s]\u001b[92m12:51:25 - LiteLLM:INFO\u001b[0m: utils.py:3119 - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.3-70B-Instruct-Turbo; provider = together_ai\n",
      "2025-06-25 12:51:25,208 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.3-70B-Instruct-Turbo; provider = together_ai\n",
      "\u001b[92m12:51:25 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "2025-06-25 12:51:25,221 - LiteLLM - INFO - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "2025-06-25 12:51:25,268 - httpx - INFO - HTTP Request: POST https://api.together.xyz/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m12:51:25 - LiteLLM:INFO\u001b[0m: utils.py:1215 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-25 12:51:25,269 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m12:51:25 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "2025-06-25 12:51:25,269 - LiteLLM - INFO - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "Average Metric: 16.83 / 19 (88.6%):  38%|‚ñà‚ñà‚ñã    | 19/50 [00:02<00:02, 11.68it/s]/Users/ytchen/Documents/experimental/Llama4/env/lib/python3.13/site-packages/pydantic/main.py:463: UserWarning: Pydantic serializer warnings:\n",
      "  PydanticSerializationUnexpectedValue(Expected 9 fields but got 5: Expected `Message` - serialized value may not be as expected [input_value=Message(content='[[ ## an...ields={'refusal': None}), input_type=Message])\n",
      "  PydanticSerializationUnexpectedValue(Expected `StreamingChoices` - serialized value may not be as expected [input_value=Choices(finish_reason='st...d': 816534491078905700}), input_type=Choices])\n",
      "  return self.__pydantic_serializer__.to_python(\n",
      "\u001b[92m12:51:25 - LiteLLM:INFO\u001b[0m: utils.py:3119 - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.3-70B-Instruct-Turbo; provider = together_ai\n",
      "2025-06-25 12:51:25,283 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.3-70B-Instruct-Turbo; provider = together_ai\n",
      "\u001b[92m12:51:25 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "2025-06-25 12:51:25,290 - LiteLLM - INFO - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "2025-06-25 12:51:25,450 - httpx - INFO - HTTP Request: POST https://api.together.xyz/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m12:51:25 - LiteLLM:INFO\u001b[0m: utils.py:1215 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-25 12:51:25,451 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m12:51:25 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "2025-06-25 12:51:25,451 - LiteLLM - INFO - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "/Users/ytchen/Documents/experimental/Llama4/env/lib/python3.13/site-packages/pydantic/main.py:463: UserWarning: Pydantic serializer warnings:\n",
      "  PydanticSerializationUnexpectedValue(Expected 9 fields but got 5: Expected `Message` - serialized value may not be as expected [input_value=Message(content='[[ ## an...ields={'refusal': None}), input_type=Message])\n",
      "  PydanticSerializationUnexpectedValue(Expected `StreamingChoices` - serialized value may not be as expected [input_value=Choices(finish_reason='st...': 3907539565438215700}), input_type=Choices])\n",
      "  return self.__pydantic_serializer__.to_python(\n",
      "\u001b[92m12:51:25 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "2025-06-25 12:51:25,461 - LiteLLM - INFO - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "2025-06-25 12:51:25,468 - httpx - INFO - HTTP Request: POST https://api.together.xyz/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Average Metric: 17.77 / 20 (88.8%):  38%|‚ñà‚ñà‚ñã    | 19/50 [00:02<00:02, 11.68it/s]\u001b[92m12:51:25 - LiteLLM:INFO\u001b[0m: utils.py:3119 - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.3-70B-Instruct-Turbo; provider = together_ai\n",
      "2025-06-25 12:51:25,483 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.3-70B-Instruct-Turbo; provider = together_ai\n",
      "\u001b[92m12:51:25 - LiteLLM:INFO\u001b[0m: utils.py:1215 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-25 12:51:25,495 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m12:51:25 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "2025-06-25 12:51:25,495 - LiteLLM - INFO - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "Average Metric: 18.70 / 21 (89.0%):  42%|‚ñà‚ñà‚ñâ    | 21/50 [00:02<00:02, 10.71it/s]\u001b[92m12:51:25 - LiteLLM:INFO\u001b[0m: utils.py:3119 - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.3-70B-Instruct-Turbo; provider = together_ai\n",
      "2025-06-25 12:51:25,504 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.3-70B-Instruct-Turbo; provider = together_ai\n",
      "/Users/ytchen/Documents/experimental/Llama4/env/lib/python3.13/site-packages/pydantic/main.py:463: UserWarning: Pydantic serializer warnings:\n",
      "  PydanticSerializationUnexpectedValue(Expected 9 fields but got 5: Expected `Message` - serialized value may not be as expected [input_value=Message(content='[[ ## an...ields={'refusal': None}), input_type=Message])\n",
      "  PydanticSerializationUnexpectedValue(Expected `StreamingChoices` - serialized value may not be as expected [input_value=Choices(finish_reason='st...': 3181777190392778000}), input_type=Choices])\n",
      "  return self.__pydantic_serializer__.to_python(\n",
      "\u001b[92m12:51:25 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "2025-06-25 12:51:25,519 - LiteLLM - INFO - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "2025-06-25 12:51:25,662 - httpx - INFO - HTTP Request: POST https://api.together.xyz/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m12:51:25 - LiteLLM:INFO\u001b[0m: utils.py:1215 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-25 12:51:25,663 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m12:51:25 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "2025-06-25 12:51:25,664 - LiteLLM - INFO - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "/Users/ytchen/Documents/experimental/Llama4/env/lib/python3.13/site-packages/pydantic/main.py:463: UserWarning: Pydantic serializer warnings:\n",
      "  PydanticSerializationUnexpectedValue(Expected 9 fields but got 5: Expected `Message` - serialized value may not be as expected [input_value=Message(content='[[ ## an...ields={'refusal': None}), input_type=Message])\n",
      "  PydanticSerializationUnexpectedValue(Expected `StreamingChoices` - serialized value may not be as expected [input_value=Choices(finish_reason='st...': 1230981236615017000}), input_type=Choices])\n",
      "  return self.__pydantic_serializer__.to_python(\n",
      "Average Metric: 19.67 / 22 (89.4%):  42%|‚ñà‚ñà‚ñâ    | 21/50 [00:02<00:02, 10.71it/s]\u001b[92m12:51:25 - LiteLLM:INFO\u001b[0m: utils.py:3119 - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.3-70B-Instruct-Turbo; provider = together_ai\n",
      "2025-06-25 12:51:25,683 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.3-70B-Instruct-Turbo; provider = together_ai\n",
      "\u001b[92m12:51:25 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "2025-06-25 12:51:25,693 - LiteLLM - INFO - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "2025-06-25 12:51:25,772 - httpx - INFO - HTTP Request: POST https://api.together.xyz/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m12:51:25 - LiteLLM:INFO\u001b[0m: utils.py:1215 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-25 12:51:25,773 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m12:51:25 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "2025-06-25 12:51:25,773 - LiteLLM - INFO - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "/Users/ytchen/Documents/experimental/Llama4/env/lib/python3.13/site-packages/pydantic/main.py:463: UserWarning: Pydantic serializer warnings:\n",
      "  PydanticSerializationUnexpectedValue(Expected 9 fields but got 5: Expected `Message` - serialized value may not be as expected [input_value=Message(content='[[ ## an...ields={'refusal': None}), input_type=Message])\n",
      "  PydanticSerializationUnexpectedValue(Expected `StreamingChoices` - serialized value may not be as expected [input_value=Choices(finish_reason='st...: 15839701024543998000}), input_type=Choices])\n",
      "  return self.__pydantic_serializer__.to_python(\n",
      "Average Metric: 20.00 / 23 (87.0%):  44%|‚ñà‚ñà‚ñà    | 22/50 [00:02<00:02, 10.71it/s]\u001b[92m12:51:25 - LiteLLM:INFO\u001b[0m: utils.py:3119 - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.3-70B-Instruct-Turbo; provider = together_ai\n",
      "2025-06-25 12:51:25,789 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.3-70B-Instruct-Turbo; provider = together_ai\n",
      "Average Metric: 20.00 / 23 (87.0%):  46%|‚ñà‚ñà‚ñà‚ñè   | 23/50 [00:02<00:02,  9.22it/s]\u001b[92m12:51:25 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "2025-06-25 12:51:25,798 - LiteLLM - INFO - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "2025-06-25 12:51:25,923 - httpx - INFO - HTTP Request: POST https://api.together.xyz/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m12:51:25 - LiteLLM:INFO\u001b[0m: utils.py:1215 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-25 12:51:25,924 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m12:51:25 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "2025-06-25 12:51:25,924 - LiteLLM - INFO - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "/Users/ytchen/Documents/experimental/Llama4/env/lib/python3.13/site-packages/pydantic/main.py:463: UserWarning: Pydantic serializer warnings:\n",
      "  PydanticSerializationUnexpectedValue(Expected 9 fields but got 5: Expected `Message` - serialized value may not be as expected [input_value=Message(content='[[ ## an...ields={'refusal': None}), input_type=Message])\n",
      "  PydanticSerializationUnexpectedValue(Expected `StreamingChoices` - serialized value may not be as expected [input_value=Choices(finish_reason='st...': 2345711061266916400}), input_type=Choices])\n",
      "  return self.__pydantic_serializer__.to_python(\n",
      "\u001b[92m12:51:25 - LiteLLM:INFO\u001b[0m: utils.py:3119 - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.3-70B-Instruct-Turbo; provider = together_ai\n",
      "2025-06-25 12:51:25,944 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.3-70B-Instruct-Turbo; provider = together_ai\n",
      "\u001b[92m12:51:25 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "2025-06-25 12:51:25,953 - LiteLLM - INFO - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "Average Metric: 20.97 / 24 (87.4%):  46%|‚ñà‚ñà‚ñà‚ñè   | 23/50 [00:03<00:02,  9.22it/s]2025-06-25 12:51:25,990 - httpx - INFO - HTTP Request: POST https://api.together.xyz/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m12:51:25 - LiteLLM:INFO\u001b[0m: utils.py:1215 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-25 12:51:25,991 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m12:51:25 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "2025-06-25 12:51:25,991 - LiteLLM - INFO - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "2025-06-25 12:51:25,998 - httpx - INFO - HTTP Request: POST https://api.together.xyz/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "/Users/ytchen/Documents/experimental/Llama4/env/lib/python3.13/site-packages/pydantic/main.py:463: UserWarning: Pydantic serializer warnings:\n",
      "  PydanticSerializationUnexpectedValue(Expected 9 fields but got 5: Expected `Message` - serialized value may not be as expected [input_value=Message(content='[[ ## an...ields={'refusal': None}), input_type=Message])\n",
      "  PydanticSerializationUnexpectedValue(Expected `StreamingChoices` - serialized value may not be as expected [input_value=Choices(finish_reason='st...: 10723307813794646000}), input_type=Choices])\n",
      "  return self.__pydantic_serializer__.to_python(\n",
      "\u001b[92m12:51:26 - LiteLLM:INFO\u001b[0m: utils.py:3119 - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.3-70B-Instruct-Turbo; provider = together_ai\n",
      "2025-06-25 12:51:26,007 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.3-70B-Instruct-Turbo; provider = together_ai\n",
      "\u001b[92m12:51:26 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "2025-06-25 12:51:26,012 - LiteLLM - INFO - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "Average Metric: 21.93 / 25 (87.7%):  50%|‚ñà‚ñà‚ñà‚ñå   | 25/50 [00:03<00:02,  9.15it/s]\u001b[92m12:51:26 - LiteLLM:INFO\u001b[0m: utils.py:1215 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-25 12:51:26,012 - httpx - INFO - HTTP Request: POST https://api.together.xyz/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-06-25 12:51:26,024 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m12:51:26 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "2025-06-25 12:51:26,025 - LiteLLM - INFO - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "/Users/ytchen/Documents/experimental/Llama4/env/lib/python3.13/site-packages/pydantic/main.py:463: UserWarning: Pydantic serializer warnings:\n",
      "  PydanticSerializationUnexpectedValue(Expected 9 fields but got 5: Expected `Message` - serialized value may not be as expected [input_value=Message(content='[[ ## an...ields={'refusal': None}), input_type=Message])\n",
      "  PydanticSerializationUnexpectedValue(Expected `StreamingChoices` - serialized value may not be as expected [input_value=Choices(finish_reason='st...': 1479263138250479600}), input_type=Choices])\n",
      "  return self.__pydantic_serializer__.to_python(\n",
      "\u001b[92m12:51:26 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "2025-06-25 12:51:26,040 - LiteLLM - INFO - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "Average Metric: 22.83 / 26 (87.8%):  50%|‚ñà‚ñà‚ñà‚ñå   | 25/50 [00:03<00:02,  9.15it/s]\u001b[92m12:51:26 - LiteLLM:INFO\u001b[0m: utils.py:3119 - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.3-70B-Instruct-Turbo; provider = together_ai\n",
      "2025-06-25 12:51:26,041 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.3-70B-Instruct-Turbo; provider = together_ai\n",
      "\u001b[92m12:51:26 - LiteLLM:INFO\u001b[0m: utils.py:1215 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-25 12:51:26,048 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m12:51:26 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "2025-06-25 12:51:26,055 - LiteLLM - INFO - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "/Users/ytchen/Documents/experimental/Llama4/env/lib/python3.13/site-packages/pydantic/main.py:463: UserWarning: Pydantic serializer warnings:\n",
      "  PydanticSerializationUnexpectedValue(Expected 9 fields but got 5: Expected `Message` - serialized value may not be as expected [input_value=Message(content='[[ ## an...ields={'refusal': None}), input_type=Message])\n",
      "  PydanticSerializationUnexpectedValue(Expected `StreamingChoices` - serialized value may not be as expected [input_value=Choices(finish_reason='st...: 12698440223101542000}), input_type=Choices])\n",
      "  return self.__pydantic_serializer__.to_python(\n",
      "\u001b[92m12:51:26 - LiteLLM:INFO\u001b[0m: utils.py:3119 - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.3-70B-Instruct-Turbo; provider = together_ai\n",
      "Average Metric: 23.80 / 27 (88.1%):  52%|‚ñà‚ñà‚ñà‚ñã   | 26/50 [00:03<00:02,  9.15it/s]2025-06-25 12:51:26,071 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.3-70B-Instruct-Turbo; provider = together_ai\n",
      "\u001b[92m12:51:26 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "2025-06-25 12:51:26,072 - LiteLLM - INFO - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "2025-06-25 12:51:26,117 - httpx - INFO - HTTP Request: POST https://api.together.xyz/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m12:51:26 - LiteLLM:INFO\u001b[0m: utils.py:1215 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-25 12:51:26,118 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m12:51:26 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "2025-06-25 12:51:26,118 - LiteLLM - INFO - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "/Users/ytchen/Documents/experimental/Llama4/env/lib/python3.13/site-packages/pydantic/main.py:463: UserWarning: Pydantic serializer warnings:\n",
      "  PydanticSerializationUnexpectedValue(Expected 9 fields but got 5: Expected `Message` - serialized value may not be as expected [input_value=Message(content='[[ ## an...ields={'refusal': None}), input_type=Message])\n",
      "  PydanticSerializationUnexpectedValue(Expected `StreamingChoices` - serialized value may not be as expected [input_value=Choices(finish_reason='st...': 7942495771328956000}), input_type=Choices])\n",
      "  return self.__pydantic_serializer__.to_python(\n",
      "Average Metric: 24.80 / 28 (88.6%):  56%|‚ñà‚ñà‚ñà‚ñâ   | 28/50 [00:03<00:01, 12.19it/s]\u001b[92m12:51:26 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "2025-06-25 12:51:26,132 - LiteLLM - INFO - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "\u001b[92m12:51:26 - LiteLLM:INFO\u001b[0m: utils.py:3119 - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.3-70B-Instruct-Turbo; provider = together_ai\n",
      "2025-06-25 12:51:26,139 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.3-70B-Instruct-Turbo; provider = together_ai\n",
      "2025-06-25 12:51:26,232 - httpx - INFO - HTTP Request: POST https://api.together.xyz/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-06-25 12:51:26,233 - httpx - INFO - HTTP Request: POST https://api.together.xyz/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m12:51:26 - LiteLLM:INFO\u001b[0m: utils.py:1215 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-25 12:51:26,234 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m12:51:26 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "2025-06-25 12:51:26,234 - LiteLLM - INFO - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "/Users/ytchen/Documents/experimental/Llama4/env/lib/python3.13/site-packages/pydantic/main.py:463: UserWarning: Pydantic serializer warnings:\n",
      "  PydanticSerializationUnexpectedValue(Expected 9 fields but got 5: Expected `Message` - serialized value may not be as expected [input_value=Message(content='[[ ## an...ields={'refusal': None}), input_type=Message])\n",
      "  PydanticSerializationUnexpectedValue(Expected `StreamingChoices` - serialized value may not be as expected [input_value=Choices(finish_reason='st...': 5026945737996574000}), input_type=Choices])\n",
      "  return self.__pydantic_serializer__.to_python(\n",
      "Average Metric: 25.47 / 29 (87.8%):  56%|‚ñà‚ñà‚ñà‚ñâ   | 28/50 [00:03<00:01, 12.19it/s]\u001b[92m12:51:26 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "2025-06-25 12:51:26,249 - LiteLLM - INFO - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "\u001b[92m12:51:26 - LiteLLM:INFO\u001b[0m: utils.py:3119 - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.3-70B-Instruct-Turbo; provider = together_ai\n",
      "2025-06-25 12:51:26,259 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.3-70B-Instruct-Turbo; provider = together_ai\n",
      "\u001b[92m12:51:26 - LiteLLM:INFO\u001b[0m: utils.py:1215 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-25 12:51:26,268 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m12:51:26 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "2025-06-25 12:51:26,268 - LiteLLM - INFO - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "\u001b[92m12:51:26 - LiteLLM:INFO\u001b[0m: utils.py:3119 - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.3-70B-Instruct-Turbo; provider = together_ai\n",
      "2025-06-25 12:51:26,276 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.3-70B-Instruct-Turbo; provider = together_ai\n",
      "2025-06-25 12:51:26,280 - httpx - INFO - HTTP Request: POST https://api.together.xyz/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Average Metric: 26.07 / 30 (86.9%):  60%|‚ñà‚ñà‚ñà‚ñà‚ñè  | 30/50 [00:03<00:01, 12.50it/s]2025-06-25 12:51:26,281 - httpx - INFO - HTTP Request: POST https://api.together.xyz/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-06-25 12:51:26,281 - httpx - INFO - HTTP Request: POST https://api.together.xyz/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "/Users/ytchen/Documents/experimental/Llama4/env/lib/python3.13/site-packages/pydantic/main.py:463: UserWarning: Pydantic serializer warnings:\n",
      "  PydanticSerializationUnexpectedValue(Expected 9 fields but got 5: Expected `Message` - serialized value may not be as expected [input_value=Message(content='[[ ## an...ields={'refusal': None}), input_type=Message])\n",
      "  PydanticSerializationUnexpectedValue(Expected `StreamingChoices` - serialized value may not be as expected [input_value=Choices(finish_reason='st...: 16737828877343943000}), input_type=Choices])\n",
      "  return self.__pydantic_serializer__.to_python(\n",
      "2025-06-25 12:51:26,288 - httpx - INFO - HTTP Request: POST https://api.together.xyz/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m12:51:26 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "2025-06-25 12:51:26,292 - LiteLLM - INFO - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "2025-06-25 12:51:26,299 - httpx - INFO - HTTP Request: POST https://api.together.xyz/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m12:51:26 - LiteLLM:INFO\u001b[0m: utils.py:1215 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-25 12:51:26,299 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m12:51:26 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "2025-06-25 12:51:26,299 - LiteLLM - INFO - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "/Users/ytchen/Documents/experimental/Llama4/env/lib/python3.13/site-packages/pydantic/main.py:463: UserWarning: Pydantic serializer warnings:\n",
      "  PydanticSerializationUnexpectedValue(Expected 9 fields but got 5: Expected `Message` - serialized value may not be as expected [input_value=Message(content='[[ ## an...ields={'refusal': None}), input_type=Message])\n",
      "  PydanticSerializationUnexpectedValue(Expected `StreamingChoices` - serialized value may not be as expected [input_value=Choices(finish_reason='st...': 2831500616604434400}), input_type=Choices])\n",
      "  return self.__pydantic_serializer__.to_python(\n",
      "\u001b[92m12:51:26 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "2025-06-25 12:51:26,313 - LiteLLM - INFO - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "Average Metric: 26.73 / 31 (86.2%):  60%|‚ñà‚ñà‚ñà‚ñà‚ñè  | 30/50 [00:03<00:01, 12.50it/s]\u001b[92m12:51:26 - LiteLLM:INFO\u001b[0m: utils.py:3119 - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.3-70B-Instruct-Turbo; provider = together_ai\n",
      "2025-06-25 12:51:26,320 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.3-70B-Instruct-Turbo; provider = together_ai\n",
      "\u001b[92m12:51:26 - LiteLLM:INFO\u001b[0m: utils.py:3119 - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.3-70B-Instruct-Turbo; provider = together_ai\n",
      "2025-06-25 12:51:26,336 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.3-70B-Instruct-Turbo; provider = together_ai\n",
      "2025-06-25 12:51:26,342 - httpx - INFO - HTTP Request: POST https://api.together.xyz/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "\u001b[92m12:51:26 - LiteLLM:INFO\u001b[0m: utils.py:1215 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-25 12:51:26,345 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m12:51:26 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "2025-06-25 12:51:26,345 - LiteLLM - INFO - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "Average Metric: 27.73 / 32 (86.7%):  62%|‚ñà‚ñà‚ñà‚ñà‚ñé  | 31/50 [00:03<00:01, 12.50it/s]/Users/ytchen/Documents/experimental/Llama4/env/lib/python3.13/site-packages/pydantic/main.py:463: UserWarning: Pydantic serializer warnings:\n",
      "  PydanticSerializationUnexpectedValue(Expected 9 fields but got 5: Expected `Message` - serialized value may not be as expected [input_value=Message(content='[[ ## an...ields={'refusal': None}), input_type=Message])\n",
      "  PydanticSerializationUnexpectedValue(Expected `StreamingChoices` - serialized value may not be as expected [input_value=Choices(finish_reason='st...: 16357369221525047000}), input_type=Choices])\n",
      "  return self.__pydantic_serializer__.to_python(\n",
      "\u001b[92m12:51:26 - LiteLLM:INFO\u001b[0m: utils.py:3119 - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.3-70B-Instruct-Turbo; provider = together_ai\n",
      "2025-06-25 12:51:26,359 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.3-70B-Instruct-Turbo; provider = together_ai\n",
      "\u001b[92m12:51:26 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "2025-06-25 12:51:26,365 - LiteLLM - INFO - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "\u001b[92m12:51:26 - LiteLLM:INFO\u001b[0m: utils.py:1215 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-25 12:51:26,377 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m12:51:26 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "2025-06-25 12:51:26,377 - LiteLLM - INFO - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "Average Metric: 28.40 / 33 (86.1%):  64%|‚ñà‚ñà‚ñà‚ñà‚ñç  | 32/50 [00:03<00:01, 12.50it/s]/Users/ytchen/Documents/experimental/Llama4/env/lib/python3.13/site-packages/pydantic/main.py:463: UserWarning: Pydantic serializer warnings:\n",
      "  PydanticSerializationUnexpectedValue(Expected 9 fields but got 5: Expected `Message` - serialized value may not be as expected [input_value=Message(content='[[ ## an...ields={'refusal': None}), input_type=Message])\n",
      "  PydanticSerializationUnexpectedValue(Expected `StreamingChoices` - serialized value may not be as expected [input_value=Choices(finish_reason='st...': 2309736306547966500}), input_type=Choices])\n",
      "  return self.__pydantic_serializer__.to_python(\n",
      "\u001b[92m12:51:26 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "2025-06-25 12:51:26,384 - LiteLLM - INFO - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "\u001b[92m12:51:26 - LiteLLM:INFO\u001b[0m: utils.py:1215 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-25 12:51:26,391 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m12:51:26 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "2025-06-25 12:51:26,392 - LiteLLM - INFO - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "/Users/ytchen/Documents/experimental/Llama4/env/lib/python3.13/site-packages/pydantic/main.py:463: UserWarning: Pydantic serializer warnings:\n",
      "  PydanticSerializationUnexpectedValue(Expected 9 fields but got 5: Expected `Message` - serialized value may not be as expected [input_value=Message(content='[[ ## an...ields={'refusal': None}), input_type=Message])\n",
      "  PydanticSerializationUnexpectedValue(Expected `StreamingChoices` - serialized value may not be as expected [input_value=Choices(finish_reason='st...': 2721789622500920000}), input_type=Choices])\n",
      "  return self.__pydantic_serializer__.to_python(\n",
      "Average Metric: 29.33 / 34 (86.3%):  68%|‚ñà‚ñà‚ñà‚ñà‚ñä  | 34/50 [00:03<00:00, 17.21it/s]\u001b[92m12:51:26 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "2025-06-25 12:51:26,399 - LiteLLM - INFO - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "\u001b[92m12:51:26 - LiteLLM:INFO\u001b[0m: utils.py:3119 - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.3-70B-Instruct-Turbo; provider = together_ai\n",
      "2025-06-25 12:51:26,412 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.3-70B-Instruct-Turbo; provider = together_ai\n",
      "2025-06-25 12:51:26,421 - httpx - INFO - HTTP Request: POST https://api.together.xyz/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m12:51:26 - LiteLLM:INFO\u001b[0m: utils.py:1215 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-25 12:51:26,424 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m12:51:26 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "2025-06-25 12:51:26,424 - LiteLLM - INFO - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "Average Metric: 29.97 / 35 (85.6%):  68%|‚ñà‚ñà‚ñà‚ñà‚ñä  | 34/50 [00:03<00:00, 17.21it/s]/Users/ytchen/Documents/experimental/Llama4/env/lib/python3.13/site-packages/pydantic/main.py:463: UserWarning: Pydantic serializer warnings:\n",
      "  PydanticSerializationUnexpectedValue(Expected 9 fields but got 5: Expected `Message` - serialized value may not be as expected [input_value=Message(content='[[ ## an...ields={'refusal': None}), input_type=Message])\n",
      "  PydanticSerializationUnexpectedValue(Expected `StreamingChoices` - serialized value may not be as expected [input_value=Choices(finish_reason='st...': 9309482805423284000}), input_type=Choices])\n",
      "  return self.__pydantic_serializer__.to_python(\n",
      "\u001b[92m12:51:26 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "2025-06-25 12:51:26,431 - LiteLLM - INFO - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "2025-06-25 12:51:26,518 - httpx - INFO - HTTP Request: POST https://api.together.xyz/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m12:51:26 - LiteLLM:INFO\u001b[0m: utils.py:1215 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-25 12:51:26,519 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m12:51:26 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "2025-06-25 12:51:26,519 - LiteLLM - INFO - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "/Users/ytchen/Documents/experimental/Llama4/env/lib/python3.13/site-packages/pydantic/main.py:463: UserWarning: Pydantic serializer warnings:\n",
      "  PydanticSerializationUnexpectedValue(Expected 9 fields but got 5: Expected `Message` - serialized value may not be as expected [input_value=Message(content='[[ ## an...ields={'refusal': None}), input_type=Message])\n",
      "  PydanticSerializationUnexpectedValue(Expected `StreamingChoices` - serialized value may not be as expected [input_value=Choices(finish_reason='st...: 15124957825259602000}), input_type=Choices])\n",
      "  return self.__pydantic_serializer__.to_python(\n",
      "\u001b[92m12:51:26 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "2025-06-25 12:51:26,527 - LiteLLM - INFO - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "2025-06-25 12:51:26,534 - httpx - INFO - HTTP Request: POST https://api.together.xyz/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "Average Metric: 30.90 / 36 (85.8%):  70%|‚ñà‚ñà‚ñà‚ñà‚ñâ  | 35/50 [00:03<00:00, 17.21it/s]2025-06-25 12:51:26,536 - httpx - INFO - HTTP Request: POST https://api.together.xyz/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "\u001b[92m12:51:26 - LiteLLM:INFO\u001b[0m: utils.py:3119 - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.3-70B-Instruct-Turbo; provider = together_ai\n",
      "2025-06-25 12:51:26,544 - httpx - INFO - HTTP Request: POST https://api.together.xyz/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-25 12:51:26,544 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.3-70B-Instruct-Turbo; provider = together_ai\n",
      "\u001b[92m12:51:26 - LiteLLM:INFO\u001b[0m: utils.py:3119 - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.3-70B-Instruct-Turbo; provider = together_ai\n",
      "2025-06-25 12:51:26,560 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.3-70B-Instruct-Turbo; provider = together_ai\n",
      "2025-06-25 12:51:26,566 - httpx - INFO - HTTP Request: POST https://api.together.xyz/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m12:51:26 - LiteLLM:INFO\u001b[0m: utils.py:3119 - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.3-70B-Instruct-Turbo; provider = together_ai\n",
      "2025-06-25 12:51:26,583 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.3-70B-Instruct-Turbo; provider = together_ai\n",
      "2025-06-25 12:51:26,584 - httpx - INFO - HTTP Request: POST https://api.together.xyz/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m12:51:26 - LiteLLM:INFO\u001b[0m: utils.py:1215 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-25 12:51:26,593 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m12:51:26 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "2025-06-25 12:51:26,593 - LiteLLM - INFO - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "/Users/ytchen/Documents/experimental/Llama4/env/lib/python3.13/site-packages/pydantic/main.py:463: UserWarning: Pydantic serializer warnings:\n",
      "  PydanticSerializationUnexpectedValue(Expected 9 fields but got 5: Expected `Message` - serialized value may not be as expected [input_value=Message(content='[[ ## an...ields={'refusal': None}), input_type=Message])\n",
      "  PydanticSerializationUnexpectedValue(Expected `StreamingChoices` - serialized value may not be as expected [input_value=Choices(finish_reason='st...: 18074094339566973000}), input_type=Choices])\n",
      "  return self.__pydantic_serializer__.to_python(\n",
      "\u001b[92m12:51:26 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "2025-06-25 12:51:26,601 - LiteLLM - INFO - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "Average Metric: 31.87 / 37 (86.1%):  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 37/50 [00:03<00:00, 16.18it/s]\u001b[92m12:51:26 - LiteLLM:INFO\u001b[0m: utils.py:1215 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-25 12:51:26,609 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m12:51:26 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "2025-06-25 12:51:26,609 - LiteLLM - INFO - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "/Users/ytchen/Documents/experimental/Llama4/env/lib/python3.13/site-packages/pydantic/main.py:463: UserWarning: Pydantic serializer warnings:\n",
      "  PydanticSerializationUnexpectedValue(Expected 9 fields but got 5: Expected `Message` - serialized value may not be as expected [input_value=Message(content='[[ ## an...ields={'refusal': None}), input_type=Message])\n",
      "  PydanticSerializationUnexpectedValue(Expected `StreamingChoices` - serialized value may not be as expected [input_value=Choices(finish_reason='st...': 2694824220565035000}), input_type=Choices])\n",
      "  return self.__pydantic_serializer__.to_python(\n",
      "Average Metric: 32.83 / 38 (86.4%):  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 37/50 [00:03<00:00, 16.18it/s]\u001b[92m12:51:26 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "2025-06-25 12:51:26,616 - LiteLLM - INFO - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "2025-06-25 12:51:26,659 - httpx - INFO - HTTP Request: POST https://api.together.xyz/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "\u001b[92m12:51:26 - LiteLLM:INFO\u001b[0m: utils.py:3119 - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.3-70B-Instruct-Turbo; provider = together_ai\n",
      "2025-06-25 12:51:26,667 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.3-70B-Instruct-Turbo; provider = together_ai\n",
      "2025-06-25 12:51:26,714 - httpx - INFO - HTTP Request: POST https://api.together.xyz/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "\u001b[92m12:51:26 - LiteLLM:INFO\u001b[0m: utils.py:3119 - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.3-70B-Instruct-Turbo; provider = together_ai\n",
      "2025-06-25 12:51:26,730 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.3-70B-Instruct-Turbo; provider = together_ai\n",
      "2025-06-25 12:51:26,738 - httpx - INFO - HTTP Request: POST https://api.together.xyz/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-06-25 12:51:26,739 - httpx - INFO - HTTP Request: POST https://api.together.xyz/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-25 12:51:26,745 - httpx - INFO - HTTP Request: POST https://api.together.xyz/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "\u001b[92m12:51:26 - LiteLLM:INFO\u001b[0m: utils.py:3119 - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.3-70B-Instruct-Turbo; provider = together_ai\n",
      "2025-06-25 12:51:26,751 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.3-70B-Instruct-Turbo; provider = together_ai\n",
      "\u001b[92m12:51:26 - LiteLLM:INFO\u001b[0m: utils.py:3119 - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.3-70B-Instruct-Turbo; provider = together_ai\n",
      "2025-06-25 12:51:26,774 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.3-70B-Instruct-Turbo; provider = together_ai\n",
      "\u001b[92m12:51:26 - LiteLLM:INFO\u001b[0m: utils.py:1215 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-25 12:51:26,784 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m12:51:26 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "2025-06-25 12:51:26,784 - LiteLLM - INFO - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "Average Metric: 33.50 / 39 (85.9%):  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 39/50 [00:03<00:00, 14.72it/s]/Users/ytchen/Documents/experimental/Llama4/env/lib/python3.13/site-packages/pydantic/main.py:463: UserWarning: Pydantic serializer warnings:\n",
      "  PydanticSerializationUnexpectedValue(Expected 9 fields but got 5: Expected `Message` - serialized value may not be as expected [input_value=Message(content='[[ ## an...ields={'refusal': None}), input_type=Message])\n",
      "  PydanticSerializationUnexpectedValue(Expected `StreamingChoices` - serialized value may not be as expected [input_value=Choices(finish_reason='st...: 14712606404165155000}), input_type=Choices])\n",
      "  return self.__pydantic_serializer__.to_python(\n",
      "\u001b[92m12:51:26 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "2025-06-25 12:51:26,791 - LiteLLM - INFO - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "2025-06-25 12:51:26,804 - httpx - INFO - HTTP Request: POST https://api.together.xyz/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m12:51:26 - LiteLLM:INFO\u001b[0m: utils.py:1215 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-25 12:51:26,805 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m12:51:26 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "2025-06-25 12:51:26,805 - LiteLLM - INFO - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "Average Metric: 34.13 / 40 (85.3%):  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 39/50 [00:03<00:00, 14.72it/s]/Users/ytchen/Documents/experimental/Llama4/env/lib/python3.13/site-packages/pydantic/main.py:463: UserWarning: Pydantic serializer warnings:\n",
      "  PydanticSerializationUnexpectedValue(Expected 9 fields but got 5: Expected `Message` - serialized value may not be as expected [input_value=Message(content='[[ ## an...ields={'refusal': None}), input_type=Message])\n",
      "  PydanticSerializationUnexpectedValue(Expected `StreamingChoices` - serialized value may not be as expected [input_value=Choices(finish_reason='st...': 3141627657562114600}), input_type=Choices])\n",
      "  return self.__pydantic_serializer__.to_python(\n",
      "\u001b[92m12:51:26 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "2025-06-25 12:51:26,813 - LiteLLM - INFO - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "2025-06-25 12:51:26,828 - httpx - INFO - HTTP Request: POST https://api.together.xyz/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "\u001b[92m12:51:26 - LiteLLM:INFO\u001b[0m: utils.py:3119 - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.3-70B-Instruct-Turbo; provider = together_ai\n",
      "2025-06-25 12:51:26,842 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.3-70B-Instruct-Turbo; provider = together_ai\n",
      "2025-06-25 12:51:26,851 - httpx - INFO - HTTP Request: POST https://api.together.xyz/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "\u001b[92m12:51:26 - LiteLLM:INFO\u001b[0m: utils.py:3119 - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.3-70B-Instruct-Turbo; provider = together_ai\n",
      "2025-06-25 12:51:26,864 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.3-70B-Instruct-Turbo; provider = together_ai\n",
      "2025-06-25 12:51:26,919 - httpx - INFO - HTTP Request: POST https://api.together.xyz/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "\u001b[92m12:51:26 - LiteLLM:INFO\u001b[0m: utils.py:3119 - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.3-70B-Instruct-Turbo; provider = together_ai\n",
      "2025-06-25 12:51:26,933 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.3-70B-Instruct-Turbo; provider = together_ai\n",
      "2025-06-25 12:51:26,944 - httpx - INFO - HTTP Request: POST https://api.together.xyz/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "\u001b[92m12:51:26 - LiteLLM:INFO\u001b[0m: utils.py:3119 - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.3-70B-Instruct-Turbo; provider = together_ai\n",
      "2025-06-25 12:51:26,956 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.3-70B-Instruct-Turbo; provider = together_ai\n",
      "2025-06-25 12:51:26,964 - httpx - INFO - HTTP Request: POST https://api.together.xyz/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "\u001b[92m12:51:26 - LiteLLM:INFO\u001b[0m: utils.py:3119 - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.3-70B-Instruct-Turbo; provider = together_ai\n",
      "2025-06-25 12:51:26,977 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.3-70B-Instruct-Turbo; provider = together_ai\n",
      "2025-06-25 12:51:26,990 - httpx - INFO - HTTP Request: POST https://api.together.xyz/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "\u001b[92m12:51:27 - LiteLLM:INFO\u001b[0m: utils.py:3119 - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.3-70B-Instruct-Turbo; provider = together_ai\n",
      "2025-06-25 12:51:27,001 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.3-70B-Instruct-Turbo; provider = together_ai\n",
      "2025-06-25 12:51:27,295 - httpx - INFO - HTTP Request: POST https://api.together.xyz/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m12:51:27 - LiteLLM:INFO\u001b[0m: utils.py:1215 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-25 12:51:27,304 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m12:51:27 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "2025-06-25 12:51:27,305 - LiteLLM - INFO - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "/Users/ytchen/Documents/experimental/Llama4/env/lib/python3.13/site-packages/pydantic/main.py:463: UserWarning: Pydantic serializer warnings:\n",
      "  PydanticSerializationUnexpectedValue(Expected 9 fields but got 5: Expected `Message` - serialized value may not be as expected [input_value=Message(content='[[ ## an...ields={'refusal': None}), input_type=Message])\n",
      "  PydanticSerializationUnexpectedValue(Expected `StreamingChoices` - serialized value may not be as expected [input_value=Choices(finish_reason='st...': 6352795981469533000}), input_type=Choices])\n",
      "  return self.__pydantic_serializer__.to_python(\n",
      "Average Metric: 34.77 / 41 (84.8%):  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 40/50 [00:04<00:00, 14.72it/s]\u001b[92m12:51:27 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "2025-06-25 12:51:27,329 - LiteLLM - INFO - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "Average Metric: 34.77 / 41 (84.8%):  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 41/50 [00:04<00:01,  8.36it/s]2025-06-25 12:51:27,401 - httpx - INFO - HTTP Request: POST https://api.together.xyz/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m12:51:27 - LiteLLM:INFO\u001b[0m: utils.py:1215 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-25 12:51:27,402 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m12:51:27 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "2025-06-25 12:51:27,402 - LiteLLM - INFO - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "/Users/ytchen/Documents/experimental/Llama4/env/lib/python3.13/site-packages/pydantic/main.py:463: UserWarning: Pydantic serializer warnings:\n",
      "  PydanticSerializationUnexpectedValue(Expected 9 fields but got 5: Expected `Message` - serialized value may not be as expected [input_value=Message(content='[[ ## an...ields={'refusal': None}), input_type=Message])\n",
      "  PydanticSerializationUnexpectedValue(Expected `StreamingChoices` - serialized value may not be as expected [input_value=Choices(finish_reason='st...': 2981227703704813600}), input_type=Choices])\n",
      "  return self.__pydantic_serializer__.to_python(\n",
      "Average Metric: 35.73 / 42 (85.1%):  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 41/50 [00:04<00:01,  8.36it/s]\u001b[92m12:51:27 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "2025-06-25 12:51:27,412 - LiteLLM - INFO - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "2025-06-25 12:51:27,484 - httpx - INFO - HTTP Request: POST https://api.together.xyz/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m12:51:27 - LiteLLM:INFO\u001b[0m: utils.py:1215 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-25 12:51:27,486 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m12:51:27 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "2025-06-25 12:51:27,486 - LiteLLM - INFO - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "\u001b[92m12:51:27 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "2025-06-25 12:51:27,487 - LiteLLM - INFO - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "/Users/ytchen/Documents/experimental/Llama4/env/lib/python3.13/site-packages/pydantic/main.py:463: UserWarning: Pydantic serializer warnings:\n",
      "  PydanticSerializationUnexpectedValue(Expected 9 fields but got 5: Expected `Message` - serialized value may not be as expected [input_value=Message(content='[[ ## an...ields={'refusal': None}), input_type=Message])\n",
      "  PydanticSerializationUnexpectedValue(Expected `StreamingChoices` - serialized value may not be as expected [input_value=Choices(finish_reason='st...: 14195845875441254000}), input_type=Choices])\n",
      "  return self.__pydantic_serializer__.to_python(\n",
      "Average Metric: 36.73 / 43 (85.4%):  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 43/50 [00:04<00:00,  9.17it/s]\u001b[92m12:51:27 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "2025-06-25 12:51:27,498 - LiteLLM - INFO - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "2025-06-25 12:51:27,509 - httpx - INFO - HTTP Request: POST https://api.together.xyz/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-06-25 12:51:27,510 - httpx - INFO - HTTP Request: POST https://api.together.xyz/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m12:51:27 - LiteLLM:INFO\u001b[0m: utils.py:1215 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-25 12:51:27,511 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m12:51:27 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "2025-06-25 12:51:27,511 - LiteLLM - INFO - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "Average Metric: 37.73 / 44 (85.8%):  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 43/50 [00:04<00:00,  9.17it/s]/Users/ytchen/Documents/experimental/Llama4/env/lib/python3.13/site-packages/pydantic/main.py:463: UserWarning: Pydantic serializer warnings:\n",
      "  PydanticSerializationUnexpectedValue(Expected 9 fields but got 5: Expected `Message` - serialized value may not be as expected [input_value=Message(content='[[ ## an...ields={'refusal': None}), input_type=Message])\n",
      "  PydanticSerializationUnexpectedValue(Expected `StreamingChoices` - serialized value may not be as expected [input_value=Choices(finish_reason='st...': 6102675164861010000}), input_type=Choices])\n",
      "  return self.__pydantic_serializer__.to_python(\n",
      "\u001b[92m12:51:27 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "2025-06-25 12:51:27,520 - LiteLLM - INFO - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "\u001b[92m12:51:27 - LiteLLM:INFO\u001b[0m: utils.py:1215 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-25 12:51:27,530 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m12:51:27 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "2025-06-25 12:51:27,530 - LiteLLM - INFO - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "Average Metric: 38.70 / 45 (86.0%):  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 44/50 [00:04<00:00,  9.17it/s]/Users/ytchen/Documents/experimental/Llama4/env/lib/python3.13/site-packages/pydantic/main.py:463: UserWarning: Pydantic serializer warnings:\n",
      "  PydanticSerializationUnexpectedValue(Expected 9 fields but got 5: Expected `Message` - serialized value may not be as expected [input_value=Message(content='[[ ## an...ields={'refusal': None}), input_type=Message])\n",
      "  PydanticSerializationUnexpectedValue(Expected `StreamingChoices` - serialized value may not be as expected [input_value=Choices(finish_reason='st...: 13898178797776167000}), input_type=Choices])\n",
      "  return self.__pydantic_serializer__.to_python(\n",
      "\u001b[92m12:51:27 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "2025-06-25 12:51:27,540 - LiteLLM - INFO - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "2025-06-25 12:51:27,750 - httpx - INFO - HTTP Request: POST https://api.together.xyz/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m12:51:27 - LiteLLM:INFO\u001b[0m: utils.py:1215 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-25 12:51:27,755 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m12:51:27 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "2025-06-25 12:51:27,756 - LiteLLM - INFO - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "/Users/ytchen/Documents/experimental/Llama4/env/lib/python3.13/site-packages/pydantic/main.py:463: UserWarning: Pydantic serializer warnings:\n",
      "  PydanticSerializationUnexpectedValue(Expected 9 fields but got 5: Expected `Message` - serialized value may not be as expected [input_value=Message(content='[[ ## an...ields={'refusal': None}), input_type=Message])\n",
      "  PydanticSerializationUnexpectedValue(Expected `StreamingChoices` - serialized value may not be as expected [input_value=Choices(finish_reason='st...': 6541239322208369000}), input_type=Choices])\n",
      "  return self.__pydantic_serializer__.to_python(\n",
      "Average Metric: 39.37 / 46 (85.6%):  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 45/50 [00:04<00:00,  9.17it/s]\u001b[92m12:51:27 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "2025-06-25 12:51:27,780 - LiteLLM - INFO - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "Average Metric: 39.37 / 46 (85.6%):  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 46/50 [00:04<00:00,  9.53it/s]2025-06-25 12:51:27,800 - httpx - INFO - HTTP Request: POST https://api.together.xyz/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m12:51:27 - LiteLLM:INFO\u001b[0m: utils.py:1215 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-25 12:51:27,802 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m12:51:27 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "2025-06-25 12:51:27,802 - LiteLLM - INFO - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "Average Metric: 40.33 / 47 (85.8%):  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 46/50 [00:04<00:00,  9.53it/s]/Users/ytchen/Documents/experimental/Llama4/env/lib/python3.13/site-packages/pydantic/main.py:463: UserWarning: Pydantic serializer warnings:\n",
      "  PydanticSerializationUnexpectedValue(Expected 9 fields but got 5: Expected `Message` - serialized value may not be as expected [input_value=Message(content='[[ ## an...ields={'refusal': None}), input_type=Message])\n",
      "  PydanticSerializationUnexpectedValue(Expected `StreamingChoices` - serialized value may not be as expected [input_value=Choices(finish_reason='st...': 2685145312080737300}), input_type=Choices])\n",
      "  return self.__pydantic_serializer__.to_python(\n",
      "\u001b[92m12:51:27 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "2025-06-25 12:51:27,813 - LiteLLM - INFO - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "2025-06-25 12:51:27,822 - httpx - INFO - HTTP Request: POST https://api.together.xyz/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m12:51:27 - LiteLLM:INFO\u001b[0m: utils.py:1215 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-25 12:51:27,823 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m12:51:27 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "2025-06-25 12:51:27,823 - LiteLLM - INFO - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "Average Metric: 41.33 / 48 (86.1%):  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 47/50 [00:05<00:00,  9.53it/s]/Users/ytchen/Documents/experimental/Llama4/env/lib/python3.13/site-packages/pydantic/main.py:463: UserWarning: Pydantic serializer warnings:\n",
      "  PydanticSerializationUnexpectedValue(Expected 9 fields but got 5: Expected `Message` - serialized value may not be as expected [input_value=Message(content='[[ ## an...ields={'refusal': None}), input_type=Message])\n",
      "  PydanticSerializationUnexpectedValue(Expected `StreamingChoices` - serialized value may not be as expected [input_value=Choices(finish_reason='st...: 13025624542815156000}), input_type=Choices])\n",
      "  return self.__pydantic_serializer__.to_python(\n",
      "\u001b[92m12:51:27 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "2025-06-25 12:51:27,831 - LiteLLM - INFO - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "2025-06-25 12:51:28,115 - httpx - INFO - HTTP Request: POST https://api.together.xyz/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m12:51:28 - LiteLLM:INFO\u001b[0m: utils.py:1215 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-25 12:51:28,120 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m12:51:28 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "2025-06-25 12:51:28,120 - LiteLLM - INFO - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "/Users/ytchen/Documents/experimental/Llama4/env/lib/python3.13/site-packages/pydantic/main.py:463: UserWarning: Pydantic serializer warnings:\n",
      "  PydanticSerializationUnexpectedValue(Expected 9 fields but got 5: Expected `Message` - serialized value may not be as expected [input_value=Message(content='[[ ## an...ields={'refusal': None}), input_type=Message])\n",
      "  PydanticSerializationUnexpectedValue(Expected `StreamingChoices` - serialized value may not be as expected [input_value=Choices(finish_reason='st...d': 478613247266706900}), input_type=Choices])\n",
      "  return self.__pydantic_serializer__.to_python(\n",
      "Average Metric: 42.00 / 49 (85.7%):  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 48/50 [00:05<00:00,  9.53it/s]\u001b[92m12:51:28 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "Average Metric: 42.00 / 49 (85.7%):  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 49/50 [00:05<00:00,  9.10it/s]2025-06-25 12:51:28,145 - LiteLLM - INFO - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "2025-06-25 12:51:28,179 - httpx - INFO - HTTP Request: POST https://api.together.xyz/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m12:51:28 - LiteLLM:INFO\u001b[0m: utils.py:1215 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-25 12:51:28,181 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m12:51:28 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "2025-06-25 12:51:28,181 - LiteLLM - INFO - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "/Users/ytchen/Documents/experimental/Llama4/env/lib/python3.13/site-packages/pydantic/main.py:463: UserWarning: Pydantic serializer warnings:\n",
      "  PydanticSerializationUnexpectedValue(Expected 9 fields but got 5: Expected `Message` - serialized value may not be as expected [input_value=Message(content='[[ ## an...ields={'refusal': None}), input_type=Message])\n",
      "  PydanticSerializationUnexpectedValue(Expected `StreamingChoices` - serialized value may not be as expected [input_value=Choices(finish_reason='st...: 16885398729299057000}), input_type=Choices])\n",
      "  return self.__pydantic_serializer__.to_python(\n",
      "Average Metric: 42.93 / 50 (85.9%): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 50/50 [00:05<00:00,  9.30it/s]\n",
      "2025/06/25 12:51:28 INFO dspy.evaluate.evaluate: Average Metric: 42.93333333333333 / 50 (85.9%)\n",
      "2025/06/25 12:51:28 INFO dspy.teleprompt.mipro_optimizer_v2: \u001b[92mBest full score so far!\u001b[0m Score: 85.87\n",
      "2025/06/25 12:51:28 INFO dspy.teleprompt.mipro_optimizer_v2: Score: 85.87 with parameters ['Predictor 0: Instruction 1', 'Predictor 0: Few-Shot Set 1'].\n",
      "2025/06/25 12:51:28 INFO dspy.teleprompt.mipro_optimizer_v2: Scores so far: [79.8, 85.87]\n",
      "2025/06/25 12:51:28 INFO dspy.teleprompt.mipro_optimizer_v2: Best score so far: 85.87\n",
      "2025/06/25 12:51:28 INFO dspy.teleprompt.mipro_optimizer_v2: =======================\n",
      "\n",
      "\n",
      "2025/06/25 12:51:28 INFO dspy.teleprompt.mipro_optimizer_v2: ===== Trial 3 / 7 =====\n",
      "\u001b[92m12:51:28 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "2025-06-25 12:51:28,199 - LiteLLM - INFO - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "\u001b[92m12:51:28 - LiteLLM:INFO\u001b[0m: utils.py:3119 - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.3-70B-Instruct-Turbo; provider = together_ai\n",
      "2025-06-25 12:51:28,220 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.3-70B-Instruct-Turbo; provider = together_ai\n",
      "\u001b[92m12:51:28 - LiteLLM:INFO\u001b[0m: utils.py:3119 - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.3-70B-Instruct-Turbo; provider = together_ai\n",
      "2025-06-25 12:51:28,237 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.3-70B-Instruct-Turbo; provider = together_ai\n",
      "\u001b[92m12:51:28 - LiteLLM:INFO\u001b[0m: utils.py:3119 - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.3-70B-Instruct-Turbo; provider = together_ai\n",
      "2025-06-25 12:51:28,253 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.3-70B-Instruct-Turbo; provider = together_ai\n",
      "\u001b[92m12:51:28 - LiteLLM:INFO\u001b[0m: utils.py:3119 - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.3-70B-Instruct-Turbo; provider = together_ai\n",
      "2025-06-25 12:51:28,303 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.3-70B-Instruct-Turbo; provider = together_ai\n",
      "\u001b[92m12:51:28 - LiteLLM:INFO\u001b[0m: utils.py:3119 - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.3-70B-Instruct-Turbo; provider = together_ai\n",
      "2025-06-25 12:51:28,304 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.3-70B-Instruct-Turbo; provider = together_ai\n",
      "\u001b[92m12:51:28 - LiteLLM:INFO\u001b[0m: utils.py:3119 - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.3-70B-Instruct-Turbo; provider = together_ai\n",
      "2025-06-25 12:51:28,312 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.3-70B-Instruct-Turbo; provider = together_ai\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[92m12:51:28 - LiteLLM:INFO\u001b[0m: utils.py:3119 - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.3-70B-Instruct-Turbo; provider = together_ai\n",
      "2025-06-25 12:51:28,328 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.3-70B-Instruct-Turbo; provider = together_ai\n",
      "\u001b[92m12:51:28 - LiteLLM:INFO\u001b[0m: utils.py:3119 - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.3-70B-Instruct-Turbo; provider = together_ai\n",
      "2025-06-25 12:51:28,341 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.3-70B-Instruct-Turbo; provider = together_ai\n",
      "\u001b[92m12:51:28 - LiteLLM:INFO\u001b[0m: utils.py:3119 - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.3-70B-Instruct-Turbo; provider = together_ai\n",
      "2025-06-25 12:51:28,350 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.3-70B-Instruct-Turbo; provider = together_ai\n",
      "\u001b[92m12:51:28 - LiteLLM:INFO\u001b[0m: utils.py:3119 - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.3-70B-Instruct-Turbo; provider = together_ai\n",
      "2025-06-25 12:51:28,357 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.3-70B-Instruct-Turbo; provider = together_ai\n",
      "\u001b[92m12:51:28 - LiteLLM:INFO\u001b[0m: utils.py:3119 - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.3-70B-Instruct-Turbo; provider = together_ai\n",
      "2025-06-25 12:51:28,368 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.3-70B-Instruct-Turbo; provider = together_ai\n",
      "\u001b[92m12:51:28 - LiteLLM:INFO\u001b[0m: utils.py:3119 - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.3-70B-Instruct-Turbo; provider = together_ai\n",
      "2025-06-25 12:51:28,375 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.3-70B-Instruct-Turbo; provider = together_ai\n",
      "\u001b[92m12:51:28 - LiteLLM:INFO\u001b[0m: utils.py:3119 - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.3-70B-Instruct-Turbo; provider = together_ai\n",
      "2025-06-25 12:51:28,377 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.3-70B-Instruct-Turbo; provider = together_ai\n",
      "\u001b[92m12:51:28 - LiteLLM:INFO\u001b[0m: utils.py:3119 - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.3-70B-Instruct-Turbo; provider = together_ai\n",
      "2025-06-25 12:51:28,403 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.3-70B-Instruct-Turbo; provider = together_ai\n",
      "\u001b[92m12:51:28 - LiteLLM:INFO\u001b[0m: utils.py:3119 - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.3-70B-Instruct-Turbo; provider = together_ai\n",
      "2025-06-25 12:51:28,410 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.3-70B-Instruct-Turbo; provider = together_ai\n",
      "\u001b[92m12:51:28 - LiteLLM:INFO\u001b[0m: utils.py:3119 - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.3-70B-Instruct-Turbo; provider = together_ai\n",
      "2025-06-25 12:51:28,411 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.3-70B-Instruct-Turbo; provider = together_ai\n",
      "\u001b[92m12:51:28 - LiteLLM:INFO\u001b[0m: utils.py:3119 - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.3-70B-Instruct-Turbo; provider = together_ai\n",
      "2025-06-25 12:51:28,411 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.3-70B-Instruct-Turbo; provider = together_ai\n",
      "\u001b[92m12:51:28 - LiteLLM:INFO\u001b[0m: utils.py:3119 - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.3-70B-Instruct-Turbo; provider = together_ai\n",
      "2025-06-25 12:51:28,495 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.3-70B-Instruct-Turbo; provider = together_ai\n",
      "2025-06-25 12:51:29,754 - httpx - INFO - HTTP Request: POST https://api.together.xyz/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m12:51:29 - LiteLLM:INFO\u001b[0m: utils.py:1215 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-25 12:51:29,765 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m12:51:29 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "2025-06-25 12:51:29,765 - LiteLLM - INFO - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "/Users/ytchen/Documents/experimental/Llama4/env/lib/python3.13/site-packages/pydantic/main.py:463: UserWarning: Pydantic serializer warnings:\n",
      "  PydanticSerializationUnexpectedValue(Expected 9 fields but got 5: Expected `Message` - serialized value may not be as expected [input_value=Message(content='[[ ## an...ields={'refusal': None}), input_type=Message])\n",
      "  PydanticSerializationUnexpectedValue(Expected `StreamingChoices` - serialized value may not be as expected [input_value=Choices(finish_reason='st...: 14794743627437460000}), input_type=Choices])\n",
      "  return self.__pydantic_serializer__.to_python(\n",
      "Average Metric: 0.93 / 1 (93.3%):   2%|‚ñè         | 1/50 [00:01<01:03,  1.29s/it]2025-06-25 12:51:29,779 - httpx - INFO - HTTP Request: POST https://api.together.xyz/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m12:51:29 - LiteLLM:INFO\u001b[0m: utils.py:3119 - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.3-70B-Instruct-Turbo; provider = together_ai\n",
      "2025-06-25 12:51:29,789 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.3-70B-Instruct-Turbo; provider = together_ai\n",
      "\u001b[92m12:51:29 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "2025-06-25 12:51:29,805 - LiteLLM - INFO - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "\u001b[92m12:51:29 - LiteLLM:INFO\u001b[0m: utils.py:1215 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-25 12:51:29,830 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m12:51:29 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "2025-06-25 12:51:29,830 - LiteLLM - INFO - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "/Users/ytchen/Documents/experimental/Llama4/env/lib/python3.13/site-packages/pydantic/main.py:463: UserWarning: Pydantic serializer warnings:\n",
      "  PydanticSerializationUnexpectedValue(Expected 9 fields but got 5: Expected `Message` - serialized value may not be as expected [input_value=Message(content='[[ ## an...ields={'refusal': None}), input_type=Message])\n",
      "  PydanticSerializationUnexpectedValue(Expected `StreamingChoices` - serialized value may not be as expected [input_value=Choices(finish_reason='st...: 13496838105901724000}), input_type=Choices])\n",
      "  return self.__pydantic_serializer__.to_python(\n",
      "Average Metric: 1.87 / 2 (93.3%):   2%|‚ñè         | 1/50 [00:01<01:03,  1.29s/it]\u001b[92m12:51:29 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "2025-06-25 12:51:29,845 - LiteLLM - INFO - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "\u001b[92m12:51:29 - LiteLLM:INFO\u001b[0m: utils.py:3119 - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.3-70B-Instruct-Turbo; provider = together_ai\n",
      "2025-06-25 12:51:29,853 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.3-70B-Instruct-Turbo; provider = together_ai\n",
      "2025-06-25 12:51:29,913 - httpx - INFO - HTTP Request: POST https://api.together.xyz/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m12:51:29 - LiteLLM:INFO\u001b[0m: utils.py:1215 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-25 12:51:29,914 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m12:51:29 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "2025-06-25 12:51:29,914 - LiteLLM - INFO - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "Average Metric: 2.83 / 3 (94.4%):   6%|‚ñå         | 3/50 [00:01<00:17,  2.62it/s]/Users/ytchen/Documents/experimental/Llama4/env/lib/python3.13/site-packages/pydantic/main.py:463: UserWarning: Pydantic serializer warnings:\n",
      "  PydanticSerializationUnexpectedValue(Expected 9 fields but got 5: Expected `Message` - serialized value may not be as expected [input_value=Message(content='[[ ## an...ields={'refusal': None}), input_type=Message])\n",
      "  PydanticSerializationUnexpectedValue(Expected `StreamingChoices` - serialized value may not be as expected [input_value=Choices(finish_reason='st...': 6380263782600175000}), input_type=Choices])\n",
      "  return self.__pydantic_serializer__.to_python(\n",
      "\u001b[92m12:51:29 - LiteLLM:INFO\u001b[0m: utils.py:3119 - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.3-70B-Instruct-Turbo; provider = together_ai\n",
      "2025-06-25 12:51:29,929 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.3-70B-Instruct-Turbo; provider = together_ai\n",
      "\u001b[92m12:51:29 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "2025-06-25 12:51:29,936 - LiteLLM - INFO - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "2025-06-25 12:51:29,981 - httpx - INFO - HTTP Request: POST https://api.together.xyz/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m12:51:29 - LiteLLM:INFO\u001b[0m: utils.py:1215 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-25 12:51:29,982 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m12:51:29 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "2025-06-25 12:51:29,982 - LiteLLM - INFO - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "Average Metric: 3.43 / 4 (85.8%):   6%|‚ñå         | 3/50 [00:01<00:17,  2.62it/s]/Users/ytchen/Documents/experimental/Llama4/env/lib/python3.13/site-packages/pydantic/main.py:463: UserWarning: Pydantic serializer warnings:\n",
      "  PydanticSerializationUnexpectedValue(Expected 9 fields but got 5: Expected `Message` - serialized value may not be as expected [input_value=Message(content='[[ ## an...ields={'refusal': None}), input_type=Message])\n",
      "  PydanticSerializationUnexpectedValue(Expected `StreamingChoices` - serialized value may not be as expected [input_value=Choices(finish_reason='st...': 2573747875716535300}), input_type=Choices])\n",
      "  return self.__pydantic_serializer__.to_python(\n",
      "\u001b[92m12:51:29 - LiteLLM:INFO\u001b[0m: utils.py:3119 - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.3-70B-Instruct-Turbo; provider = together_ai\n",
      "2025-06-25 12:51:29,997 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.3-70B-Instruct-Turbo; provider = together_ai\n",
      "\u001b[92m12:51:30 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "2025-06-25 12:51:30,034 - LiteLLM - INFO - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "2025-06-25 12:51:30,070 - httpx - INFO - HTTP Request: POST https://api.together.xyz/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m12:51:30 - LiteLLM:INFO\u001b[0m: utils.py:1215 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-25 12:51:30,072 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m12:51:30 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "2025-06-25 12:51:30,072 - LiteLLM - INFO - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "Average Metric: 4.07 / 5 (81.3%):   8%|‚ñä         | 4/50 [00:01<00:17,  2.62it/s]\u001b[92m12:51:30 - LiteLLM:INFO\u001b[0m: utils.py:3119 - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.3-70B-Instruct-Turbo; provider = together_ai\n",
      "2025-06-25 12:51:30,080 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.3-70B-Instruct-Turbo; provider = together_ai\n",
      "/Users/ytchen/Documents/experimental/Llama4/env/lib/python3.13/site-packages/pydantic/main.py:463: UserWarning: Pydantic serializer warnings:\n",
      "  PydanticSerializationUnexpectedValue(Expected 9 fields but got 5: Expected `Message` - serialized value may not be as expected [input_value=Message(content='[[ ## an...ields={'refusal': None}), input_type=Message])\n",
      "  PydanticSerializationUnexpectedValue(Expected `StreamingChoices` - serialized value may not be as expected [input_value=Choices(finish_reason='st...ed': 56146074895190430}), input_type=Choices])\n",
      "  return self.__pydantic_serializer__.to_python(\n",
      "Average Metric: 4.07 / 5 (81.3%):  10%|‚ñà         | 5/50 [00:01<00:10,  4.34it/s]\u001b[92m12:51:30 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "2025-06-25 12:51:30,095 - LiteLLM - INFO - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "2025-06-25 12:51:30,102 - httpx - INFO - HTTP Request: POST https://api.together.xyz/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-06-25 12:51:30,102 - httpx - INFO - HTTP Request: POST https://api.together.xyz/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m12:51:30 - LiteLLM:INFO\u001b[0m: utils.py:1215 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-25 12:51:30,103 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m12:51:30 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "2025-06-25 12:51:30,103 - LiteLLM - INFO - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "/Users/ytchen/Documents/experimental/Llama4/env/lib/python3.13/site-packages/pydantic/main.py:463: UserWarning: Pydantic serializer warnings:\n",
      "  PydanticSerializationUnexpectedValue(Expected 9 fields but got 5: Expected `Message` - serialized value may not be as expected [input_value=Message(content='[[ ## an...ields={'refusal': None}), input_type=Message])\n",
      "  PydanticSerializationUnexpectedValue(Expected `StreamingChoices` - serialized value may not be as expected [input_value=Choices(finish_reason='st...: 12137069874101266000}), input_type=Choices])\n",
      "  return self.__pydantic_serializer__.to_python(\n",
      "Average Metric: 5.00 / 6 (83.3%):  10%|‚ñà         | 5/50 [00:01<00:10,  4.34it/s]\u001b[92m12:51:30 - LiteLLM:INFO\u001b[0m: utils.py:3119 - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.3-70B-Instruct-Turbo; provider = together_ai\n",
      "2025-06-25 12:51:30,117 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.3-70B-Instruct-Turbo; provider = together_ai\n",
      "\u001b[92m12:51:30 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "2025-06-25 12:51:30,125 - LiteLLM - INFO - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "\u001b[92m12:51:30 - LiteLLM:INFO\u001b[0m: utils.py:1215 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-25 12:51:30,131 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m12:51:30 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "2025-06-25 12:51:30,131 - LiteLLM - INFO - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "/Users/ytchen/Documents/experimental/Llama4/env/lib/python3.13/site-packages/pydantic/main.py:463: UserWarning: Pydantic serializer warnings:\n",
      "  PydanticSerializationUnexpectedValue(Expected 9 fields but got 5: Expected `Message` - serialized value may not be as expected [input_value=Message(content='[[ ## an...ields={'refusal': None}), input_type=Message])\n",
      "  PydanticSerializationUnexpectedValue(Expected `StreamingChoices` - serialized value may not be as expected [input_value=Choices(finish_reason='st...: 12142701331710845000}), input_type=Choices])\n",
      "  return self.__pydantic_serializer__.to_python(\n",
      "Average Metric: 5.93 / 7 (84.8%):  12%|‚ñà‚ñè        | 6/50 [00:01<00:10,  4.34it/s]\u001b[92m12:51:30 - LiteLLM:INFO\u001b[0m: utils.py:3119 - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.3-70B-Instruct-Turbo; provider = together_ai\n",
      "2025-06-25 12:51:30,140 - httpx - INFO - HTTP Request: POST https://api.together.xyz/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m12:51:30 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "2025-06-25 12:51:30,139 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.3-70B-Instruct-Turbo; provider = together_ai\n",
      "2025-06-25 12:51:30,146 - LiteLLM - INFO - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "\u001b[92m12:51:30 - LiteLLM:INFO\u001b[0m: utils.py:1215 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-25 12:51:30,162 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m12:51:30 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "2025-06-25 12:51:30,162 - LiteLLM - INFO - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "/Users/ytchen/Documents/experimental/Llama4/env/lib/python3.13/site-packages/pydantic/main.py:463: UserWarning: Pydantic serializer warnings:\n",
      "  PydanticSerializationUnexpectedValue(Expected 9 fields but got 5: Expected `Message` - serialized value may not be as expected [input_value=Message(content='[[ ## an...ields={'refusal': None}), input_type=Message])\n",
      "  PydanticSerializationUnexpectedValue(Expected `StreamingChoices` - serialized value may not be as expected [input_value=Choices(finish_reason='st...: 15013743034930090000}), input_type=Choices])\n",
      "  return self.__pydantic_serializer__.to_python(\n",
      "\u001b[92m12:51:30 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "2025-06-25 12:51:30,169 - LiteLLM - INFO - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "2025-06-25 12:51:30,175 - httpx - INFO - HTTP Request: POST https://api.together.xyz/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Average Metric: 6.93 / 8 (86.7%):  14%|‚ñà‚ñç        | 7/50 [00:01<00:09,  4.34it/s]\u001b[92m12:51:30 - LiteLLM:INFO\u001b[0m: utils.py:3119 - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.3-70B-Instruct-Turbo; provider = together_ai\n",
      "2025-06-25 12:51:30,183 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.3-70B-Instruct-Turbo; provider = together_ai\n",
      "\u001b[92m12:51:30 - LiteLLM:INFO\u001b[0m: utils.py:1215 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-25 12:51:30,192 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m12:51:30 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "2025-06-25 12:51:30,192 - LiteLLM - INFO - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "Average Metric: 7.90 / 9 (87.8%):  18%|‚ñà‚ñä        | 9/50 [00:01<00:04,  9.18it/s]/Users/ytchen/Documents/experimental/Llama4/env/lib/python3.13/site-packages/pydantic/main.py:463: UserWarning: Pydantic serializer warnings:\n",
      "  PydanticSerializationUnexpectedValue(Expected 9 fields but got 5: Expected `Message` - serialized value may not be as expected [input_value=Message(content='[[ ## an...ields={'refusal': None}), input_type=Message])\n",
      "  PydanticSerializationUnexpectedValue(Expected `StreamingChoices` - serialized value may not be as expected [input_value=Choices(finish_reason='st...: 14835630949475717000}), input_type=Choices])\n",
      "  return self.__pydantic_serializer__.to_python(\n",
      "\u001b[92m12:51:30 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "2025-06-25 12:51:30,205 - LiteLLM - INFO - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "\u001b[92m12:51:30 - LiteLLM:INFO\u001b[0m: utils.py:3119 - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.3-70B-Instruct-Turbo; provider = together_ai\n",
      "2025-06-25 12:51:30,212 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.3-70B-Instruct-Turbo; provider = together_ai\n",
      "2025-06-25 12:51:30,218 - httpx - INFO - HTTP Request: POST https://api.together.xyz/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m12:51:30 - LiteLLM:INFO\u001b[0m: utils.py:1215 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-25 12:51:30,221 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m12:51:30 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "2025-06-25 12:51:30,222 - LiteLLM - INFO - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "\u001b[92m12:51:30 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "Average Metric: 8.90 / 10 (89.0%):  18%|‚ñà‚ñå       | 9/50 [00:01<00:04,  9.18it/s]2025-06-25 12:51:30,222 - LiteLLM - INFO - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "/Users/ytchen/Documents/experimental/Llama4/env/lib/python3.13/site-packages/pydantic/main.py:463: UserWarning: Pydantic serializer warnings:\n",
      "  PydanticSerializationUnexpectedValue(Expected 9 fields but got 5: Expected `Message` - serialized value may not be as expected [input_value=Message(content='[[ ## an...ields={'refusal': None}), input_type=Message])\n",
      "  PydanticSerializationUnexpectedValue(Expected `StreamingChoices` - serialized value may not be as expected [input_value=Choices(finish_reason='st...: 11676414599643800000}), input_type=Choices])\n",
      "  return self.__pydantic_serializer__.to_python(\n",
      "\u001b[92m12:51:30 - LiteLLM:INFO\u001b[0m: utils.py:3119 - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.3-70B-Instruct-Turbo; provider = together_ai\n",
      "2025-06-25 12:51:30,230 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.3-70B-Instruct-Turbo; provider = together_ai\n",
      "\u001b[92m12:51:30 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "2025-06-25 12:51:30,236 - LiteLLM - INFO - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "2025-06-25 12:51:30,574 - httpx - INFO - HTTP Request: POST https://api.together.xyz/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-06-25 12:51:30,577 - httpx - INFO - HTTP Request: POST https://api.together.xyz/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-06-25 12:51:30,578 - httpx - INFO - HTTP Request: POST https://api.together.xyz/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m12:51:30 - LiteLLM:INFO\u001b[0m: utils.py:1215 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-25 12:51:30,584 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m12:51:30 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "2025-06-25 12:51:30,579 - httpx - INFO - HTTP Request: POST https://api.together.xyz/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m12:51:30 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "2025-06-25 12:51:30,591 - LiteLLM - INFO - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "\u001b[92m12:51:30 - LiteLLM:INFO\u001b[0m: utils.py:1215 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-25 12:51:30,591 - LiteLLM - INFO - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "/Users/ytchen/Documents/experimental/Llama4/env/lib/python3.13/site-packages/pydantic/main.py:463: UserWarning: Pydantic serializer warnings:\n",
      "  PydanticSerializationUnexpectedValue(Expected 9 fields but got 5: Expected `Message` - serialized value may not be as expected [input_value=Message(content='[[ ## an...ields={'refusal': None}), input_type=Message])\n",
      "  PydanticSerializationUnexpectedValue(Expected `StreamingChoices` - serialized value may not be as expected [input_value=Choices(finish_reason='st...': 4209893329455908400}), input_type=Choices])\n",
      "  return self.__pydantic_serializer__.to_python(\n",
      "2025-06-25 12:51:30,593 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m12:51:30 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "2025-06-25 12:51:30,617 - LiteLLM - INFO - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "\u001b[92m12:51:30 - LiteLLM:INFO\u001b[0m: utils.py:3119 - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.3-70B-Instruct-Turbo; provider = together_ai\n",
      "2025-06-25 12:51:30,619 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.3-70B-Instruct-Turbo; provider = together_ai\n",
      "/Users/ytchen/Documents/experimental/Llama4/env/lib/python3.13/site-packages/pydantic/main.py:463: UserWarning: Pydantic serializer warnings:\n",
      "  PydanticSerializationUnexpectedValue(Expected 9 fields but got 5: Expected `Message` - serialized value may not be as expected [input_value=Message(content='[[ ## an...ields={'refusal': None}), input_type=Message])\n",
      "  PydanticSerializationUnexpectedValue(Expected `StreamingChoices` - serialized value may not be as expected [input_value=Choices(finish_reason='st...: 17786401089673521000}), input_type=Choices])\n",
      "  return self.__pydantic_serializer__.to_python(\n",
      "Average Metric: 9.87 / 11 (89.7%):  20%|‚ñà‚ñå      | 10/50 [00:02<00:04,  9.18it/s]\u001b[92m12:51:30 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "2025-06-25 12:51:30,634 - LiteLLM - INFO - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "Average Metric: 10.80 / 12 (90.0%):  22%|‚ñà‚ñå     | 11/50 [00:02<00:04,  9.18it/s]\u001b[92m12:51:30 - LiteLLM:INFO\u001b[0m: utils.py:3119 - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.3-70B-Instruct-Turbo; provider = together_ai\n",
      "2025-06-25 12:51:30,657 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.3-70B-Instruct-Turbo; provider = together_ai\n",
      "Average Metric: 10.80 / 12 (90.0%):  24%|‚ñà‚ñã     | 12/50 [00:02<00:04,  7.96it/s]\u001b[92m12:51:30 - LiteLLM:INFO\u001b[0m: utils.py:1215 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-25 12:51:30,666 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m12:51:30 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "2025-06-25 12:51:30,667 - LiteLLM - INFO - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "\u001b[92m12:51:30 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "2025-06-25 12:51:30,667 - LiteLLM - INFO - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "Average Metric: 11.40 / 13 (87.7%):  24%|‚ñà‚ñã     | 12/50 [00:02<00:04,  7.96it/s]\u001b[92m12:51:30 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "2025-06-25 12:51:30,670 - LiteLLM - INFO - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "/Users/ytchen/Documents/experimental/Llama4/env/lib/python3.13/site-packages/pydantic/main.py:463: UserWarning: Pydantic serializer warnings:\n",
      "  PydanticSerializationUnexpectedValue(Expected 9 fields but got 5: Expected `Message` - serialized value may not be as expected [input_value=Message(content='[[ ## an...ields={'refusal': None}), input_type=Message])\n",
      "  PydanticSerializationUnexpectedValue(Expected `StreamingChoices` - serialized value may not be as expected [input_value=Choices(finish_reason='st...: 18006800988541520000}), input_type=Choices])\n",
      "  return self.__pydantic_serializer__.to_python(\n",
      "\u001b[92m12:51:30 - LiteLLM:INFO\u001b[0m: utils.py:3119 - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.3-70B-Instruct-Turbo; provider = together_ai\n",
      "2025-06-25 12:51:30,677 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.3-70B-Instruct-Turbo; provider = together_ai\n",
      "\u001b[92m12:51:30 - LiteLLM:INFO\u001b[0m: utils.py:1215 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-25 12:51:30,678 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m12:51:30 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "2025-06-25 12:51:30,697 - LiteLLM - INFO - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "\u001b[92m12:51:30 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "2025-06-25 12:51:30,697 - LiteLLM - INFO - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "\u001b[92m12:51:30 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "2025-06-25 12:51:30,698 - LiteLLM - INFO - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "/Users/ytchen/Documents/experimental/Llama4/env/lib/python3.13/site-packages/pydantic/main.py:463: UserWarning: Pydantic serializer warnings:\n",
      "  PydanticSerializationUnexpectedValue(Expected 9 fields but got 5: Expected `Message` - serialized value may not be as expected [input_value=Message(content='[[ ## an...ields={'refusal': None}), input_type=Message])\n",
      "  PydanticSerializationUnexpectedValue(Expected `StreamingChoices` - serialized value may not be as expected [input_value=Choices(finish_reason='st...': 5686098252345505000}), input_type=Choices])\n",
      "  return self.__pydantic_serializer__.to_python(\n",
      "\u001b[92m12:51:30 - LiteLLM:INFO\u001b[0m: utils.py:3119 - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.3-70B-Instruct-Turbo; provider = together_ai\n",
      "2025-06-25 12:51:30,720 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.3-70B-Instruct-Turbo; provider = together_ai\n",
      "\u001b[92m12:51:30 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "2025-06-25 12:51:30,725 - LiteLLM - INFO - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "Average Metric: 12.33 / 14 (88.1%):  26%|‚ñà‚ñä     | 13/50 [00:02<00:04,  7.96it/s]2025-06-25 12:51:30,883 - httpx - INFO - HTTP Request: POST https://api.together.xyz/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m12:51:30 - LiteLLM:INFO\u001b[0m: utils.py:1215 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-25 12:51:30,884 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m12:51:30 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "2025-06-25 12:51:30,884 - LiteLLM - INFO - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "/Users/ytchen/Documents/experimental/Llama4/env/lib/python3.13/site-packages/pydantic/main.py:463: UserWarning: Pydantic serializer warnings:\n",
      "  PydanticSerializationUnexpectedValue(Expected 9 fields but got 5: Expected `Message` - serialized value may not be as expected [input_value=Message(content='[[ ## an...ields={'refusal': None}), input_type=Message])\n",
      "  PydanticSerializationUnexpectedValue(Expected `StreamingChoices` - serialized value may not be as expected [input_value=Choices(finish_reason='st...: 15678305275090125000}), input_type=Choices])\n",
      "  return self.__pydantic_serializer__.to_python(\n",
      "Average Metric: 13.30 / 15 (88.7%):  30%|‚ñà‚ñà     | 15/50 [00:02<00:03,  9.13it/s]\u001b[92m12:51:30 - LiteLLM:INFO\u001b[0m: utils.py:3119 - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.3-70B-Instruct-Turbo; provider = together_ai\n",
      "2025-06-25 12:51:30,898 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.3-70B-Instruct-Turbo; provider = together_ai\n",
      "\u001b[92m12:51:30 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "2025-06-25 12:51:30,910 - httpx - INFO - HTTP Request: POST https://api.together.xyz/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-06-25 12:51:30,910 - httpx - INFO - HTTP Request: POST https://api.together.xyz/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-06-25 12:51:30,901 - LiteLLM - INFO - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "\u001b[92m12:51:30 - LiteLLM:INFO\u001b[0m: utils.py:1215 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-25 12:51:30,911 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m12:51:30 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "2025-06-25 12:51:30,918 - LiteLLM - INFO - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "\u001b[92m12:51:30 - LiteLLM:INFO\u001b[0m: utils.py:3119 - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.3-70B-Instruct-Turbo; provider = together_ai\n",
      "2025-06-25 12:51:30,926 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.3-70B-Instruct-Turbo; provider = together_ai\n",
      "Average Metric: 14.30 / 16 (89.4%):  30%|‚ñà‚ñà     | 15/50 [00:02<00:03,  9.13it/s]\u001b[92m12:51:30 - LiteLLM:INFO\u001b[0m: utils.py:1215 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-25 12:51:30,934 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m12:51:30 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "2025-06-25 12:51:30,934 - LiteLLM - INFO - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "/Users/ytchen/Documents/experimental/Llama4/env/lib/python3.13/site-packages/pydantic/main.py:463: UserWarning: Pydantic serializer warnings:\n",
      "  PydanticSerializationUnexpectedValue(Expected 9 fields but got 5: Expected `Message` - serialized value may not be as expected [input_value=Message(content='[[ ## an...ields={'refusal': None}), input_type=Message])\n",
      "  PydanticSerializationUnexpectedValue(Expected `StreamingChoices` - serialized value may not be as expected [input_value=Choices(finish_reason='st...: 14055467281413499000}), input_type=Choices])\n",
      "  return self.__pydantic_serializer__.to_python(\n",
      "\u001b[92m12:51:30 - LiteLLM:INFO\u001b[0m: utils.py:3119 - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.3-70B-Instruct-Turbo; provider = together_ai\n",
      "2025-06-25 12:51:30,948 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.3-70B-Instruct-Turbo; provider = together_ai\n",
      "\u001b[92m12:51:30 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "2025-06-25 12:51:30,954 - LiteLLM - INFO - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "Average Metric: 15.30 / 17 (90.0%):  32%|‚ñà‚ñà‚ñè    | 16/50 [00:02<00:03,  9.13it/s]/Users/ytchen/Documents/experimental/Llama4/env/lib/python3.13/site-packages/pydantic/main.py:463: UserWarning: Pydantic serializer warnings:\n",
      "  PydanticSerializationUnexpectedValue(Expected 9 fields but got 5: Expected `Message` - serialized value may not be as expected [input_value=Message(content='[[ ## an...ields={'refusal': None}), input_type=Message])\n",
      "  PydanticSerializationUnexpectedValue(Expected `StreamingChoices` - serialized value may not be as expected [input_value=Choices(finish_reason='st...': 2873560035097690600}), input_type=Choices])\n",
      "  return self.__pydantic_serializer__.to_python(\n",
      "\u001b[92m12:51:30 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "2025-06-25 12:51:30,970 - LiteLLM - INFO - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "2025-06-25 12:51:31,061 - httpx - INFO - HTTP Request: POST https://api.together.xyz/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m12:51:31 - LiteLLM:INFO\u001b[0m: utils.py:1215 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-25 12:51:31,062 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m12:51:31 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "2025-06-25 12:51:31,062 - LiteLLM - INFO - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "Average Metric: 16.27 / 18 (90.4%):  36%|‚ñà‚ñà‚ñå    | 18/50 [00:02<00:02, 10.89it/s]/Users/ytchen/Documents/experimental/Llama4/env/lib/python3.13/site-packages/pydantic/main.py:463: UserWarning: Pydantic serializer warnings:\n",
      "  PydanticSerializationUnexpectedValue(Expected 9 fields but got 5: Expected `Message` - serialized value may not be as expected [input_value=Message(content='[[ ## an...ields={'refusal': None}), input_type=Message])\n",
      "  PydanticSerializationUnexpectedValue(Expected `StreamingChoices` - serialized value may not be as expected [input_value=Choices(finish_reason='st...': 2444416911453547000}), input_type=Choices])\n",
      "  return self.__pydantic_serializer__.to_python(\n",
      "\u001b[92m12:51:31 - LiteLLM:INFO\u001b[0m: utils.py:3119 - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.3-70B-Instruct-Turbo; provider = together_ai\n",
      "2025-06-25 12:51:31,076 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.3-70B-Instruct-Turbo; provider = together_ai\n",
      "\u001b[92m12:51:31 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "2025-06-25 12:51:31,083 - LiteLLM - INFO - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "2025-06-25 12:51:31,093 - httpx - INFO - HTTP Request: POST https://api.together.xyz/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m12:51:31 - LiteLLM:INFO\u001b[0m: utils.py:1215 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-25 12:51:31,094 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m12:51:31 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "2025-06-25 12:51:31,094 - LiteLLM - INFO - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "/Users/ytchen/Documents/experimental/Llama4/env/lib/python3.13/site-packages/pydantic/main.py:463: UserWarning: Pydantic serializer warnings:\n",
      "  PydanticSerializationUnexpectedValue(Expected 9 fields but got 5: Expected `Message` - serialized value may not be as expected [input_value=Message(content='[[ ## an...ields={'refusal': None}), input_type=Message])\n",
      "  PydanticSerializationUnexpectedValue(Expected `StreamingChoices` - serialized value may not be as expected [input_value=Choices(finish_reason='st...: 14238389427774210000}), input_type=Choices])\n",
      "  return self.__pydantic_serializer__.to_python(\n",
      "Average Metric: 16.90 / 19 (88.9%):  36%|‚ñà‚ñà‚ñå    | 18/50 [00:02<00:02, 10.89it/s]\u001b[92m12:51:31 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "2025-06-25 12:51:31,108 - LiteLLM - INFO - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "\u001b[92m12:51:31 - LiteLLM:INFO\u001b[0m: utils.py:3119 - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.3-70B-Instruct-Turbo; provider = together_ai\n",
      "2025-06-25 12:51:31,114 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.3-70B-Instruct-Turbo; provider = together_ai\n",
      "2025-06-25 12:51:31,141 - httpx - INFO - HTTP Request: POST https://api.together.xyz/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m12:51:31 - LiteLLM:INFO\u001b[0m: utils.py:1215 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-25 12:51:31,142 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m12:51:31 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "2025-06-25 12:51:31,142 - LiteLLM - INFO - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "/Users/ytchen/Documents/experimental/Llama4/env/lib/python3.13/site-packages/pydantic/main.py:463: UserWarning: Pydantic serializer warnings:\n",
      "  PydanticSerializationUnexpectedValue(Expected 9 fields but got 5: Expected `Message` - serialized value may not be as expected [input_value=Message(content='[[ ## an...ields={'refusal': None}), input_type=Message])\n",
      "  PydanticSerializationUnexpectedValue(Expected `StreamingChoices` - serialized value may not be as expected [input_value=Choices(finish_reason='st...': 7197511340652999000}), input_type=Choices])\n",
      "  return self.__pydantic_serializer__.to_python(\n",
      "\u001b[92m12:51:31 - LiteLLM:INFO\u001b[0m: utils.py:3119 - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.3-70B-Instruct-Turbo; provider = together_ai\n",
      "2025-06-25 12:51:31,155 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.3-70B-Instruct-Turbo; provider = together_ai\n",
      "Average Metric: 17.87 / 20 (89.3%):  38%|‚ñà‚ñà‚ñã    | 19/50 [00:02<00:02, 10.89it/s]\u001b[92m12:51:31 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "2025-06-25 12:51:31,164 - LiteLLM - INFO - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "2025-06-25 12:51:31,180 - httpx - INFO - HTTP Request: POST https://api.together.xyz/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m12:51:31 - LiteLLM:INFO\u001b[0m: utils.py:1215 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-25 12:51:31,181 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m12:51:31 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "2025-06-25 12:51:31,181 - LiteLLM - INFO - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "/Users/ytchen/Documents/experimental/Llama4/env/lib/python3.13/site-packages/pydantic/main.py:463: UserWarning: Pydantic serializer warnings:\n",
      "  PydanticSerializationUnexpectedValue(Expected 9 fields but got 5: Expected `Message` - serialized value may not be as expected [input_value=Message(content='[[ ## an...ields={'refusal': None}), input_type=Message])\n",
      "  PydanticSerializationUnexpectedValue(Expected `StreamingChoices` - serialized value may not be as expected [input_value=Choices(finish_reason='st...': 8099618605363113000}), input_type=Choices])\n",
      "  return self.__pydantic_serializer__.to_python(\n",
      "\u001b[92m12:51:31 - LiteLLM:INFO\u001b[0m: utils.py:3119 - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.3-70B-Instruct-Turbo; provider = together_ai\n",
      "2025-06-25 12:51:31,194 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.3-70B-Instruct-Turbo; provider = together_ai\n",
      "\u001b[92m12:51:31 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "2025-06-25 12:51:31,201 - LiteLLM - INFO - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "Average Metric: 18.80 / 21 (89.5%):  42%|‚ñà‚ñà‚ñâ    | 21/50 [00:02<00:02, 12.93it/s]2025-06-25 12:51:31,229 - httpx - INFO - HTTP Request: POST https://api.together.xyz/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m12:51:31 - LiteLLM:INFO\u001b[0m: utils.py:1215 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-25 12:51:31,230 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m12:51:31 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "2025-06-25 12:51:31,230 - LiteLLM - INFO - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "Average Metric: 19.13 / 22 (87.0%):  42%|‚ñà‚ñà‚ñâ    | 21/50 [00:02<00:02, 12.93it/s]/Users/ytchen/Documents/experimental/Llama4/env/lib/python3.13/site-packages/pydantic/main.py:463: UserWarning: Pydantic serializer warnings:\n",
      "  PydanticSerializationUnexpectedValue(Expected 9 fields but got 5: Expected `Message` - serialized value may not be as expected [input_value=Message(content='[[ ## an...ields={'refusal': None}), input_type=Message])\n",
      "  PydanticSerializationUnexpectedValue(Expected `StreamingChoices` - serialized value may not be as expected [input_value=Choices(finish_reason='st...: 12087473181441802000}), input_type=Choices])\n",
      "  return self.__pydantic_serializer__.to_python(\n",
      "\u001b[92m12:51:31 - LiteLLM:INFO\u001b[0m: utils.py:3119 - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.3-70B-Instruct-Turbo; provider = together_ai\n",
      "2025-06-25 12:51:31,244 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.3-70B-Instruct-Turbo; provider = together_ai\n",
      "\u001b[92m12:51:31 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "2025-06-25 12:51:31,250 - LiteLLM - INFO - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "2025-06-25 12:51:31,293 - httpx - INFO - HTTP Request: POST https://api.together.xyz/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m12:51:31 - LiteLLM:INFO\u001b[0m: utils.py:1215 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-25 12:51:31,294 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m12:51:31 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "2025-06-25 12:51:31,294 - LiteLLM - INFO - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "Average Metric: 20.07 / 23 (87.2%):  44%|‚ñà‚ñà‚ñà    | 22/50 [00:02<00:02, 12.93it/s]/Users/ytchen/Documents/experimental/Llama4/env/lib/python3.13/site-packages/pydantic/main.py:463: UserWarning: Pydantic serializer warnings:\n",
      "  PydanticSerializationUnexpectedValue(Expected 9 fields but got 5: Expected `Message` - serialized value may not be as expected [input_value=Message(content='[[ ## an...ields={'refusal': None}), input_type=Message])\n",
      "  PydanticSerializationUnexpectedValue(Expected `StreamingChoices` - serialized value may not be as expected [input_value=Choices(finish_reason='st...: 16204445582579600000}), input_type=Choices])\n",
      "  return self.__pydantic_serializer__.to_python(\n",
      "\u001b[92m12:51:31 - LiteLLM:INFO\u001b[0m: utils.py:3119 - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.3-70B-Instruct-Turbo; provider = together_ai\n",
      "2025-06-25 12:51:31,307 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.3-70B-Instruct-Turbo; provider = together_ai\n",
      "\u001b[92m12:51:31 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "2025-06-25 12:51:31,313 - LiteLLM - INFO - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "2025-06-25 12:51:31,366 - httpx - INFO - HTTP Request: POST https://api.together.xyz/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "\u001b[92m12:51:31 - LiteLLM:INFO\u001b[0m: utils.py:3119 - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.3-70B-Instruct-Turbo; provider = together_ai\n",
      "2025-06-25 12:51:31,374 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.3-70B-Instruct-Turbo; provider = together_ai\n",
      "2025-06-25 12:51:31,434 - httpx - INFO - HTTP Request: POST https://api.together.xyz/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "\u001b[92m12:51:31 - LiteLLM:INFO\u001b[0m: utils.py:3119 - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.3-70B-Instruct-Turbo; provider = together_ai\n",
      "2025-06-25 12:51:31,441 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.3-70B-Instruct-Turbo; provider = together_ai\n",
      "2025-06-25 12:51:31,457 - httpx - INFO - HTTP Request: POST https://api.together.xyz/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m12:51:31 - LiteLLM:INFO\u001b[0m: utils.py:1215 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-25 12:51:31,458 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m12:51:31 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "2025-06-25 12:51:31,458 - LiteLLM - INFO - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "/Users/ytchen/Documents/experimental/Llama4/env/lib/python3.13/site-packages/pydantic/main.py:463: UserWarning: Pydantic serializer warnings:\n",
      "  PydanticSerializationUnexpectedValue(Expected 9 fields but got 5: Expected `Message` - serialized value may not be as expected [input_value=Message(content='[[ ## an...ields={'refusal': None}), input_type=Message])\n",
      "  PydanticSerializationUnexpectedValue(Expected `StreamingChoices` - serialized value may not be as expected [input_value=Choices(finish_reason='st...: 13159085932299848000}), input_type=Choices])\n",
      "  return self.__pydantic_serializer__.to_python(\n",
      "\u001b[92m12:51:31 - LiteLLM:INFO\u001b[0m: utils.py:3119 - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.3-70B-Instruct-Turbo; provider = together_ai\n",
      "2025-06-25 12:51:31,471 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.3-70B-Instruct-Turbo; provider = together_ai\n",
      "\u001b[92m12:51:31 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "2025-06-25 12:51:31,481 - LiteLLM - INFO - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "2025-06-25 12:51:31,488 - httpx - INFO - HTTP Request: POST https://api.together.xyz/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Average Metric: 20.40 / 24 (85.0%):  48%|‚ñà‚ñà‚ñà‚ñé   | 24/50 [00:02<00:02, 12.16it/s]\u001b[92m12:51:31 - LiteLLM:INFO\u001b[0m: utils.py:1215 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-25 12:51:31,489 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m12:51:31 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "2025-06-25 12:51:31,489 - LiteLLM - INFO - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "/Users/ytchen/Documents/experimental/Llama4/env/lib/python3.13/site-packages/pydantic/main.py:463: UserWarning: Pydantic serializer warnings:\n",
      "  PydanticSerializationUnexpectedValue(Expected 9 fields but got 5: Expected `Message` - serialized value may not be as expected [input_value=Message(content='[[ ## an...ields={'refusal': None}), input_type=Message])\n",
      "  PydanticSerializationUnexpectedValue(Expected `StreamingChoices` - serialized value may not be as expected [input_value=Choices(finish_reason='st...': 4936843225331079000}), input_type=Choices])\n",
      "  return self.__pydantic_serializer__.to_python(\n",
      "Average Metric: 20.67 / 25 (82.7%):  48%|‚ñà‚ñà‚ñà‚ñé   | 24/50 [00:02<00:02, 12.16it/s]\u001b[92m12:51:31 - LiteLLM:INFO\u001b[0m: utils.py:3119 - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.3-70B-Instruct-Turbo; provider = together_ai\n",
      "2025-06-25 12:51:31,503 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.3-70B-Instruct-Turbo; provider = together_ai\n",
      "\u001b[92m12:51:31 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "2025-06-25 12:51:31,510 - LiteLLM - INFO - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "2025-06-25 12:51:31,518 - httpx - INFO - HTTP Request: POST https://api.together.xyz/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "\u001b[92m12:51:31 - LiteLLM:INFO\u001b[0m: utils.py:3119 - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.3-70B-Instruct-Turbo; provider = together_ai\n",
      "2025-06-25 12:51:31,533 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.3-70B-Instruct-Turbo; provider = together_ai\n",
      "2025-06-25 12:51:31,625 - httpx - INFO - HTTP Request: POST https://api.together.xyz/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m12:51:31 - LiteLLM:INFO\u001b[0m: utils.py:1215 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-25 12:51:31,626 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m12:51:31 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "2025-06-25 12:51:31,626 - LiteLLM - INFO - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "\u001b[92m12:51:31 - LiteLLM:INFO\u001b[0m: utils.py:3119 - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.3-70B-Instruct-Turbo; provider = together_ai\n",
      "Average Metric: 21.33 / 26 (82.1%):  52%|‚ñà‚ñà‚ñà‚ñã   | 26/50 [00:03<00:01, 12.50it/s]2025-06-25 12:51:31,633 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.3-70B-Instruct-Turbo; provider = together_ai\n",
      "/Users/ytchen/Documents/experimental/Llama4/env/lib/python3.13/site-packages/pydantic/main.py:463: UserWarning: Pydantic serializer warnings:\n",
      "  PydanticSerializationUnexpectedValue(Expected 9 fields but got 5: Expected `Message` - serialized value may not be as expected [input_value=Message(content='[[ ## an...ields={'refusal': None}), input_type=Message])\n",
      "  PydanticSerializationUnexpectedValue(Expected `StreamingChoices` - serialized value may not be as expected [input_value=Choices(finish_reason='st...: 17543586681849821000}), input_type=Choices])\n",
      "  return self.__pydantic_serializer__.to_python(\n",
      "\u001b[92m12:51:31 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "2025-06-25 12:51:31,649 - LiteLLM - INFO - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "2025-06-25 12:51:31,656 - httpx - INFO - HTTP Request: POST https://api.together.xyz/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-06-25 12:51:31,656 - httpx - INFO - HTTP Request: POST https://api.together.xyz/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-25 12:51:31,656 - httpx - INFO - HTTP Request: POST https://api.together.xyz/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "\u001b[92m12:51:31 - LiteLLM:INFO\u001b[0m: utils.py:3119 - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.3-70B-Instruct-Turbo; provider = together_ai\n",
      "2025-06-25 12:51:31,665 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.3-70B-Instruct-Turbo; provider = together_ai\n",
      "2025-06-25 12:51:31,673 - httpx - INFO - HTTP Request: POST https://api.together.xyz/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "\u001b[92m12:51:31 - LiteLLM:INFO\u001b[0m: utils.py:3119 - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.3-70B-Instruct-Turbo; provider = together_ai\n",
      "2025-06-25 12:51:31,680 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.3-70B-Instruct-Turbo; provider = together_ai\n",
      "\u001b[92m12:51:31 - LiteLLM:INFO\u001b[0m: utils.py:1215 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-25 12:51:31,688 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m12:51:31 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "2025-06-25 12:51:31,689 - LiteLLM - INFO - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "/Users/ytchen/Documents/experimental/Llama4/env/lib/python3.13/site-packages/pydantic/main.py:463: UserWarning: Pydantic serializer warnings:\n",
      "  PydanticSerializationUnexpectedValue(Expected 9 fields but got 5: Expected `Message` - serialized value may not be as expected [input_value=Message(content='[[ ## an...ields={'refusal': None}), input_type=Message])\n",
      "  PydanticSerializationUnexpectedValue(Expected `StreamingChoices` - serialized value may not be as expected [input_value=Choices(finish_reason='st...': 8367960873216088000}), input_type=Choices])\n",
      "  return self.__pydantic_serializer__.to_python(\n",
      "\u001b[92m12:51:31 - LiteLLM:INFO\u001b[0m: utils.py:3119 - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.3-70B-Instruct-Turbo; provider = together_ai\n",
      "2025-06-25 12:51:31,702 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.3-70B-Instruct-Turbo; provider = together_ai\n",
      "\u001b[92m12:51:31 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "2025-06-25 12:51:31,708 - LiteLLM - INFO - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "Average Metric: 22.30 / 27 (82.6%):  52%|‚ñà‚ñà‚ñà‚ñã   | 26/50 [00:03<00:01, 12.50it/s]\u001b[92m12:51:31 - LiteLLM:INFO\u001b[0m: utils.py:3119 - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.3-70B-Instruct-Turbo; provider = together_ai\n",
      "2025-06-25 12:51:31,729 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.3-70B-Instruct-Turbo; provider = together_ai\n",
      "2025-06-25 12:51:31,765 - httpx - INFO - HTTP Request: POST https://api.together.xyz/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m12:51:31 - LiteLLM:INFO\u001b[0m: utils.py:1215 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-25 12:51:31,765 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m12:51:31 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "2025-06-25 12:51:31,765 - LiteLLM - INFO - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "/Users/ytchen/Documents/experimental/Llama4/env/lib/python3.13/site-packages/pydantic/main.py:463: UserWarning: Pydantic serializer warnings:\n",
      "  PydanticSerializationUnexpectedValue(Expected 9 fields but got 5: Expected `Message` - serialized value may not be as expected [input_value=Message(content='[[ ## an...ields={'refusal': None}), input_type=Message])\n",
      "  PydanticSerializationUnexpectedValue(Expected `StreamingChoices` - serialized value may not be as expected [input_value=Choices(finish_reason='st...: 16088613858099090000}), input_type=Choices])\n",
      "  return self.__pydantic_serializer__.to_python(\n",
      "\u001b[92m12:51:31 - LiteLLM:INFO\u001b[0m: utils.py:3119 - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.3-70B-Instruct-Turbo; provider = together_ai\n",
      "2025-06-25 12:51:31,779 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.3-70B-Instruct-Turbo; provider = together_ai\n",
      "2025-06-25 12:51:31,785 - httpx - INFO - HTTP Request: POST https://api.together.xyz/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "Average Metric: 23.20 / 28 (82.9%):  56%|‚ñà‚ñà‚ñà‚ñâ   | 28/50 [00:03<00:01, 12.66it/s]\u001b[92m12:51:31 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "2025-06-25 12:51:31,786 - LiteLLM - INFO - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "\u001b[92m12:51:31 - LiteLLM:INFO\u001b[0m: utils.py:3119 - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.3-70B-Instruct-Turbo; provider = together_ai\n",
      "2025-06-25 12:51:31,801 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.3-70B-Instruct-Turbo; provider = together_ai\n",
      "2025-06-25 12:51:31,808 - httpx - INFO - HTTP Request: POST https://api.together.xyz/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m12:51:31 - LiteLLM:INFO\u001b[0m: utils.py:1215 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-25 12:51:31,810 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m12:51:31 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "2025-06-25 12:51:31,810 - LiteLLM - INFO - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "\u001b[92m12:51:31 - LiteLLM:INFO\u001b[0m: utils.py:3119 - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.3-70B-Instruct-Turbo; provider = together_ai\n",
      "2025-06-25 12:51:31,818 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.3-70B-Instruct-Turbo; provider = together_ai\n",
      "2025-06-25 12:51:31,823 - httpx - INFO - HTTP Request: POST https://api.together.xyz/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "/Users/ytchen/Documents/experimental/Llama4/env/lib/python3.13/site-packages/pydantic/main.py:463: UserWarning: Pydantic serializer warnings:\n",
      "  PydanticSerializationUnexpectedValue(Expected 9 fields but got 5: Expected `Message` - serialized value may not be as expected [input_value=Message(content='[[ ## an...ields={'refusal': None}), input_type=Message])\n",
      "  PydanticSerializationUnexpectedValue(Expected `StreamingChoices` - serialized value may not be as expected [input_value=Choices(finish_reason='st...': 7425336236425187000}), input_type=Choices])\n",
      "  return self.__pydantic_serializer__.to_python(\n",
      "Average Metric: 23.83 / 29 (82.2%):  56%|‚ñà‚ñà‚ñà‚ñâ   | 28/50 [00:03<00:01, 12.66it/s]\u001b[92m12:51:31 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "2025-06-25 12:51:31,832 - LiteLLM - INFO - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "\u001b[92m12:51:31 - LiteLLM:INFO\u001b[0m: utils.py:1215 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-25 12:51:31,839 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m12:51:31 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "2025-06-25 12:51:31,839 - LiteLLM - INFO - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "Average Metric: 24.47 / 30 (81.6%):  58%|‚ñà‚ñà‚ñà‚ñà   | 29/50 [00:03<00:01, 12.66it/s]/Users/ytchen/Documents/experimental/Llama4/env/lib/python3.13/site-packages/pydantic/main.py:463: UserWarning: Pydantic serializer warnings:\n",
      "  PydanticSerializationUnexpectedValue(Expected 9 fields but got 5: Expected `Message` - serialized value may not be as expected [input_value=Message(content='[[ ## an...ields={'refusal': None}), input_type=Message])\n",
      "  PydanticSerializationUnexpectedValue(Expected `StreamingChoices` - serialized value may not be as expected [input_value=Choices(finish_reason='st...: 13421208075312812000}), input_type=Choices])\n",
      "  return self.__pydantic_serializer__.to_python(\n",
      "\u001b[92m12:51:31 - LiteLLM:INFO\u001b[0m: utils.py:3119 - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.3-70B-Instruct-Turbo; provider = together_ai\n",
      "2025-06-25 12:51:31,853 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.3-70B-Instruct-Turbo; provider = together_ai\n",
      "\u001b[92m12:51:31 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "2025-06-25 12:51:31,859 - LiteLLM - INFO - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "2025-06-25 12:51:31,866 - httpx - INFO - HTTP Request: POST https://api.together.xyz/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-25 12:51:31,866 - httpx - INFO - HTTP Request: POST https://api.together.xyz/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-25 12:51:31,866 - httpx - INFO - HTTP Request: POST https://api.together.xyz/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-06-25 12:51:31,878 - httpx - INFO - HTTP Request: POST https://api.together.xyz/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "\u001b[92m12:51:31 - LiteLLM:INFO\u001b[0m: utils.py:3119 - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.3-70B-Instruct-Turbo; provider = together_ai\n",
      "2025-06-25 12:51:31,885 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.3-70B-Instruct-Turbo; provider = together_ai\n",
      "2025-06-25 12:51:31,893 - httpx - INFO - HTTP Request: POST https://api.together.xyz/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m12:51:31 - LiteLLM:INFO\u001b[0m: utils.py:3119 - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.3-70B-Instruct-Turbo; provider = together_ai\n",
      "2025-06-25 12:51:31,905 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.3-70B-Instruct-Turbo; provider = together_ai\n",
      "\u001b[92m12:51:31 - LiteLLM:INFO\u001b[0m: utils.py:1215 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-25 12:51:31,914 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m12:51:31 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "2025-06-25 12:51:31,914 - LiteLLM - INFO - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "/Users/ytchen/Documents/experimental/Llama4/env/lib/python3.13/site-packages/pydantic/main.py:463: UserWarning: Pydantic serializer warnings:\n",
      "  PydanticSerializationUnexpectedValue(Expected 9 fields but got 5: Expected `Message` - serialized value may not be as expected [input_value=Message(content='[[ ## an...ields={'refusal': None}), input_type=Message])\n",
      "  PydanticSerializationUnexpectedValue(Expected `StreamingChoices` - serialized value may not be as expected [input_value=Choices(finish_reason='st...: 18239640370908633000}), input_type=Choices])\n",
      "  return self.__pydantic_serializer__.to_python(\n",
      "\u001b[92m12:51:31 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "2025-06-25 12:51:31,928 - LiteLLM - INFO - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "Average Metric: 25.43 / 31 (82.0%):  62%|‚ñà‚ñà‚ñà‚ñà‚ñé  | 31/50 [00:03<00:01, 14.55it/s]2025-06-25 12:51:31,934 - httpx - INFO - HTTP Request: POST https://api.together.xyz/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m12:51:31 - LiteLLM:INFO\u001b[0m: utils.py:3119 - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.3-70B-Instruct-Turbo; provider = together_ai\n",
      "2025-06-25 12:51:31,936 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.3-70B-Instruct-Turbo; provider = together_ai\n",
      "2025-06-25 12:51:31,944 - httpx - INFO - HTTP Request: POST https://api.together.xyz/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m12:51:31 - LiteLLM:INFO\u001b[0m: utils.py:3119 - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.3-70B-Instruct-Turbo; provider = together_ai\n",
      "2025-06-25 12:51:31,959 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.3-70B-Instruct-Turbo; provider = together_ai\n",
      "\u001b[92m12:51:31 - LiteLLM:INFO\u001b[0m: utils.py:1215 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-25 12:51:31,968 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m12:51:31 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "2025-06-25 12:51:31,968 - LiteLLM - INFO - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "Average Metric: 26.40 / 32 (82.5%):  62%|‚ñà‚ñà‚ñà‚ñà‚ñé  | 31/50 [00:03<00:01, 14.55it/s]\u001b[92m12:51:31 - LiteLLM:INFO\u001b[0m: utils.py:3119 - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.3-70B-Instruct-Turbo; provider = together_ai\n",
      "2025-06-25 12:51:31,975 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.3-70B-Instruct-Turbo; provider = together_ai\n",
      "/Users/ytchen/Documents/experimental/Llama4/env/lib/python3.13/site-packages/pydantic/main.py:463: UserWarning: Pydantic serializer warnings:\n",
      "  PydanticSerializationUnexpectedValue(Expected 9 fields but got 5: Expected `Message` - serialized value may not be as expected [input_value=Message(content='[[ ## an...ields={'refusal': None}), input_type=Message])\n",
      "  PydanticSerializationUnexpectedValue(Expected `StreamingChoices` - serialized value may not be as expected [input_value=Choices(finish_reason='st...': 2838888113168728000}), input_type=Choices])\n",
      "  return self.__pydantic_serializer__.to_python(\n",
      "2025-06-25 12:51:31,989 - httpx - INFO - HTTP Request: POST https://api.together.xyz/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m12:51:31 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "2025-06-25 12:51:31,990 - LiteLLM - INFO - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "\u001b[92m12:51:31 - LiteLLM:INFO\u001b[0m: utils.py:1215 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-25 12:51:31,998 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m12:51:31 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "2025-06-25 12:51:31,998 - LiteLLM - INFO - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "Average Metric: 27.07 / 33 (82.0%):  64%|‚ñà‚ñà‚ñà‚ñà‚ñç  | 32/50 [00:03<00:01, 14.55it/s]/Users/ytchen/Documents/experimental/Llama4/env/lib/python3.13/site-packages/pydantic/main.py:463: UserWarning: Pydantic serializer warnings:\n",
      "  PydanticSerializationUnexpectedValue(Expected 9 fields but got 5: Expected `Message` - serialized value may not be as expected [input_value=Message(content='[[ ## an...ields={'refusal': None}), input_type=Message])\n",
      "  PydanticSerializationUnexpectedValue(Expected `StreamingChoices` - serialized value may not be as expected [input_value=Choices(finish_reason='st...: 15120896089069140000}), input_type=Choices])\n",
      "  return self.__pydantic_serializer__.to_python(\n",
      "\u001b[92m12:51:32 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "2025-06-25 12:51:32,005 - LiteLLM - INFO - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "\u001b[92m12:51:32 - LiteLLM:INFO\u001b[0m: utils.py:1215 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-25 12:51:32,013 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m12:51:32 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "2025-06-25 12:51:32,013 - LiteLLM - INFO - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "Average Metric: 27.70 / 34 (81.5%):  66%|‚ñà‚ñà‚ñà‚ñà‚ñå  | 33/50 [00:03<00:01, 14.55it/s]/Users/ytchen/Documents/experimental/Llama4/env/lib/python3.13/site-packages/pydantic/main.py:463: UserWarning: Pydantic serializer warnings:\n",
      "  PydanticSerializationUnexpectedValue(Expected 9 fields but got 5: Expected `Message` - serialized value may not be as expected [input_value=Message(content='[[ ## an...ields={'refusal': None}), input_type=Message])\n",
      "  PydanticSerializationUnexpectedValue(Expected `StreamingChoices` - serialized value may not be as expected [input_value=Choices(finish_reason='st...': 5496884992185789000}), input_type=Choices])\n",
      "  return self.__pydantic_serializer__.to_python(\n",
      "\u001b[92m12:51:32 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "2025-06-25 12:51:32,020 - LiteLLM - INFO - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "\u001b[92m12:51:32 - LiteLLM:INFO\u001b[0m: utils.py:1215 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-25 12:51:32,027 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m12:51:32 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "2025-06-25 12:51:32,028 - LiteLLM - INFO - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "/Users/ytchen/Documents/experimental/Llama4/env/lib/python3.13/site-packages/pydantic/main.py:463: UserWarning: Pydantic serializer warnings:\n",
      "  PydanticSerializationUnexpectedValue(Expected 9 fields but got 5: Expected `Message` - serialized value may not be as expected [input_value=Message(content='[[ ## an...ields={'refusal': None}), input_type=Message])\n",
      "  PydanticSerializationUnexpectedValue(Expected `StreamingChoices` - serialized value may not be as expected [input_value=Choices(finish_reason='st...': 3031123266952216600}), input_type=Choices])\n",
      "  return self.__pydantic_serializer__.to_python(\n",
      "\u001b[92m12:51:32 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "2025-06-25 12:51:32,034 - LiteLLM - INFO - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "Average Metric: 28.63 / 35 (81.8%):  70%|‚ñà‚ñà‚ñà‚ñà‚ñâ  | 35/50 [00:03<00:00, 19.26it/s]2025-06-25 12:51:32,100 - httpx - INFO - HTTP Request: POST https://api.together.xyz/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m12:51:32 - LiteLLM:INFO\u001b[0m: utils.py:1215 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-25 12:51:32,101 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m12:51:32 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "2025-06-25 12:51:32,101 - LiteLLM - INFO - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "/Users/ytchen/Documents/experimental/Llama4/env/lib/python3.13/site-packages/pydantic/main.py:463: UserWarning: Pydantic serializer warnings:\n",
      "  PydanticSerializationUnexpectedValue(Expected 9 fields but got 5: Expected `Message` - serialized value may not be as expected [input_value=Message(content='[[ ## an...ields={'refusal': None}), input_type=Message])\n",
      "  PydanticSerializationUnexpectedValue(Expected `StreamingChoices` - serialized value may not be as expected [input_value=Choices(finish_reason='st...: 16857664730020840000}), input_type=Choices])\n",
      "  return self.__pydantic_serializer__.to_python(\n",
      "Average Metric: 29.27 / 36 (81.3%):  70%|‚ñà‚ñà‚ñà‚ñà‚ñâ  | 35/50 [00:03<00:00, 19.26it/s]\u001b[92m12:51:32 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "2025-06-25 12:51:32,108 - LiteLLM - INFO - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "2025-06-25 12:51:32,723 - httpx - INFO - HTTP Request: POST https://api.together.xyz/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-06-25 12:51:32,728 - httpx - INFO - HTTP Request: POST https://api.together.xyz/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-06-25 12:51:32,729 - httpx - INFO - HTTP Request: POST https://api.together.xyz/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-06-25 12:51:32,729 - httpx - INFO - HTTP Request: POST https://api.together.xyz/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-06-25 12:51:32,730 - httpx - INFO - HTTP Request: POST https://api.together.xyz/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m12:51:32 - LiteLLM:INFO\u001b[0m: utils.py:1215 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-25 12:51:32,740 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m12:51:32 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "2025-06-25 12:51:32,744 - LiteLLM - INFO - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "/Users/ytchen/Documents/experimental/Llama4/env/lib/python3.13/site-packages/pydantic/main.py:463: UserWarning: Pydantic serializer warnings:\n",
      "  PydanticSerializationUnexpectedValue(Expected 9 fields but got 5: Expected `Message` - serialized value may not be as expected [input_value=Message(content='[[ ## an...ields={'refusal': None}), input_type=Message])\n",
      "  PydanticSerializationUnexpectedValue(Expected `StreamingChoices` - serialized value may not be as expected [input_value=Choices(finish_reason='st...': 8403785419891296000}), input_type=Choices])\n",
      "  return self.__pydantic_serializer__.to_python(\n",
      "Average Metric: 30.20 / 37 (81.6%):  72%|‚ñà‚ñà‚ñà‚ñà‚ñà  | 36/50 [00:04<00:00, 19.26it/s]\u001b[92m12:51:32 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "2025-06-25 12:51:32,767 - LiteLLM - INFO - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "\u001b[92m12:51:32 - LiteLLM:INFO\u001b[0m: utils.py:1215 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-25 12:51:32,780 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m12:51:32 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "2025-06-25 12:51:32,781 - LiteLLM - INFO - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "/Users/ytchen/Documents/experimental/Llama4/env/lib/python3.13/site-packages/pydantic/main.py:463: UserWarning: Pydantic serializer warnings:\n",
      "  PydanticSerializationUnexpectedValue(Expected 9 fields but got 5: Expected `Message` - serialized value may not be as expected [input_value=Message(content='[[ ## an...ields={'refusal': None}), input_type=Message])\n",
      "  PydanticSerializationUnexpectedValue(Expected `StreamingChoices` - serialized value may not be as expected [input_value=Choices(finish_reason='st...d': 467902406555594200}), input_type=Choices])\n",
      "  return self.__pydantic_serializer__.to_python(\n",
      "Average Metric: 30.83 / 38 (81.1%):  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 38/50 [00:04<00:01,  9.01it/s]\u001b[92m12:51:32 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "2025-06-25 12:51:32,790 - LiteLLM - INFO - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "\u001b[92m12:51:32 - LiteLLM:INFO\u001b[0m: utils.py:1215 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-25 12:51:32,800 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m12:51:32 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "2025-06-25 12:51:32,801 - LiteLLM - INFO - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "Average Metric: 31.80 / 39 (81.5%):  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 38/50 [00:04<00:01,  9.01it/s]/Users/ytchen/Documents/experimental/Llama4/env/lib/python3.13/site-packages/pydantic/main.py:463: UserWarning: Pydantic serializer warnings:\n",
      "  PydanticSerializationUnexpectedValue(Expected 9 fields but got 5: Expected `Message` - serialized value may not be as expected [input_value=Message(content='[[ ## an...ields={'refusal': None}), input_type=Message])\n",
      "  PydanticSerializationUnexpectedValue(Expected `StreamingChoices` - serialized value may not be as expected [input_value=Choices(finish_reason='st...': 9527090616779300000}), input_type=Choices])\n",
      "  return self.__pydantic_serializer__.to_python(\n",
      "\u001b[92m12:51:32 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "2025-06-25 12:51:32,809 - LiteLLM - INFO - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "\u001b[92m12:51:32 - LiteLLM:INFO\u001b[0m: utils.py:1215 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-25 12:51:32,817 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m12:51:32 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "2025-06-25 12:51:32,817 - LiteLLM - INFO - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "/Users/ytchen/Documents/experimental/Llama4/env/lib/python3.13/site-packages/pydantic/main.py:463: UserWarning: Pydantic serializer warnings:\n",
      "  PydanticSerializationUnexpectedValue(Expected 9 fields but got 5: Expected `Message` - serialized value may not be as expected [input_value=Message(content='[[ ## an...ields={'refusal': None}), input_type=Message])\n",
      "  PydanticSerializationUnexpectedValue(Expected `StreamingChoices` - serialized value may not be as expected [input_value=Choices(finish_reason='st...': 3026135925476848000}), input_type=Choices])\n",
      "  return self.__pydantic_serializer__.to_python(\n",
      "Average Metric: 32.40 / 40 (81.0%):  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 39/50 [00:04<00:01,  9.01it/s]\u001b[92m12:51:32 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "2025-06-25 12:51:32,826 - LiteLLM - INFO - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "\u001b[92m12:51:32 - LiteLLM:INFO\u001b[0m: utils.py:1215 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-25 12:51:32,833 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m12:51:32 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "2025-06-25 12:51:32,834 - LiteLLM - INFO - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "Average Metric: 33.33 / 41 (81.3%):  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 40/50 [00:04<00:01,  9.01it/s]/Users/ytchen/Documents/experimental/Llama4/env/lib/python3.13/site-packages/pydantic/main.py:463: UserWarning: Pydantic serializer warnings:\n",
      "  PydanticSerializationUnexpectedValue(Expected 9 fields but got 5: Expected `Message` - serialized value may not be as expected [input_value=Message(content='[[ ## an...ields={'refusal': None}), input_type=Message])\n",
      "  PydanticSerializationUnexpectedValue(Expected `StreamingChoices` - serialized value may not be as expected [input_value=Choices(finish_reason='st...': 8915894569663110000}), input_type=Choices])\n",
      "  return self.__pydantic_serializer__.to_python(\n",
      "\u001b[92m12:51:32 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "2025-06-25 12:51:32,841 - LiteLLM - INFO - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "2025-06-25 12:51:32,855 - httpx - INFO - HTTP Request: POST https://api.together.xyz/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m12:51:32 - LiteLLM:INFO\u001b[0m: utils.py:1215 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-25 12:51:32,856 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m12:51:32 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "2025-06-25 12:51:32,856 - LiteLLM - INFO - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "/Users/ytchen/Documents/experimental/Llama4/env/lib/python3.13/site-packages/pydantic/main.py:463: UserWarning: Pydantic serializer warnings:\n",
      "  PydanticSerializationUnexpectedValue(Expected 9 fields but got 5: Expected `Message` - serialized value may not be as expected [input_value=Message(content='[[ ## an...ields={'refusal': None}), input_type=Message])\n",
      "  PydanticSerializationUnexpectedValue(Expected `StreamingChoices` - serialized value may not be as expected [input_value=Choices(finish_reason='st...d': 947042083949031800}), input_type=Choices])\n",
      "  return self.__pydantic_serializer__.to_python(\n",
      "\u001b[92m12:51:32 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "2025-06-25 12:51:32,863 - LiteLLM - INFO - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "Average Metric: 34.27 / 42 (81.6%):  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 41/50 [00:04<00:00,  9.01it/s]2025-06-25 12:51:32,934 - httpx - INFO - HTTP Request: POST https://api.together.xyz/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m12:51:32 - LiteLLM:INFO\u001b[0m: utils.py:1215 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-25 12:51:32,935 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m12:51:32 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "2025-06-25 12:51:32,935 - LiteLLM - INFO - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "/Users/ytchen/Documents/experimental/Llama4/env/lib/python3.13/site-packages/pydantic/main.py:463: UserWarning: Pydantic serializer warnings:\n",
      "  PydanticSerializationUnexpectedValue(Expected 9 fields but got 5: Expected `Message` - serialized value may not be as expected [input_value=Message(content='[[ ## an...ields={'refusal': None}), input_type=Message])\n",
      "  PydanticSerializationUnexpectedValue(Expected `StreamingChoices` - serialized value may not be as expected [input_value=Choices(finish_reason='st...': 6911825942384374000}), input_type=Choices])\n",
      "  return self.__pydantic_serializer__.to_python(\n",
      "\u001b[92m12:51:32 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "2025-06-25 12:51:32,942 - LiteLLM - INFO - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "Average Metric: 35.27 / 43 (82.0%):  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 42/50 [00:04<00:00,  9.01it/s]2025-06-25 12:51:32,950 - httpx - INFO - HTTP Request: POST https://api.together.xyz/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Average Metric: 35.27 / 43 (82.0%):  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 43/50 [00:04<00:00, 12.77it/s]\u001b[92m12:51:32 - LiteLLM:INFO\u001b[0m: utils.py:1215 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-25 12:51:32,951 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m12:51:32 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "2025-06-25 12:51:32,952 - LiteLLM - INFO - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "Average Metric: 35.93 / 44 (81.7%):  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 43/50 [00:04<00:00, 12.77it/s]/Users/ytchen/Documents/experimental/Llama4/env/lib/python3.13/site-packages/pydantic/main.py:463: UserWarning: Pydantic serializer warnings:\n",
      "  PydanticSerializationUnexpectedValue(Expected 9 fields but got 5: Expected `Message` - serialized value may not be as expected [input_value=Message(content='[[ ## an...ields={'refusal': None}), input_type=Message])\n",
      "  PydanticSerializationUnexpectedValue(Expected `StreamingChoices` - serialized value may not be as expected [input_value=Choices(finish_reason='st...': 8088668289346695000}), input_type=Choices])\n",
      "  return self.__pydantic_serializer__.to_python(\n",
      "\u001b[92m12:51:32 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "2025-06-25 12:51:32,959 - LiteLLM - INFO - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "2025-06-25 12:51:32,966 - httpx - INFO - HTTP Request: POST https://api.together.xyz/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m12:51:32 - LiteLLM:INFO\u001b[0m: utils.py:1215 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-25 12:51:32,968 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m12:51:32 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "2025-06-25 12:51:32,968 - LiteLLM - INFO - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "/Users/ytchen/Documents/experimental/Llama4/env/lib/python3.13/site-packages/pydantic/main.py:463: UserWarning: Pydantic serializer warnings:\n",
      "  PydanticSerializationUnexpectedValue(Expected 9 fields but got 5: Expected `Message` - serialized value may not be as expected [input_value=Message(content='[[ ## an...ields={'refusal': None}), input_type=Message])\n",
      "  PydanticSerializationUnexpectedValue(Expected `StreamingChoices` - serialized value may not be as expected [input_value=Choices(finish_reason='st...: 10449896886116410000}), input_type=Choices])\n",
      "  return self.__pydantic_serializer__.to_python(\n",
      "Average Metric: 36.93 / 45 (82.1%):  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 44/50 [00:04<00:00, 12.77it/s]\u001b[92m12:51:32 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "2025-06-25 12:51:32,977 - LiteLLM - INFO - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "2025-06-25 12:51:33,004 - httpx - INFO - HTTP Request: POST https://api.together.xyz/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m12:51:33 - LiteLLM:INFO\u001b[0m: utils.py:1215 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-25 12:51:33,005 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m12:51:33 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "2025-06-25 12:51:33,005 - LiteLLM - INFO - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "/Users/ytchen/Documents/experimental/Llama4/env/lib/python3.13/site-packages/pydantic/main.py:463: UserWarning: Pydantic serializer warnings:\n",
      "  PydanticSerializationUnexpectedValue(Expected 9 fields but got 5: Expected `Message` - serialized value may not be as expected [input_value=Message(content='[[ ## an...ields={'refusal': None}), input_type=Message])\n",
      "  PydanticSerializationUnexpectedValue(Expected `StreamingChoices` - serialized value may not be as expected [input_value=Choices(finish_reason='st...: 16530244326079877000}), input_type=Choices])\n",
      "  return self.__pydantic_serializer__.to_python(\n",
      "Average Metric: 37.90 / 46 (82.4%):  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 45/50 [00:04<00:00, 12.77it/s]\u001b[92m12:51:33 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "2025-06-25 12:51:33,012 - LiteLLM - INFO - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "2025-06-25 12:51:33,040 - httpx - INFO - HTTP Request: POST https://api.together.xyz/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m12:51:33 - LiteLLM:INFO\u001b[0m: utils.py:1215 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-25 12:51:33,041 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m12:51:33 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "2025-06-25 12:51:33,041 - LiteLLM - INFO - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "Average Metric: 38.90 / 47 (82.8%):  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 46/50 [00:04<00:00, 12.77it/s]/Users/ytchen/Documents/experimental/Llama4/env/lib/python3.13/site-packages/pydantic/main.py:463: UserWarning: Pydantic serializer warnings:\n",
      "  PydanticSerializationUnexpectedValue(Expected 9 fields but got 5: Expected `Message` - serialized value may not be as expected [input_value=Message(content='[[ ## an...ields={'refusal': None}), input_type=Message])\n",
      "  PydanticSerializationUnexpectedValue(Expected `StreamingChoices` - serialized value may not be as expected [input_value=Choices(finish_reason='st...: 12459824675050613000}), input_type=Choices])\n",
      "  return self.__pydantic_serializer__.to_python(\n",
      "\u001b[92m12:51:33 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "2025-06-25 12:51:33,048 - LiteLLM - INFO - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "2025-06-25 12:51:33,098 - httpx - INFO - HTTP Request: POST https://api.together.xyz/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m12:51:33 - LiteLLM:INFO\u001b[0m: utils.py:1215 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-25 12:51:33,099 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m12:51:33 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "2025-06-25 12:51:33,099 - LiteLLM - INFO - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "Average Metric: 39.87 / 48 (83.1%):  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 47/50 [00:04<00:00, 12.77it/s]/Users/ytchen/Documents/experimental/Llama4/env/lib/python3.13/site-packages/pydantic/main.py:463: UserWarning: Pydantic serializer warnings:\n",
      "  PydanticSerializationUnexpectedValue(Expected 9 fields but got 5: Expected `Message` - serialized value may not be as expected [input_value=Message(content='[[ ## an...ields={'refusal': None}), input_type=Message])\n",
      "  PydanticSerializationUnexpectedValue(Expected `StreamingChoices` - serialized value may not be as expected [input_value=Choices(finish_reason='st...': 6883804066130235000}), input_type=Choices])\n",
      "  return self.__pydantic_serializer__.to_python(\n",
      "Average Metric: 39.87 / 48 (83.1%):  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 48/50 [00:04<00:00, 16.48it/s]\u001b[92m12:51:33 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "2025-06-25 12:51:33,106 - LiteLLM - INFO - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "2025-06-25 12:51:33,440 - httpx - INFO - HTTP Request: POST https://api.together.xyz/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m12:51:33 - LiteLLM:INFO\u001b[0m: utils.py:1215 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-25 12:51:33,445 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m12:51:33 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "2025-06-25 12:51:33,446 - LiteLLM - INFO - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "/Users/ytchen/Documents/experimental/Llama4/env/lib/python3.13/site-packages/pydantic/main.py:463: UserWarning: Pydantic serializer warnings:\n",
      "  PydanticSerializationUnexpectedValue(Expected 9 fields but got 5: Expected `Message` - serialized value may not be as expected [input_value=Message(content='[[ ## an...ields={'refusal': None}), input_type=Message])\n",
      "  PydanticSerializationUnexpectedValue(Expected `StreamingChoices` - serialized value may not be as expected [input_value=Choices(finish_reason='st...: 14748998745933840000}), input_type=Choices])\n",
      "  return self.__pydantic_serializer__.to_python(\n",
      "Average Metric: 40.53 / 49 (82.7%):  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 48/50 [00:04<00:00, 16.48it/s]2025-06-25 12:51:33,458 - httpx - INFO - HTTP Request: POST https://api.together.xyz/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m12:51:33 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "2025-06-25 12:51:33,469 - LiteLLM - INFO - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "\u001b[92m12:51:33 - LiteLLM:INFO\u001b[0m: utils.py:1215 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-25 12:51:33,486 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m12:51:33 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "2025-06-25 12:51:33,486 - LiteLLM - INFO - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "Average Metric: 41.17 / 50 (82.3%): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 50/50 [00:04<00:00, 10.03it/s]\n",
      "2025/06/25 12:51:33 INFO dspy.evaluate.evaluate: Average Metric: 41.166666666666664 / 50 (82.3%)\n",
      "2025/06/25 12:51:33 INFO dspy.teleprompt.mipro_optimizer_v2: Score: 82.33 with parameters ['Predictor 0: Instruction 2', 'Predictor 0: Few-Shot Set 1'].\n",
      "2025/06/25 12:51:33 INFO dspy.teleprompt.mipro_optimizer_v2: Scores so far: [79.8, 85.87, 82.33]\n",
      "2025/06/25 12:51:33 INFO dspy.teleprompt.mipro_optimizer_v2: Best score so far: 85.87\n",
      "/Users/ytchen/Documents/experimental/Llama4/env/lib/python3.13/site-packages/pydantic/main.py:463: UserWarning: Pydantic serializer warnings:\n",
      "  PydanticSerializationUnexpectedValue(Expected 9 fields but got 5: Expected `Message` - serialized value may not be as expected [input_value=Message(content='[[ ## an...ields={'refusal': None}), input_type=Message])\n",
      "  PydanticSerializationUnexpectedValue(Expected `StreamingChoices` - serialized value may not be as expected [input_value=Choices(finish_reason='st...: 16333612127311200000}), input_type=Choices])\n",
      "  return self.__pydantic_serializer__.to_python(\n",
      "2025/06/25 12:51:33 INFO dspy.teleprompt.mipro_optimizer_v2: =======================\n",
      "\n",
      "\n",
      "\u001b[92m12:51:33 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "2025/06/25 12:51:33 INFO dspy.teleprompt.mipro_optimizer_v2: ===== Trial 4 / 7 =====\n",
      "2025-06-25 12:51:33,502 - LiteLLM - INFO - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "\u001b[92m12:51:33 - LiteLLM:INFO\u001b[0m: utils.py:3119 - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.3-70B-Instruct-Turbo; provider = together_ai\n",
      "2025-06-25 12:51:33,527 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.3-70B-Instruct-Turbo; provider = together_ai\n",
      "\u001b[92m12:51:33 - LiteLLM:INFO\u001b[0m: utils.py:3119 - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.3-70B-Instruct-Turbo; provider = together_ai\n",
      "2025-06-25 12:51:33,542 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.3-70B-Instruct-Turbo; provider = together_ai\n",
      "\u001b[92m12:51:33 - LiteLLM:INFO\u001b[0m: utils.py:3119 - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.3-70B-Instruct-Turbo; provider = together_ai\n",
      "2025-06-25 12:51:33,544 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.3-70B-Instruct-Turbo; provider = together_ai\n",
      "\u001b[92m12:51:33 - LiteLLM:INFO\u001b[0m: utils.py:3119 - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.3-70B-Instruct-Turbo; provider = together_ai\n",
      "2025-06-25 12:51:33,559 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.3-70B-Instruct-Turbo; provider = together_ai\n",
      "\u001b[92m12:51:33 - LiteLLM:INFO\u001b[0m: utils.py:3119 - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.3-70B-Instruct-Turbo; provider = together_ai\n",
      "2025-06-25 12:51:33,566 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.3-70B-Instruct-Turbo; provider = together_ai\n",
      "\u001b[92m12:51:33 - LiteLLM:INFO\u001b[0m: utils.py:3119 - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.3-70B-Instruct-Turbo; provider = together_ai\n",
      "2025-06-25 12:51:33,574 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.3-70B-Instruct-Turbo; provider = together_ai\n",
      "\u001b[92m12:51:33 - LiteLLM:INFO\u001b[0m: utils.py:3119 - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.3-70B-Instruct-Turbo; provider = together_ai\n",
      "2025-06-25 12:51:33,597 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.3-70B-Instruct-Turbo; provider = together_ai\n",
      "\u001b[92m12:51:33 - LiteLLM:INFO\u001b[0m: utils.py:3119 - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.3-70B-Instruct-Turbo; provider = together_ai\n",
      "2025-06-25 12:51:33,598 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.3-70B-Instruct-Turbo; provider = together_ai\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[92m12:51:33 - LiteLLM:INFO\u001b[0m: utils.py:3119 - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.3-70B-Instruct-Turbo; provider = together_ai\n",
      "2025-06-25 12:51:33,619 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.3-70B-Instruct-Turbo; provider = together_ai\n",
      "\u001b[92m12:51:33 - LiteLLM:INFO\u001b[0m: utils.py:3119 - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.3-70B-Instruct-Turbo; provider = together_ai\n",
      "2025-06-25 12:51:33,633 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.3-70B-Instruct-Turbo; provider = together_ai\n",
      "\u001b[92m12:51:33 - LiteLLM:INFO\u001b[0m: utils.py:3119 - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.3-70B-Instruct-Turbo; provider = together_ai\n",
      "2025-06-25 12:51:33,635 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.3-70B-Instruct-Turbo; provider = together_ai\n",
      "\u001b[92m12:51:33 - LiteLLM:INFO\u001b[0m: utils.py:3119 - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.3-70B-Instruct-Turbo; provider = together_ai\n",
      "2025-06-25 12:51:33,635 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.3-70B-Instruct-Turbo; provider = together_ai\n",
      "\u001b[92m12:51:33 - LiteLLM:INFO\u001b[0m: utils.py:3119 - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.3-70B-Instruct-Turbo; provider = together_ai\n",
      "2025-06-25 12:51:33,649 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.3-70B-Instruct-Turbo; provider = together_ai\n",
      "\u001b[92m12:51:33 - LiteLLM:INFO\u001b[0m: utils.py:3119 - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.3-70B-Instruct-Turbo; provider = together_ai\n",
      "2025-06-25 12:51:33,664 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.3-70B-Instruct-Turbo; provider = together_ai\n",
      "\u001b[92m12:51:33 - LiteLLM:INFO\u001b[0m: utils.py:3119 - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.3-70B-Instruct-Turbo; provider = together_ai\n",
      "2025-06-25 12:51:33,686 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.3-70B-Instruct-Turbo; provider = together_ai\n",
      "\u001b[92m12:51:33 - LiteLLM:INFO\u001b[0m: utils.py:3119 - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.3-70B-Instruct-Turbo; provider = together_ai\n",
      "2025-06-25 12:51:33,687 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.3-70B-Instruct-Turbo; provider = together_ai\n",
      "\u001b[92m12:51:33 - LiteLLM:INFO\u001b[0m: utils.py:3119 - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.3-70B-Instruct-Turbo; provider = together_ai\n",
      "2025-06-25 12:51:33,696 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.3-70B-Instruct-Turbo; provider = together_ai\n",
      "\u001b[92m12:51:33 - LiteLLM:INFO\u001b[0m: utils.py:3119 - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.3-70B-Instruct-Turbo; provider = together_ai\n",
      "2025-06-25 12:51:33,703 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.3-70B-Instruct-Turbo; provider = together_ai\n",
      "2025-06-25 12:51:33,938 - httpx - INFO - HTTP Request: POST https://api.together.xyz/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "\u001b[92m12:51:33 - LiteLLM:INFO\u001b[0m: utils.py:3119 - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.3-70B-Instruct-Turbo; provider = together_ai\n",
      "2025-06-25 12:51:33,950 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.3-70B-Instruct-Turbo; provider = together_ai\n",
      "2025-06-25 12:51:34,932 - httpx - INFO - HTTP Request: POST https://api.together.xyz/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m12:51:34 - LiteLLM:INFO\u001b[0m: utils.py:1215 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-25 12:51:34,937 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m12:51:34 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "2025-06-25 12:51:34,938 - LiteLLM - INFO - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "\u001b[92m12:51:34 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "Average Metric: 0.60 / 1 (60.0%):   0%|                  | 0/50 [00:01<?, ?it/s]2025-06-25 12:51:34,938 - LiteLLM - INFO - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "/Users/ytchen/Documents/experimental/Llama4/env/lib/python3.13/site-packages/pydantic/main.py:463: UserWarning: Pydantic serializer warnings:\n",
      "  PydanticSerializationUnexpectedValue(Expected 9 fields but got 5: Expected `Message` - serialized value may not be as expected [input_value=Message(content='[[ ## an...ields={'refusal': None}), input_type=Message])\n",
      "  PydanticSerializationUnexpectedValue(Expected `StreamingChoices` - serialized value may not be as expected [input_value=Choices(finish_reason='st...: 15733075982665976000}), input_type=Choices])\n",
      "  return self.__pydantic_serializer__.to_python(\n",
      "Average Metric: 0.60 / 1 (60.0%):   2%|‚ñè         | 1/50 [00:01<01:01,  1.25s/it]2025-06-25 12:51:34,971 - httpx - INFO - HTTP Request: POST https://api.together.xyz/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m12:51:34 - LiteLLM:INFO\u001b[0m: utils.py:3119 - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.3-70B-Instruct-Turbo; provider = together_ai\n",
      "2025-06-25 12:51:34,981 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.3-70B-Instruct-Turbo; provider = together_ai\n",
      "2025-06-25 12:51:34,985 - httpx - INFO - HTTP Request: POST https://api.together.xyz/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m12:51:34 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "2025-06-25 12:51:34,986 - LiteLLM - INFO - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "2025-06-25 12:51:34,999 - httpx - INFO - HTTP Request: POST https://api.together.xyz/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m12:51:35 - LiteLLM:INFO\u001b[0m: utils.py:1215 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-25 12:51:35,004 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m12:51:35 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "2025-06-25 12:51:35,004 - LiteLLM - INFO - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "/Users/ytchen/Documents/experimental/Llama4/env/lib/python3.13/site-packages/pydantic/main.py:463: UserWarning: Pydantic serializer warnings:\n",
      "  PydanticSerializationUnexpectedValue(Expected 9 fields but got 5: Expected `Message` - serialized value may not be as expected [input_value=Message(content='[[ ## an...ields={'refusal': None}), input_type=Message])\n",
      "  PydanticSerializationUnexpectedValue(Expected `StreamingChoices` - serialized value may not be as expected [input_value=Choices(finish_reason='st...': 9550769056853467000}), input_type=Choices])\n",
      "  return self.__pydantic_serializer__.to_python(\n",
      "\u001b[92m12:51:35 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "2025-06-25 12:51:35,020 - LiteLLM - INFO - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "Average Metric: 1.20 / 2 (60.0%):   2%|‚ñè         | 1/50 [00:01<01:01,  1.25s/it]\u001b[92m12:51:35 - LiteLLM:INFO\u001b[0m: utils.py:3119 - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.3-70B-Instruct-Turbo; provider = together_ai\n",
      "2025-06-25 12:51:35,030 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.3-70B-Instruct-Turbo; provider = together_ai\n",
      "2025-06-25 12:51:35,038 - httpx - INFO - HTTP Request: POST https://api.together.xyz/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m12:51:35 - LiteLLM:INFO\u001b[0m: utils.py:1215 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-25 12:51:35,039 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m12:51:35 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "2025-06-25 12:51:35,039 - LiteLLM - INFO - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "\u001b[92m12:51:35 - LiteLLM:INFO\u001b[0m: utils.py:3119 - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.3-70B-Instruct-Turbo; provider = together_ai\n",
      "2025-06-25 12:51:35,046 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.3-70B-Instruct-Turbo; provider = together_ai\n",
      "/Users/ytchen/Documents/experimental/Llama4/env/lib/python3.13/site-packages/pydantic/main.py:463: UserWarning: Pydantic serializer warnings:\n",
      "  PydanticSerializationUnexpectedValue(Expected 9 fields but got 5: Expected `Message` - serialized value may not be as expected [input_value=Message(content='[[ ## an...ields={'refusal': None}), input_type=Message])\n",
      "  PydanticSerializationUnexpectedValue(Expected `StreamingChoices` - serialized value may not be as expected [input_value=Choices(finish_reason='st...: 14087596724193477000}), input_type=Choices])\n",
      "  return self.__pydantic_serializer__.to_python(\n",
      "^Cider = together_ai\n",
      "\n",
      "/Users/ytchen/Documents/experimental/Llama4/env/lib/python3.13/site-packages/pydantic/main.py:463: UserWarning: Pydantic serializer warnings:\n",
      "  PydanticSerializationUnexpectedValue(Expected 9 fields but got 5: Expected `Message` - serialized value may not be as expected [input_value=Message(content='[[ ## an...ields={'refusal': None}), input_type=Message])\n",
      "  PydanticSerializationUnexpectedValue(Expected `StreamingChoices` - serialized value may not be as expected [input_value=Choices(finish_reason='st...': 7713093636552914000}), input_type=Choices])\n",
      "  return self.__pydantic_serializer__.to_python(\n",
      "\u001b[92m12:51:35 - LiteLLM:INFO\u001b[0m: utils.py:1215 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-25 12:51:35,112 - httpx - INFO - HTTP Request: POST https://api.together.xyz/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m12:51:35 - LiteLLM:INFO\u001b[0m: utils.py:1215 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-25 12:51:35,107 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.3-70B-Instruct-Turbo; provider = together_ai\n",
      "2025/06/25 12:51:35 WARNING dspy.utils.parallelizer: SIGINT received. Cancelling.\n",
      "\u001b[92m12:51:35 - LiteLLM:INFO\u001b[0m: utils.py:1215 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-25 12:51:35,109 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m12:51:35 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "2025-06-25 12:51:35,109 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m12:51:35 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "2025-06-25 12:51:35,110 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-25 12:51:35,160 - LiteLLM - INFO - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "2025-06-25 12:51:35,169 - LiteLLM - INFO - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "\u001b[92m12:51:35 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "2025-06-25 12:51:35,169 - LiteLLM - INFO - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "\u001b[92m12:51:35 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "2025-06-25 12:51:35,169 - LiteLLM - INFO - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "/Users/ytchen/Documents/experimental/Llama4/env/lib/python3.13/site-packages/pydantic/main.py:463: UserWarning: Pydantic serializer warnings:\n",
      "  PydanticSerializationUnexpectedValue(Expected 9 fields but got 5: Expected `Message` - serialized value may not be as expected [input_value=Message(content='[[ ## an...ields={'refusal': None}), input_type=Message])\n",
      "  PydanticSerializationUnexpectedValue(Expected `StreamingChoices` - serialized value may not be as expected [input_value=Choices(finish_reason='st...: 15416390925086956000}), input_type=Choices])\n",
      "  return self.__pydantic_serializer__.to_python(\n",
      "\u001b[92m12:51:35 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "2025-06-25 12:51:35,208 - httpx - INFO - HTTP Request: POST https://api.together.xyz/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-06-25 12:51:35,169 - LiteLLM - INFO - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "\u001b[92m12:51:35 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "\u001b[92m12:51:35 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "/Users/ytchen/Documents/experimental/Llama4/env/lib/python3.13/site-packages/pydantic/main.py:463: UserWarning: Pydantic serializer warnings:\n",
      "  PydanticSerializationUnexpectedValue(Expected 9 fields but got 5: Expected `Message` - serialized value may not be as expected [input_value=Message(content='[[ ## an...ields={'refusal': None}), input_type=Message])\n",
      "  PydanticSerializationUnexpectedValue(Expected `StreamingChoices` - serialized value may not be as expected [input_value=Choices(finish_reason='st...': 3523463958299342300}), input_type=Choices])\n",
      "  return self.__pydantic_serializer__.to_python(\n",
      "\u001b[92m12:51:35 - LiteLLM:INFO\u001b[0m: utils.py:1215 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-25 12:51:35,169 - LiteLLM - INFO - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "/Users/ytchen/Documents/experimental/Llama4/env/lib/python3.13/site-packages/pydantic/main.py:463: UserWarning: Pydantic serializer warnings:\n",
      "  PydanticSerializationUnexpectedValue(Expected 9 fields but got 5: Expected `Message` - serialized value may not be as expected [input_value=Message(content='[[ ## an...ields={'refusal': None}), input_type=Message])\n",
      "  PydanticSerializationUnexpectedValue(Expected `StreamingChoices` - serialized value may not be as expected [input_value=Choices(finish_reason='st...': 9868674134072058000}), input_type=Choices])\n",
      "  return self.__pydantic_serializer__.to_python(\n",
      "\u001b[92m12:51:35 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "2025-06-25 12:51:35,189 - LiteLLM - INFO - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "\u001b[92m12:51:35 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "2025-06-25 12:51:35,200 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m12:51:35 - LiteLLM:INFO\u001b[0m: utils.py:1215 - Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m12:51:35 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "2025-06-25 12:51:35,211 - LiteLLM - INFO - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "\u001b[92m12:51:35 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "\u001b[92m12:51:35 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "2025-06-25 12:51:35,219 - LiteLLM - INFO - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "2025-06-25 12:51:35,220 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m12:51:35 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "2025-06-25 12:51:35,227 - LiteLLM - INFO - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "\u001b[92m12:51:35 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "2025-06-25 12:51:35,227 - LiteLLM - INFO - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "2025-06-25 12:51:35,228 - LiteLLM - INFO - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "/Users/ytchen/Documents/experimental/Llama4/env/lib/python3.13/site-packages/pydantic/main.py:463: UserWarning: Pydantic serializer warnings:\n",
      "  PydanticSerializationUnexpectedValue(Expected 9 fields but got 5: Expected `Message` - serialized value may not be as expected [input_value=Message(content='[[ ## an...ields={'refusal': None}), input_type=Message])\n",
      "  PydanticSerializationUnexpectedValue(Expected `StreamingChoices` - serialized value may not be as expected [input_value=Choices(finish_reason='st...: 15021567499563330000}), input_type=Choices])\n",
      "  return self.__pydantic_serializer__.to_python(\n",
      "2025-06-25 12:51:35,241 - LiteLLM - INFO - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "\u001b[92m12:51:35 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "2025-06-25 12:51:35,248 - LiteLLM - INFO - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "/Users/ytchen/Documents/experimental/Llama4/env/lib/python3.13/site-packages/pydantic/main.py:463: UserWarning: Pydantic serializer warnings:\n",
      "  PydanticSerializationUnexpectedValue(Expected 9 fields but got 5: Expected `Message` - serialized value may not be as expected [input_value=Message(content='[[ ## an...ields={'refusal': None}), input_type=Message])\n",
      "  PydanticSerializationUnexpectedValue(Expected `StreamingChoices` - serialized value may not be as expected [input_value=Choices(finish_reason='st...': 4771890100211098000}), input_type=Choices])\n",
      "  return self.__pydantic_serializer__.to_python(\n",
      "\u001b[92m12:51:35 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "2025-06-25 12:51:35,255 - LiteLLM - INFO - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "2025-06-25 12:51:35,255 - httpx - INFO - HTTP Request: POST https://api.together.xyz/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-06-25 12:51:35,262 - httpx - INFO - HTTP Request: POST https://api.together.xyz/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-06-25 12:51:35,262 - LiteLLM - INFO - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "\u001b[92m12:51:35 - LiteLLM:INFO\u001b[0m: utils.py:1215 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-25 12:51:35,276 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m12:51:35 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "2025-06-25 12:51:35,276 - LiteLLM - INFO - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "/Users/ytchen/Documents/experimental/Llama4/env/lib/python3.13/site-packages/pydantic/main.py:463: UserWarning: Pydantic serializer warnings:\n",
      "  PydanticSerializationUnexpectedValue(Expected 9 fields but got 5: Expected `Message` - serialized value may not be as expected [input_value=Message(content='[[ ## an...ields={'refusal': None}), input_type=Message])\n",
      "  PydanticSerializationUnexpectedValue(Expected `StreamingChoices` - serialized value may not be as expected [input_value=Choices(finish_reason='st...: 15803584635110908000}), input_type=Choices])\n",
      "  return self.__pydantic_serializer__.to_python(\n",
      "\u001b[92m12:51:35 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "2025-06-25 12:51:35,283 - LiteLLM - INFO - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "\u001b[92m12:51:35 - LiteLLM:INFO\u001b[0m: utils.py:1215 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-25 12:51:35,290 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m12:51:35 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "2025-06-25 12:51:35,290 - LiteLLM - INFO - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "/Users/ytchen/Documents/experimental/Llama4/env/lib/python3.13/site-packages/pydantic/main.py:463: UserWarning: Pydantic serializer warnings:\n",
      "  PydanticSerializationUnexpectedValue(Expected 9 fields but got 5: Expected `Message` - serialized value may not be as expected [input_value=Message(content='[[ ## an...ields={'refusal': None}), input_type=Message])\n",
      "  PydanticSerializationUnexpectedValue(Expected `StreamingChoices` - serialized value may not be as expected [input_value=Choices(finish_reason='st...: 13748991175812780000}), input_type=Choices])\n",
      "  return self.__pydantic_serializer__.to_python(\n",
      "\u001b[92m12:51:35 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "2025-06-25 12:51:35,296 - LiteLLM - INFO - selected model name for cost calculation: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "\u001b[33m[W 2025-06-25 12:51:35,169]\u001b[0m Trial 3 failed with parameters: {'0_predictor_instruction': 4, '0_predictor_demos': 1} because of the following error: KeyboardInterrupt().\u001b[0m\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/ytchen/Documents/experimental/Llama4/env/lib/python3.13/site-packages/optuna/study/_optimize.py\", line 201, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"/Users/ytchen/Documents/experimental/Llama4/env/lib/python3.13/site-packages/dspy/teleprompt/mipro_optimizer_v2.py\", line 522, in objective\n",
      "    score = eval_candidate_program(batch_size, valset, candidate_program, evaluate, self.rng)\n",
      "  File \"/Users/ytchen/Documents/experimental/Llama4/env/lib/python3.13/site-packages/dspy/teleprompt/utils.py\", line 52, in eval_candidate_program\n",
      "    return evaluate(candidate_program, devset=trainset, return_all_scores=return_all_scores, callback_metadata={\"metric_key\": \"eval_full\"})\n",
      "  File \"/Users/ytchen/Documents/experimental/Llama4/env/lib/python3.13/site-packages/dspy/utils/callback.py\", line 266, in wrapper\n",
      "    return fn(instance, *args, **kwargs)\n",
      "  File \"/Users/ytchen/Documents/experimental/Llama4/env/lib/python3.13/site-packages/dspy/evaluate/evaluate.py\", line 169, in __call__\n",
      "    results = executor.execute(process_item, devset)\n",
      "  File \"/Users/ytchen/Documents/experimental/Llama4/env/lib/python3.13/site-packages/dspy/utils/parallelizer.py\", line 45, in execute\n",
      "    return self._execute_parallel(wrapped, data)\n",
      "           ~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^\n",
      "  File \"/Users/ytchen/Documents/experimental/Llama4/env/lib/python3.13/site-packages/dspy/utils/parallelizer.py\", line 148, in _execute_parallel\n",
      "    done, not_done = wait(\n",
      "                     ~~~~^\n",
      "        futures_set, timeout=1, return_when=FIRST_COMPLETED\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    )\n",
      "    ^\n",
      "  File \"/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/concurrent/futures/_base.py\", line 305, in wait\n",
      "    waiter.event.wait(timeout)\n",
      "    ~~~~~~~~~~~~~~~~~^^^^^^^^^\n",
      "  File \"/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/threading.py\", line 659, in wait\n",
      "    signaled = self._cond.wait(timeout)\n",
      "  File \"/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/threading.py\", line 363, in wait\n",
      "    gotit = waiter.acquire(True, timeout)\n",
      "  File \"/Users/ytchen/Documents/experimental/Llama4/env/lib/python3.13/site-packages/dspy/utils/parallelizer.py\", line 106, in handler\n",
      "    orig_handler(sig, frame)\n",
      "    ~~~~~~~~~~~~^^^^^^^^^^^^\n",
      "KeyboardInterrupt\n",
      "\u001b[33m[W 2025-06-25 12:51:35,303]\u001b[0m Trial 3 failed with value None.\u001b[0m\n",
      "\n",
      "Aborted!\n",
      "2025-06-25 12:51:35,334 - httpx - INFO - HTTP Request: POST https://api.together.xyz/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m12:51:35 - LiteLLM:INFO\u001b[0m: utils.py:1215 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-25 12:51:35,335 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m12:51:35 - LiteLLM:INFO\u001b[0m: utils.py:3119 - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.3-70B-Instruct-Turbo; provider = together_ai\n",
      "2025-06-25 12:51:35,353 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.3-70B-Instruct-Turbo; provider = together_ai\n"
     ]
    }
   ],
   "source": [
    "#run_optimization = False     # or chnage to True to run\n",
    "run_optimization = True\n",
    "\n",
    "if run_optimization:\n",
    "    #!cd my-project && llama-prompt-ops migrate --api-key-env TOGETHERAI_API_KEY\n",
    "    !cd my-project && llama-prompt-ops migrate --api-key-env TOGETHER_API_KEY"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "553cb2f6-0966-4ce9-b2b0-60b96503f1a8",
   "metadata": {},
   "source": [
    "## Analyzing the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9f2b2a97-24c1-401e-a362-7cfbbb83697d",
   "metadata": {
    "height": 132
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "json_files = glob.glob(\"my-project/results/*.json\")\n",
    "\n",
    "import json\n",
    "with open(json_files[0], \"r\") as f:\n",
    "    data = json.load(f)\n",
    "optimized_prompt = data['prompt']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "efe1bf0d-4b34-4b18-adc6-0bf5ab01967b",
   "metadata": {
    "height": 47
   },
   "outputs": [],
   "source": [
    "with open(\"my-project/prompts/prompt.txt\", \"r\", encoding=\"utf-8\") as file:\n",
    "    original_prompt = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "56937fce-0f3c-4812-a7d8-9f8fd4aa9244",
   "metadata": {
    "height": 166
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table style=\"width: 100%; border-collapse: collapse;\"><tr><th>Original Prompt</th><th>Optimized Prompt</th></tr><tr><td style=\"width:50% padding: 10px; vertical-align: top;\"><pre style=\"white-space: pre-wrap; word-wrap: break-word;\">You are a helpful assistant. Extract and return a json with the following keys and values:\n",
       "- \"urgency\" as one of `high`, `medium`, `low`\n",
       "- \"sentiment\" as one of `negative`, `neutral`, `positive`\n",
       "- \"categories\" Create a dictionary with categories as keys and boolean values (True/False), where the value indicates whether the category is one of the best matching support category tags from: `emergency_repair_services`, `routine_maintenance_requests`, `quality_and_safety_concerns`, `specialized_cleaning_services`, `general_inquiries`, `sustainability_and_environmental_practices`, `training_and_support_requests`, `cleaning_services_scheduling`, `customer_feedback_and_complaints`, `facility_management_issues`\n",
       "Your complete message should be a valid json string that can be read directly and only contain the keys mentioned in the list above. Never enclose it in ```json...```, no newlines, no unnessacary whitespaces.\n",
       "</pre></td><td style=\"width: 50% padding: 10px; vertical-align: top;\"><pre style=\"white-space: pre-wrap; word-wrap: break-word;\">You are a customer support specialist for ProCare Facility Solutions, tasked with analyzing incoming customer inquiries and requests. Extract and return a json with the following keys and values: - \"urgency\" as one of `high`, `medium`, `low` - \"sentiment\" as one of `negative`, `neutral`, `positive` - \"categories\" Create a dictionary with categories as keys and boolean values (True/False), where the value indicates whether the category is one of the best matching support category tags from: `emergency_repair_services`, `routine_maintenance_requests`, `quality_and_safety_concerns`, `specialized_cleaning_services`, `general_inquiries`, `sustainability_and_environmental_practices`, `training_and_support_requests`, `cleaning_services_scheduling`, `customer_feedback_and_complaints`, `facility_management_issues`. Your complete message should be a valid json string that can be read directly and only contain the keys mentioned in the list above.</pre></td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, HTML\n",
    "\n",
    "def compare_strings_side_by_side(text1, text2):\n",
    "    html = '<table style=\"width: 100%; border-collapse: collapse;\"><tr><th>Original Prompt</th><th>Optimized Prompt</th></tr>'\n",
    "    html += f'<tr><td style=\"width:50% padding: 10px; vertical-align: top;\"><pre style=\"white-space: pre-wrap; word-wrap: break-word;\">{text1}</pre></td><td style=\"width: 50% padding: 10px; vertical-align: top;\"><pre style=\"white-space: pre-wrap; word-wrap: break-word;\">{text2}</pre></td></tr></table>'\n",
    "\n",
    "    display(HTML(html))\n",
    "\n",
    "compare_strings_side_by_side(original_prompt, optimized_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be468a56-effe-4ed4-960e-1020315c914f",
   "metadata": {},
   "source": [
    "## Few-shot examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c5297b14-aa9c-4ecc-b94d-1f934743afc3",
   "metadata": {
    "height": 30
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Subject: Request for Training and Support on Facility Management Best Practices\\n\\nDear ProCare Support Team,\\n\\nI hope this message finds you well. My name is Dr. Alex Turner, and I am a wildlife ecologist who has been utilizing your facility management services for our research center. We have been quite satisfied with the overall maintenance and cleaning services provided by ProCare Facility Solutions.\\n\\nI am reaching out to request some additional training and support for our in-house maintenance team. As our research activities expand, we find ourselves needing to better understand the best practices in facility management, particularly in areas related to energy efficiency and environmental impact reduction. This knowledge is crucial for us to maintain our facility in a way that aligns with our ecological research goals.\\n\\nSo far, we have tried to implement some basic practices based on general guidelines, but we believe that a more structured training program from your experts would be highly beneficial. We are looking for comprehensive training sessions that can be scheduled at a convenient time for our team.\\n\\nCould you please provide us with information on the available training programs and how we can arrange for these sessions? Additionally, any resources or documentation that could help us in the interim would be greatly appreciated.\\n\\nThank you for your attention to this matter. We look forward to your guidance and support.\\n\\nBest regards,\\n\\nDr. Alex Turner  \\nWildlife Ecologist  \\n[Sender]'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['few_shots'][0]['question']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3c9476d0-35f5-488d-897f-0e3bbcde2524",
   "metadata": {
    "height": 30
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\"categories\": {\"routine_maintenance_requests\": false, \"customer_feedback_and_complaints\": false, \"training_and_support_requests\": true, \"quality_and_safety_concerns\": false, \"sustainability_and_environmental_practices\": true, \"cleaning_services_scheduling\": false, \"specialized_cleaning_services\": false, \"emergency_repair_services\": false, \"facility_management_issues\": true, \"general_inquiries\": false}, \"sentiment\": \"neutral\", \"urgency\": \"low\"}'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['few_shots'][0]['answer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3359dc9e-9bf2-4272-88ae-41a8a7a46702",
   "metadata": {
    "height": 30
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data['few_shots'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ab19cc5b-2fdc-4b05-a6b2-71f6af2ca0cf",
   "metadata": {
    "height": 81
   },
   "outputs": [],
   "source": [
    "few_shots = \"\\n\\nFew shot examples\\n\\n\"\n",
    "for i, shot in enumerate(data['few_shots']):\n",
    "  few_shots += f\"\"\"Example {i+1}\\n=================\\nQuestion:\\n\n",
    "  {shot['question']}\\n\\nAnswer:\\n{shot['answer']}\\n\\n\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "555c36ff-3060-4857-8f6e-e98e9b007619",
   "metadata": {},
   "source": [
    "## Compare optimized and original prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6f584214-3e31-4b9e-b666-d7a8d6908515",
   "metadata": {
    "height": 81
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(\"my-project/data/dataset.json\", 'r') as f:\n",
    "  ds = json.load(f)\n",
    "\n",
    "len(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3bbdaa7b-40ae-43ea-94fb-7f8ea99f3f59",
   "metadata": {
    "height": 47
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_test = ds[int(len(ds)*0.7):]\n",
    "len(ds_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "bb396e81-9458-4330-823e-7598e5f7c6f5",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "from utils import evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "30979087-60fd-4871-a9dc-11774227a7a3",
   "metadata": {
    "height": 353
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 60/60 [00:54<00:00,  1.09it/s]\n"
     ]
    }
   ],
   "source": [
    "from together import Together\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "result_original = []\n",
    "client = Together()\n",
    "\n",
    "for entry in tqdm(ds_test):\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": original_prompt},\n",
    "        {\"role\": \"user\", \"content\": entry[\"fields\"][\"input\"]},\n",
    "    ]\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "      model=\"meta-llama/Llama-3.3-70B-Instruct-Turbo\",\n",
    "      messages=messages,\n",
    "      temperature=0\n",
    "    )\n",
    "\n",
    "    prediction = response.choices[0].message.content\n",
    "    result_original.append(evaluate(entry[\"answer\"], prediction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "bb88844f-1a18-4be1-9da5-2a9fdb2b9625",
   "metadata": {
    "height": 285
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 60/60 [01:04<00:00,  1.08s/it]\n"
     ]
    }
   ],
   "source": [
    "result_optimized = []\n",
    "\n",
    "for entry in tqdm(ds_test):\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": optimized_prompt + few_shots},\n",
    "        {\"role\": \"user\", \"content\": entry[\"fields\"][\"input\"]},\n",
    "    ]\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "      model=\"meta-llama/Llama-3.3-70B-Instruct-Turbo\",\n",
    "      messages=messages,\n",
    "      temperature=0\n",
    "    )\n",
    "\n",
    "    prediction = response.choices[0].message.content\n",
    "    result_optimized.append(evaluate(entry[\"answer\"], prediction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "9dd5b9d6-3d17-494e-89ee-d25c144622fa",
   "metadata": {
    "height": 30
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'is_valid_json': True,\n",
       " 'correct_categories': 0.9,\n",
       " 'correct_sentiment': True,\n",
       " 'correct_urgency': True,\n",
       " 'total': 0.9666666666666667}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_optimized[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "8fa27268-9e3c-477c-b7fc-9284c7df11ef",
   "metadata": {
    "height": 30
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'is_valid_json': True,\n",
       " 'correct_categories': 0.9,\n",
       " 'correct_sentiment': False,\n",
       " 'correct_urgency': True,\n",
       " 'total': 0.6333333333333333}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_optimized[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "10141053-db33-435e-812c-0e5549c638e8",
   "metadata": {
    "height": 64
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'is_valid_json': 1.0,\n",
       " 'correct_categories': 0.9299999999999999,\n",
       " 'correct_sentiment': 0.5,\n",
       " 'correct_urgency': 0.8666666666666667,\n",
       " 'total': 0.7655555555555555}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "float_keys = [k for k, v in result_original[0].items() if isinstance(v,\n",
    "                                                (int, float, bool))]\n",
    "{k: sum([e[k] for e in result_original])/len(result_original) for k in float_keys}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ed502c37-21f9-4421-9705-cdea6c9759a8",
   "metadata": {
    "height": 30
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'is_valid_json': 1.0,\n",
       " 'correct_categories': 0.9483333333333334,\n",
       " 'correct_sentiment': 0.7,\n",
       " 'correct_urgency': 0.95,\n",
       " 'total': 0.8661111111111112}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{k: sum([e[k] for e in result_optimized])/len(result_optimized) for k in float_keys}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfe99719-9560-4ace-9717-56ee31eebdc2",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bae96c70-965d-4b8b-a63c-d8107f53284a",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62531e29-5e87-43e8-9173-2329dd8a046d",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f22af04-f412-4b7f-a248-11e9204c9d91",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa59b1b2-c569-452d-98c1-ccb4c2f4827d",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "057437d5-7e48-4cf6-a931-424f679e262e",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
