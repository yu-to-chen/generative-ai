{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ad96800d-eacf-474c-8e94-4b239266e963",
   "metadata": {},
   "source": [
    "# Prompt Format"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c986db25-00c0-4496-ba3d-1827c5ae3bc5",
   "metadata": {},
   "source": [
    "<p style=\"background-color:#fff6e4; padding:15px; border-width:3px; border-color:#f5ecda; border-style:solid; border-radius:6px\"> ⏳ <b>Note <code>(Kernel Starting)</code>:</b> This notebook takes about 30 seconds to be ready to use. You may start and watch the video while you wait.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f128538c-f354-4151-87ec-8afedbb813b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting gradio\n",
      "  Downloading gradio-5.0.2-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting aiofiles<24.0,>=22.0 (from gradio)\n",
      "  Using cached aiofiles-23.2.1-py3-none-any.whl.metadata (9.7 kB)\n",
      "Requirement already satisfied: anyio<5.0,>=3.0 in /opt/homebrew/Cellar/jupyterlab/4.2.5_1/libexec/lib/python3.12/site-packages (from gradio) (4.4.0)\n",
      "Collecting fastapi<1.0 (from gradio)\n",
      "  Downloading fastapi-0.115.2-py3-none-any.whl.metadata (27 kB)\n",
      "Collecting ffmpy (from gradio)\n",
      "  Using cached ffmpy-0.4.0-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting gradio-client==1.4.0 (from gradio)\n",
      "  Downloading gradio_client-1.4.0-py3-none-any.whl.metadata (7.1 kB)\n",
      "Requirement already satisfied: httpx>=0.24.1 in /opt/homebrew/Cellar/jupyterlab/4.2.5_1/libexec/lib/python3.12/site-packages (from gradio) (0.27.2)\n",
      "Collecting huggingface-hub>=0.25.1 (from gradio)\n",
      "  Using cached huggingface_hub-0.25.2-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: jinja2<4.0 in /opt/homebrew/Cellar/jupyterlab/4.2.5_1/libexec/lib/python3.12/site-packages (from gradio) (3.1.4)\n",
      "Requirement already satisfied: markupsafe~=2.0 in /opt/homebrew/Cellar/jupyterlab/4.2.5_1/libexec/lib/python3.12/site-packages (from gradio) (2.1.5)\n",
      "Requirement already satisfied: numpy<3.0,>=1.0 in /opt/homebrew/Cellar/jupyterlab/4.2.5_1/libexec/lib/python3.12/site-packages (from gradio) (2.1.2)\n",
      "Collecting orjson~=3.0 (from gradio)\n",
      "  Downloading orjson-3.10.7-cp312-cp312-macosx_10_15_x86_64.macosx_11_0_arm64.macosx_10_15_universal2.whl.metadata (50 kB)\n",
      "Requirement already satisfied: packaging in /opt/homebrew/Cellar/jupyterlab/4.2.5_1/libexec/lib/python3.12/site-packages (from gradio) (24.1)\n",
      "Collecting pandas<3.0,>=1.0 (from gradio)\n",
      "  Downloading pandas-2.2.3-cp312-cp312-macosx_11_0_arm64.whl.metadata (89 kB)\n",
      "Requirement already satisfied: pillow<11.0,>=8.0 in /opt/homebrew/Cellar/jupyterlab/4.2.5_1/libexec/lib/python3.12/site-packages (from gradio) (10.4.0)\n",
      "Collecting pydantic>=2.0 (from gradio)\n",
      "  Using cached pydantic-2.9.2-py3-none-any.whl.metadata (149 kB)\n",
      "Collecting pydub (from gradio)\n",
      "  Using cached pydub-0.25.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Collecting python-multipart>=0.0.9 (from gradio)\n",
      "  Using cached python_multipart-0.0.12-py3-none-any.whl.metadata (1.9 kB)\n",
      "Requirement already satisfied: pyyaml<7.0,>=5.0 in /opt/homebrew/Cellar/jupyterlab/4.2.5_1/libexec/lib/python3.12/site-packages (from gradio) (6.0.2)\n",
      "Collecting ruff>=0.2.2 (from gradio)\n",
      "  Using cached ruff-0.6.9-py3-none-macosx_11_0_arm64.whl.metadata (25 kB)\n",
      "Collecting semantic-version~=2.0 (from gradio)\n",
      "  Using cached semantic_version-2.10.0-py2.py3-none-any.whl.metadata (9.7 kB)\n",
      "Collecting tomlkit==0.12.0 (from gradio)\n",
      "  Using cached tomlkit-0.12.0-py3-none-any.whl.metadata (2.7 kB)\n",
      "Collecting typer<1.0,>=0.12 (from gradio)\n",
      "  Using cached typer-0.12.5-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting typing-extensions~=4.0 (from gradio)\n",
      "  Using cached typing_extensions-4.12.2-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting uvicorn>=0.14.0 (from gradio)\n",
      "  Using cached uvicorn-0.31.1-py3-none-any.whl.metadata (6.6 kB)\n",
      "Collecting fsspec (from gradio-client==1.4.0->gradio)\n",
      "  Using cached fsspec-2024.9.0-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting websockets<13.0,>=10.0 (from gradio-client==1.4.0->gradio)\n",
      "  Using cached websockets-12.0-cp312-cp312-macosx_11_0_arm64.whl.metadata (6.6 kB)\n",
      "Requirement already satisfied: idna>=2.8 in /opt/homebrew/Cellar/jupyterlab/4.2.5_1/libexec/lib/python3.12/site-packages (from anyio<5.0,>=3.0->gradio) (3.8)\n",
      "Requirement already satisfied: sniffio>=1.1 in /opt/homebrew/Cellar/jupyterlab/4.2.5_1/libexec/lib/python3.12/site-packages (from anyio<5.0,>=3.0->gradio) (1.3.1)\n",
      "Collecting starlette<0.41.0,>=0.37.2 (from fastapi<1.0->gradio)\n",
      "  Downloading starlette-0.39.2-py3-none-any.whl.metadata (6.0 kB)\n",
      "Requirement already satisfied: certifi in /opt/homebrew/opt/certifi/lib/python3.12/site-packages (from httpx>=0.24.1->gradio) (2024.8.30)\n",
      "Requirement already satisfied: httpcore==1.* in /opt/homebrew/Cellar/jupyterlab/4.2.5_1/libexec/lib/python3.12/site-packages (from httpx>=0.24.1->gradio) (1.0.5)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /opt/homebrew/Cellar/jupyterlab/4.2.5_1/libexec/lib/python3.12/site-packages (from httpcore==1.*->httpx>=0.24.1->gradio) (0.14.0)\n",
      "Collecting filelock (from huggingface-hub>=0.25.1->gradio)\n",
      "  Using cached filelock-3.16.1-py3-none-any.whl.metadata (2.9 kB)\n",
      "Requirement already satisfied: requests in /opt/homebrew/Cellar/jupyterlab/4.2.5_1/libexec/lib/python3.12/site-packages (from huggingface-hub>=0.25.1->gradio) (2.32.3)\n",
      "Collecting tqdm>=4.42.1 (from huggingface-hub>=0.25.1->gradio)\n",
      "  Using cached tqdm-4.66.5-py3-none-any.whl.metadata (57 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/homebrew/Cellar/jupyterlab/4.2.5_1/libexec/lib/python3.12/site-packages (from pandas<3.0,>=1.0->gradio) (2.9.0.post0)\n",
      "Collecting pytz>=2020.1 (from pandas<3.0,>=1.0->gradio)\n",
      "  Using cached pytz-2024.2-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting tzdata>=2022.7 (from pandas<3.0,>=1.0->gradio)\n",
      "  Using cached tzdata-2024.2-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Collecting annotated-types>=0.6.0 (from pydantic>=2.0->gradio)\n",
      "  Using cached annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting pydantic-core==2.23.4 (from pydantic>=2.0->gradio)\n",
      "  Downloading pydantic_core-2.23.4-cp312-cp312-macosx_11_0_arm64.whl.metadata (6.6 kB)\n",
      "Collecting click>=8.0.0 (from typer<1.0,>=0.12->gradio)\n",
      "  Using cached click-8.1.7-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting shellingham>=1.3.0 (from typer<1.0,>=0.12->gradio)\n",
      "  Using cached shellingham-1.5.4-py2.py3-none-any.whl.metadata (3.5 kB)\n",
      "Collecting rich>=10.11.0 (from typer<1.0,>=0.12->gradio)\n",
      "  Using cached rich-13.9.2-py3-none-any.whl.metadata (18 kB)\n",
      "Requirement already satisfied: six>=1.5 in /opt/homebrew/Cellar/jupyterlab/4.2.5_1/libexec/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas<3.0,>=1.0->gradio) (1.16.0)\n",
      "Collecting markdown-it-py>=2.2.0 (from rich>=10.11.0->typer<1.0,>=0.12->gradio)\n",
      "  Using cached markdown_it_py-3.0.0-py3-none-any.whl.metadata (6.9 kB)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/homebrew/Cellar/jupyterlab/4.2.5_1/libexec/lib/python3.12/site-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.18.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/homebrew/Cellar/jupyterlab/4.2.5_1/libexec/lib/python3.12/site-packages (from requests->huggingface-hub>=0.25.1->gradio) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/homebrew/Cellar/jupyterlab/4.2.5_1/libexec/lib/python3.12/site-packages (from requests->huggingface-hub>=0.25.1->gradio) (2.2.2)\n",
      "Collecting mdurl~=0.1 (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio)\n",
      "  Using cached mdurl-0.1.2-py3-none-any.whl.metadata (1.6 kB)\n",
      "Downloading gradio-5.0.2-py3-none-any.whl (42.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.3/42.3 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading gradio_client-1.4.0-py3-none-any.whl (319 kB)\n",
      "Using cached tomlkit-0.12.0-py3-none-any.whl (37 kB)\n",
      "Using cached aiofiles-23.2.1-py3-none-any.whl (15 kB)\n",
      "Downloading fastapi-0.115.2-py3-none-any.whl (94 kB)\n",
      "Using cached huggingface_hub-0.25.2-py3-none-any.whl (436 kB)\n",
      "Downloading orjson-3.10.7-cp312-cp312-macosx_10_15_x86_64.macosx_11_0_arm64.macosx_10_15_universal2.whl (251 kB)\n",
      "Downloading pandas-2.2.3-cp312-cp312-macosx_11_0_arm64.whl (11.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.4/11.4 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hUsing cached pydantic-2.9.2-py3-none-any.whl (434 kB)\n",
      "Downloading pydantic_core-2.23.4-cp312-cp312-macosx_11_0_arm64.whl (1.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hUsing cached python_multipart-0.0.12-py3-none-any.whl (23 kB)\n",
      "Using cached ruff-0.6.9-py3-none-macosx_11_0_arm64.whl (9.7 MB)\n",
      "Using cached semantic_version-2.10.0-py2.py3-none-any.whl (15 kB)\n",
      "Using cached typer-0.12.5-py3-none-any.whl (47 kB)\n",
      "Using cached typing_extensions-4.12.2-py3-none-any.whl (37 kB)\n",
      "Using cached uvicorn-0.31.1-py3-none-any.whl (63 kB)\n",
      "Using cached ffmpy-0.4.0-py3-none-any.whl (5.8 kB)\n",
      "Using cached pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n",
      "Using cached annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
      "Using cached click-8.1.7-py3-none-any.whl (97 kB)\n",
      "Using cached fsspec-2024.9.0-py3-none-any.whl (179 kB)\n",
      "Using cached pytz-2024.2-py2.py3-none-any.whl (508 kB)\n",
      "Using cached rich-13.9.2-py3-none-any.whl (242 kB)\n",
      "Using cached shellingham-1.5.4-py2.py3-none-any.whl (9.8 kB)\n",
      "Downloading starlette-0.39.2-py3-none-any.whl (73 kB)\n",
      "Using cached tqdm-4.66.5-py3-none-any.whl (78 kB)\n",
      "Using cached tzdata-2024.2-py2.py3-none-any.whl (346 kB)\n",
      "Using cached websockets-12.0-cp312-cp312-macosx_11_0_arm64.whl (121 kB)\n",
      "Using cached filelock-3.16.1-py3-none-any.whl (16 kB)\n",
      "Using cached markdown_it_py-3.0.0-py3-none-any.whl (87 kB)\n",
      "Using cached mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
      "Installing collected packages: pytz, pydub, websockets, tzdata, typing-extensions, tqdm, tomlkit, shellingham, semantic-version, ruff, python-multipart, orjson, mdurl, fsspec, filelock, ffmpy, click, annotated-types, aiofiles, uvicorn, starlette, pydantic-core, pandas, markdown-it-py, huggingface-hub, rich, pydantic, gradio-client, typer, fastapi, gradio\n",
      "Successfully installed aiofiles-23.2.1 annotated-types-0.7.0 click-8.1.7 fastapi-0.115.2 ffmpy-0.4.0 filelock-3.16.1 fsspec-2024.9.0 gradio-5.0.2 gradio-client-1.4.0 huggingface-hub-0.25.2 markdown-it-py-3.0.0 mdurl-0.1.2 orjson-3.10.7 pandas-2.2.3 pydantic-2.9.2 pydantic-core-2.23.4 pydub-0.25.1 python-multipart-0.0.12 pytz-2024.2 rich-13.9.2 ruff-0.6.9 semantic-version-2.10.0 shellingham-1.5.4 starlette-0.39.2 tomlkit-0.12.0 tqdm-4.66.5 typer-0.12.5 typing-extensions-4.12.2 tzdata-2024.2 uvicorn-0.31.1 websockets-12.0\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install gradio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "12069b71-c878-4199-8d7b-0ba469294742",
   "metadata": {
    "height": 47
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1b7c6f4-67c2-4b9c-838f-4ae4c8c48b70",
   "metadata": {},
   "source": [
    "If you would like to view the utils.py file, uncomment the following line and run it. Alternately to view it in an editor, click `file->open` and look for utils.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "378a2aac-8aba-42c2-b98a-6260e9c866df",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "#!cat utils.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "878c8b00-1f82-4ae8-973d-6775e8d1c14f",
   "metadata": {},
   "source": [
    "<p style=\"background-color:#fff6ff; padding:15px; border-width:3px; border-color:#efe6ef; border-style:solid; border-radius:6px\"> 💻 &nbsp; <b>Access <code>requirements.txt</code> and <code>utils.py</code> files:</b> 1) click on the <em>\"File\"</em> option on the top menu of the notebook and then 2) click on <em>\"Open\"</em>. For more help, please see the <em>\"Appendix - Tips and Help\"</em> Lesson.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26f3131d-c1e0-4ce9-909b-222063cab03d",
   "metadata": {
    "id": "DC3VBQGs7ggW"
   },
   "source": [
    "## Using the \"user\" and \"assistant\" roles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8a39424b-ddc4-4a99-969d-270a06adf472",
   "metadata": {
    "height": 200
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|><|start_header_id|>user<|end_header_id|>Who wrote the book Charlotte's Web?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n"
     ]
    }
   ],
   "source": [
    "question = \"Who wrote the book Charlotte's Web?\"\n",
    "\n",
    "prompt = (\n",
    "    \"<|begin_of_text|>\"                              # start of prompt\n",
    "    \"<|start_header_id|>user<|end_header_id|>\"       # user header\n",
    "    f\"{question}\"                                    # user input\n",
    "    \"<|eot_id|>\"                                     #end of turn\n",
    "    \"<|start_header_id|>assistant<|end_header_id|>\"  #assistant header\n",
    ")\n",
    "\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ccfba2fe-8fa3-41b4-b91c-2f66dc6bfe5f",
   "metadata": {
    "height": 64
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "The book \"Charlotte's Web\" was written by E.B. White.\n"
     ]
    }
   ],
   "source": [
    "from  utils import llama31\n",
    "response = llama31(prompt,8)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6a7ffce6-514a-4e4a-acdf-ca65e49d2627",
   "metadata": {
    "height": 81
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m    \u001b[39;49;00m\u001b[94m\"id\"\u001b[39;49;00m:\u001b[37m \u001b[39;49;00m\u001b[33m\"8d259bb47eab4a12-TPE\"\u001b[39;49;00m,\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m    \u001b[39;49;00m\u001b[94m\"object\"\u001b[39;49;00m:\u001b[37m \u001b[39;49;00m\u001b[33m\"text.completion\"\u001b[39;49;00m,\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m    \u001b[39;49;00m\u001b[94m\"created\"\u001b[39;49;00m:\u001b[37m \u001b[39;49;00m\u001b[34m1728888343\u001b[39;49;00m,\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m    \u001b[39;49;00m\u001b[94m\"model\"\u001b[39;49;00m:\u001b[37m \u001b[39;49;00m\u001b[33m\"meta-llama/Meta-Llama-3.1-8B-Instruct-Turbo\"\u001b[39;49;00m,\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m    \u001b[39;49;00m\u001b[94m\"prompt\"\u001b[39;49;00m:\u001b[37m \u001b[39;49;00m[],\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m    \u001b[39;49;00m\u001b[94m\"choices\"\u001b[39;49;00m:\u001b[37m \u001b[39;49;00m[\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m        \u001b[39;49;00m{\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m            \u001b[39;49;00m\u001b[94m\"text\"\u001b[39;49;00m:\u001b[37m \u001b[39;49;00m\u001b[33m\"\\n\\nThe book \\\"Charlotte's Web\\\" was written by E.B. White.\"\u001b[39;49;00m,\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m            \u001b[39;49;00m\u001b[94m\"finish_reason\"\u001b[39;49;00m:\u001b[37m \u001b[39;49;00m\u001b[33m\"eos\"\u001b[39;49;00m,\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m            \u001b[39;49;00m\u001b[94m\"seed\"\u001b[39;49;00m:\u001b[37m \u001b[39;49;00m\u001b[34m7349019907653514000\u001b[39;49;00m,\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m            \u001b[39;49;00m\u001b[94m\"logprobs\"\u001b[39;49;00m:\u001b[37m \u001b[39;49;00m\u001b[34mnull\u001b[39;49;00m,\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m            \u001b[39;49;00m\u001b[94m\"index\"\u001b[39;49;00m:\u001b[37m \u001b[39;49;00m\u001b[34m0\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m        \u001b[39;49;00m}\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m    \u001b[39;49;00m],\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m    \u001b[39;49;00m\u001b[94m\"usage\"\u001b[39;49;00m:\u001b[37m \u001b[39;49;00m{\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m        \u001b[39;49;00m\u001b[94m\"prompt_tokens\"\u001b[39;49;00m:\u001b[37m \u001b[39;49;00m\u001b[34m17\u001b[39;49;00m,\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m        \u001b[39;49;00m\u001b[94m\"completion_tokens\"\u001b[39;49;00m:\u001b[37m \u001b[39;49;00m\u001b[34m17\u001b[39;49;00m,\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m        \u001b[39;49;00m\u001b[94m\"total_tokens\"\u001b[39;49;00m:\u001b[37m \u001b[39;49;00m\u001b[34m34\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m    \u001b[39;49;00m}\u001b[37m\u001b[39;49;00m\n",
      "}\u001b[37m\u001b[39;49;00m\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from utils import cprint\n",
    "response = llama31(prompt,8, raw=True)\n",
    "\n",
    "cprint(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8160c83-47d3-42b5-92fb-86f5fd97af51",
   "metadata": {},
   "source": [
    "# Multi-turn chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "24c191c7-e158-4865-a80d-fade3d145d58",
   "metadata": {
    "height": 251
   },
   "outputs": [],
   "source": [
    "follow_up_question = \"Three best quotes in it\"\n",
    "follow_up_prompt = (\n",
    "    \"<|begin_of_text|>\"                              # start of prompt\n",
    "    \"<|start_header_id|>user<|end_header_id|>\"        # past  \n",
    "    f\"{question}\"                                     # past\n",
    "    \"<|eot_id|>\"                                      # past\n",
    "    \"<|start_header_id|>assistant<|end_header_id|>\"   # past\n",
    "    f\"{response}\"                                     # past\n",
    "    \"<|eot_id|>\"                                      # past\n",
    "    \"<|start_header_id|>user<|end_header_id|>\"       # new\n",
    "    f\"{follow_up_question}\"                          # new\n",
    "    \"<|eot_id|>\"                                     # new\n",
    "    \"<|start_header_id|>assistant<|end_header_id|>\"  # new\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e798493a-e9c8-4d96-8baf-f18c8bc8d2c6",
   "metadata": {
    "height": 47
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here are three of the most famous quotes from \"Charlotte's Web\" by E.B. White:\n",
      "\n",
      "1. **\"Alone, alone, alone. That's the way I like it best.\"** - Charlotte (the spider)\n",
      "\n",
      "This quote highlights Charlotte's independent nature and her preference for solitude, which is a recurring theme throughout the book.\n",
      "\n",
      "2. **\"Some people only use half their brains.\"** - Templeton the Rat\n",
      "\n",
      "This quote is a humorous observation made by Templeton, the wise-cracking rat who becomes a friend to Wilbur and Charlotte. It's a clever commentary on the idea that people often don't use their full potential.\n",
      "\n",
      "3. **\"It's not often that someone comes along who is a true friend and a good writer. Charlotte was both.\"** - Wilbur the Pig\n",
      "\n",
      "This quote is a heartfelt tribute to Charlotte's friendship and writing skills, which ultimately save Wilbur's life. It's a beautiful expression of the special bond between Wilbur and Charlotte.\n"
     ]
    }
   ],
   "source": [
    "follow_up_response = llama31(follow_up_prompt)\n",
    "print(follow_up_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bb293be0-612e-48df-b74a-d4d2d98ca8b2",
   "metadata": {
    "height": 47
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Here are three of the most famous quotes from \"Charlotte's Web\" by E.B. White:\n",
       "\n",
       "1. **\"Alone, alone, alone. That's the way I like it best.\"** - Charlotte (the spider)\n",
       "\n",
       "This quote highlights Charlotte's independent nature and her preference for solitude, which is a recurring theme throughout the book.\n",
       "\n",
       "2. **\"Some people only use half their brains.\"** - Templeton the Rat\n",
       "\n",
       "This quote is a humorous observation made by Templeton, the wise-cracking rat who becomes a friend to Wilbur and Charlotte. It's a clever commentary on the idea that people often don't use their full potential.\n",
       "\n",
       "3. **\"It's not often that someone comes along who is a true friend and a good writer. Charlotte was both.\"** - Wilbur the Pig\n",
       "\n",
       "This quote is a heartfelt tribute to Charlotte's friendship and writing skills, which ultimately save Wilbur's life. It's a beautiful expression of the special bond between Wilbur and Charlotte."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Markdown, display\n",
    "display(Markdown(follow_up_response))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6036cfb-876b-4487-8325-33f03b4ff085",
   "metadata": {},
   "source": [
    "#### Without message history\n",
    "This is not in the video, but you can try the same prompt *Without* including history and see the models responds with three quotes - but not from the book."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "899eba88-632f-4ff9-bae6-8a5831284bfa",
   "metadata": {
    "height": 183
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "\n",
       "Here are three best quotes:\n",
       "\n",
       "1. **\"Believe you can and you're halfway there.\"** - Theodore Roosevelt\n",
       "\n",
       "This quote emphasizes the importance of having a positive mindset and believing in oneself. It suggests that having confidence and faith in one's abilities can lead to success.\n",
       "\n",
       "2. **\"The only way to do great work is to love what you do.\"** - Steve Jobs\n",
       "\n",
       "This quote highlights the significance of passion and enjoyment in one's work. It suggests that when we love what we do, we are more likely to be motivated, inspired, and produce high-quality work.\n",
       "\n",
       "3. **\"Success is not final, failure is not fatal: It is the courage to continue that counts.\"** - Winston Churchill\n",
       "\n",
       "This quote emphasizes the importance of perseverance and resilience in the face of challenges and setbacks. It suggests that success is not a guarantee, but it's the courage to keep trying and learning from our mistakes that ultimately leads to growth and achievement."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "question = \"Three Best quotes\"\n",
    "prompt = (\n",
    "    \"<|begin_of_text|>\"  \n",
    "    \"<|start_header_id|>user<|end_header_id|>\"\n",
    "    f\"{question}\"\n",
    "    \"<|eot_id|>\"\n",
    "    \"<|start_header_id|>assistant<|end_header_id|>\"\n",
    ")\n",
    "response = llama31(prompt,8)\n",
    "display(Markdown(response))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edd0f383-4115-44d5-9a58-62463ec5cbfb",
   "metadata": {},
   "source": [
    "## Using \"system\" role"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fed0d3ce-a42b-446b-8a88-a4cefb185acc",
   "metadata": {
    "height": 336
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "\n",
       "* \"Champions are made from something they have deep inside them - a desire, a dream, a vision.\" - Muhammad Ali\n",
       "* \"It's not whether you get knocked down, it's whether you get up.\" - Vince Lombardi\n",
       "* \"Hard work, dedication, and a willingness to learn are the keys to success in any field, including sports.\" - Michael Jordan"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "question = \"Three Great quotes\"\n",
    "system_message = (\n",
    "    \"You are an expert in quotes about sports \" \n",
    "    \"You provide just the quotes and no commentary. \"\n",
    "    \"Reply in markdown\"\n",
    ")\n",
    "\n",
    "prompt = (\n",
    "    \"<|begin_of_text|>\"\n",
    "    \"<|start_header_id|>system<|end_header_id|>\"\n",
    "    f\"{system_message}\"\n",
    "    \"<|eot_id|>\"    \n",
    "    \"<|start_header_id|>user<|end_header_id|>\"\n",
    "    f\"{question}\"\n",
    "    \"<|eot_id|>\"\n",
    "    \"<|start_header_id|>assistant<|end_header_id|>\"\n",
    ")\n",
    "response = llama31(prompt,70)\n",
    "display(Markdown(response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1f5c210f-080b-4fe1-8a7b-7ff38d0ae9c8",
   "metadata": {
    "height": 336
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "\n",
       "* \"The only way to get started is to quit talking and begin doing.\" - Walt Disney\n",
       "* \"It's not about being the best, it's about being better than you were yesterday.\" - Bo Jackson\n",
       "* \"Don't watch the clock, do what it does. Keep going.\" - Sam Levenson"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "follow_up_question = \"three more\"\n",
    "follow_up_prompt = (\n",
    "    \"<|begin_of_text|>\"\n",
    "    \"<|start_header_id|>system<|end_header_id|>\"      # system\n",
    "    f\"{system_message}\"                               # system\n",
    "    \"<|eot_id|>\"                                      # system\n",
    "    \"<|start_header_id|>user<|end_header_id|>\"        # past\n",
    "    f\"{question}\"                                     # past\n",
    "    \"<|eot_id|>\"                                      # past\n",
    "    \"<|start_header_id|>assistant<|end_header_id|>\"   # past\n",
    "    f\"{response}\"                                     # past\n",
    "    \"<|eot_id|>\"                                      # past\n",
    "    \"<|start_header_id|>user<|end_header_id|>\"         # new\n",
    "    f\"{follow_up_question}\"                            # new     \n",
    "    \"<|eot_id|>\"                                       # new\n",
    "    \"<|start_header_id|>assistant<|end_header_id|>\"    # new\n",
    ")\n",
    "response = llama31(follow_up_prompt)\n",
    "display(Markdown(response))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "139ecab0-a233-414e-a62d-dfb7b6bfbeef",
   "metadata": {},
   "source": [
    "# Chat with Llama using higher-level message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "67920bea-5b75-4a42-a3a0-0aae77bc0842",
   "metadata": {
    "height": 166
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "The book \"Charlotte's Web\" was written by E.B. White.\n"
     ]
    }
   ],
   "source": [
    "prompt = (\n",
    "    \"<|begin_of_text|>\"                              # start of prompt\n",
    "    \"<|start_header_id|>user<|end_header_id|>\"       # user header\n",
    "    \"Who wrote the book Charlotte's Web?\"                                    # user input\n",
    "    \"<|eot_id|>\"                                     #end of turn\n",
    "    \"<|start_header_id|>assistant<|end_header_id|>\"  #assistant header\n",
    ")\n",
    "response = llama31(prompt,8)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "58aa6676-3f4b-4d26-b679-ce392ec73845",
   "metadata": {
    "height": 149,
    "id": "nw9GEik8PWrx"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The book \"Charlotte's Web\" was written by E.B. White.\n"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "    {\n",
    "      \"role\": \"user\",\n",
    "      \"content\": \"Who wrote the book Charlotte's Web?\"\n",
    "    }\n",
    "  ]\n",
    "response = llama31(messages,8)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "67d3c1da-383e-423c-a1e8-d2c355febde0",
   "metadata": {
    "height": 404
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here are three notable quotes from \"Charlotte's Web\":\n",
      "\n",
      "1. \"Alone, alone, alone. That's the way it is when you're a pig. You're born, you live, you die. You're a meal. You're a meal, a meal, a meal. That's all.\"\n",
      "- Wilbur the Pig\n",
      "\n",
      "2. \"Some people only use us for our usefulness. But Charlotte saw me as a friend.\"\n",
      "- Wilbur the Pig\n",
      "\n",
      "3. \"It is not often that someone comes along who is a true friend and a good writer. Charlotte was both.\"\n",
      "- Wilbur the Pig\n"
     ]
    }
   ],
   "source": [
    "follow_up_question = \"Three Best quotes\"\n",
    "\n",
    "messages = [\n",
    "    {\n",
    "      \"role\": \"system\",\n",
    "      \"content\": \"You are an terse expert in children's literature\"\n",
    "    },\n",
    "    {\n",
    "      \"role\": \"user\",\n",
    "      \"content\": \"Who wrote the book Charlotte's Web?\"\n",
    "    },\n",
    "    {\n",
    "      \"role\": \"assistant\",\n",
    "      \"content\": response\n",
    "    },\n",
    "    {\n",
    "      \"role\": \"user\",\n",
    "      \"content\": follow_up_question\n",
    "    }\n",
    "  ]\n",
    "\n",
    "response = llama31(messages,8)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "01b98689-fa40-4466-86f7-4ac17e206d81",
   "metadata": {
    "height": 200
   },
   "outputs": [],
   "source": [
    "class Conversation:\n",
    "    def __init__(self, system=\"\"):\n",
    "        self.messages = []\n",
    "        if system:\n",
    "            self.messages.append({\"role\": \"system\", \"content\": system})\n",
    "    def generate(self, user_question, model=8, temp=0):\n",
    "        self.messages.append({\"role\": \"user\", \"content\":user_question})\n",
    "        response = llama31(self.messages, model, temperature=temp)\n",
    "        self.messages.append({\"role\":\"assistant\", \"content\":response})\n",
    "        return response\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7479e19b-8270-4368-9bd9-cdeed5ed5664",
   "metadata": {
    "height": 64
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'E.B. White wrote the book \"Charlotte\\'s Web\".'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "system_message = \"You are a terse expert in Childrens literature.\" \n",
    "conv = Conversation(system_message)\n",
    "conv.generate(\"Who wrote the book Charlotte's Web?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "da3f5c43-262a-4b40-b00f-e6af7269e3eb",
   "metadata": {
    "height": 47
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Here are three quotes from \"Charlotte's Web\":\n",
       "\n",
       "1. \"Wherever you go, go with all your heart.\"\n",
       "2. \"It's not often that someone comes along who is a true friend and a good listener.\"\n",
       "3. \"It's not often that someone comes along who is a true friend and a good listener. That's what Wilbur was, and maybe still is.\""
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "response = conv.generate(\"What are three quotes\")\n",
    "display(Markdown(response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "be4e57bc-f77d-40f2-9b92-36c4dd8f8756",
   "metadata": {
    "height": 30
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'role': 'system',\n",
       "  'content': 'You are a terse expert in Childrens literature.'},\n",
       " {'role': 'user', 'content': \"Who wrote the book Charlotte's Web?\"},\n",
       " {'role': 'assistant',\n",
       "  'content': 'E.B. White wrote the book \"Charlotte\\'s Web\".'},\n",
       " {'role': 'user', 'content': 'What are three quotes'},\n",
       " {'role': 'assistant',\n",
       "  'content': 'Here are three quotes from \"Charlotte\\'s Web\":\\n\\n1. \"Wherever you go, go with all your heart.\"\\n2. \"It\\'s not often that someone comes along who is a true friend and a good listener.\"\\n3. \"It\\'s not often that someone comes along who is a true friend and a good listener. That\\'s what Wilbur was, and maybe still is.\"'}]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv.messages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64d68ae5-31e7-4ebc-9100-88be9470ee45",
   "metadata": {
    "id": "5QntbEMl9uO5"
   },
   "source": [
    "## Multi-lingual Llama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4842cb63-d0b6-487c-b9ed-38f6e86b7ea0",
   "metadata": {
    "height": 132
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Here are 3 basic phrases in 8 languages:\n",
       "\n",
       "**English**\n",
       "1. Hello - Hello\n",
       "2. Thank you - Thank you\n",
       "3. Goodbye - Goodbye\n",
       "\n",
       "**German**\n",
       "1. Hallo - Hello\n",
       "2. Danke - Thank you\n",
       "3. Auf Wiedersehen - Goodbye\n",
       "\n",
       "**French**\n",
       "1. Bonjour - Hello\n",
       "2. Merci - Thank you\n",
       "3. Au revoir - Goodbye\n",
       "\n",
       "**Italian**\n",
       "1. Ciao - Hello/Goodbye\n",
       "2. Grazie - Thank you\n",
       "3. Arrivederci - Goodbye\n",
       "\n",
       "**Portuguese**\n",
       "1. Olá - Hello\n",
       "2. Obrigado/Obrigada - Thank you (male/female)\n",
       "3. Tchau - Goodbye\n",
       "\n",
       "**Hindi**\n",
       "1. नमस्ते (Namaste) - Hello\n",
       "2. धन्यवाद (Dhanyavad) - Thank you\n",
       "3. अलविदा (Alvida) - Goodbye\n",
       "\n",
       "**Spanish**\n",
       "1. Hola - Hello\n",
       "2. Gracias - Thank you\n",
       "3. Adiós - Goodbye\n",
       "\n",
       "**Thai**\n",
       "1. สวัสดี (Sawatdee) - Hello\n",
       "2. ขอขอบคุณ (Khop Khun Krap/Khob Kun Kha) - Thank you (male/female)\n",
       "3. สวัสดีครับ/สวัสดีค่ะ (Sawatdee Krap/Sawatdee Ka) - Goodbye (male/female)"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "system_message = (\n",
    "    \"Answer in 8 languages: English, German, French, Italian,\" \n",
    "    \"Portuguese, Hindi, Spanish, and Thai\"\n",
    ")\n",
    "ml_conv = Conversation(system_message)\n",
    "response = ml_conv.generate(\"3 basic phrases\")\n",
    "display(Markdown(response))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a2bd3d8-c291-4edd-9b60-8a3faffc5100",
   "metadata": {},
   "source": [
    "## Chatbot App"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "dda1354e-b37b-491e-a7f5-c2bb397d14d5",
   "metadata": {
    "height": 523
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here's a Python script for a Gradio chatbot app that you can run within a Jupyter notebook:\n",
      "\n",
      "```python\n",
      "import gradio as gr\n",
      "\n",
      "# Initialize the Conversation class with a system message\n",
      "conversation = Conversation(\"You are an expert in literature. You provide brief replies.\")\n",
      "\n",
      "# Define a function to handle user input\n",
      "def chatbot(user_input, model_size, temperature):\n",
      "    # Generate a response using the Conversation class\n",
      "    response = conversation.generate(user_input, model=model_size, temp=temperature)\n",
      "    return response\n",
      "\n",
      "# Create a Gradio interface for the chatbot\n",
      "demo = gr.Interface(\n",
      "    fn=chatbot,\n",
      "    inputs=[\n",
      "        gr.Textbox(label=\"User Input\"),\n",
      "        gr.Radio(label=\"Model Size\", choices=[8, 70, 405], value=8),\n",
      "        gr.Slider(label=\"Temperature\", minimum=0, maximum=1, step=0.1, value=0)\n",
      "    ],\n",
      "    outputs=gr.Textbox(label=\"Chatbot Response\"),\n",
      "    title=\"Literature Expert Chatbot\",\n",
      "    description=\"Ask a question about literature and get a brief response.\"\n",
      ")\n",
      "\n",
      "# Launch the Gradio app\n",
      "demo.launch()\n",
      "```\n",
      "\n",
      "To run this script within a Jupyter notebook, simply copy and paste the code into a new cell and execute it. The Gradio app will launch in a new window, allowing you to interact with the chatbot.\n",
      "\n",
      "Note: Make sure you have the `gradio` library installed in your Jupyter notebook environment. You can install it using `!pip install gradio` or `!conda install -c conda-forge gradio`.\n"
     ]
    }
   ],
   "source": [
    "code_query = \"\"\"\n",
    "I need a Python script for a Gradio chatbot app that should be run\n",
    "within a Jupyter notebook.\n",
    "1) When calling the LLM, use this class, which is already defined,\n",
    "    dont redefine it:\n",
    "class Conversation:\n",
    "    def __init__(self, system=\"\"):\n",
    "        self.messages = []\n",
    "        if system:\n",
    "            self.messages.append({\"role\": \"system\", \"content\": system})\n",
    "\n",
    "    def generate(self, user_question, model=8, temp=0):\n",
    "        self.messages.append({\"role\": \"user\", \"content\": user_question})\n",
    "        response = llama31(self.messages, model, temperature=temp)  \n",
    "        self.messages.append({\"role\": \"assistant\", \"content\": response})\n",
    "        return response\n",
    "2) initialize the class with a system message of:\n",
    "    \"You are an expert in liturature. You provide brief replies\"\n",
    "3) the llama() function is defined like this:\n",
    "    def llama(prompt_or_messages, model_size=8, \n",
    "              temperature=0, raw=False, debug=False):\n",
    "    and returns a reponse in a string. Don't redefine this.\n",
    "    valid model sizes are 8, 70 and 405.\n",
    "\"\"\"\n",
    "coder_system_message = (\n",
    "    \"You are an expert writing python Gradio chatbots\" \n",
    ")\n",
    "coder_conv = Conversation(coder_system_message)\n",
    "response = coder_conv.generate(code_query, 405)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "43dd04e2-0986-41f7-9b44-21459445d3d8",
   "metadata": {
    "height": 659
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://0.0.0.0:7860\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://localhost:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This is the code generated by Llama in the video\n",
    "# Your response to the above prompt may differ - try running your response as well!\n",
    "# note, the final line was hand edited to add \"server_name=\"0.0.0.0\" to run in the course environment\n",
    "\n",
    "import gradio as gr\n",
    "\n",
    "# Initialize the conversation with a system message\n",
    "conversation = Conversation(\"You are an expert in literature. You provide brief replies.\")\n",
    "\n",
    "# Define a function to generate a response to the user's input\n",
    "def generate_response(user_input, model_size, temperature):\n",
    "    try:\n",
    "        model_size = int(model_size)\n",
    "        if model_size not in [8, 70, 405]:\n",
    "            return \"Invalid model size. Please choose from 8, 70, or 405.\"\n",
    "        temperature = float(temperature)\n",
    "        if temperature < 0 or temperature > 1:\n",
    "            return \"Invalid temperature. Please choose a value between 0 and 1.\"\n",
    "        response = conversation.generate(user_input, model=model_size, temp=temperature)\n",
    "        return response\n",
    "    except Exception as e:\n",
    "        return str(e)\n",
    "\n",
    "# Create a Gradio interface for the chatbot\n",
    "demo = gr.Interface(\n",
    "    fn=generate_response,\n",
    "    inputs=[\n",
    "        gr.Textbox(label=\"User Input\"),\n",
    "        gr.Radio(label=\"Model Size\", choices=[\"8\", \"70\", \"405\"]),\n",
    "        gr.Slider(label=\"Temperature\", minimum=0, maximum=1, step=0.1, value=0)\n",
    "    ],\n",
    "    outputs=gr.Textbox(label=\"Response\"),\n",
    "    title=\"Literature Expert Chatbot\",\n",
    "    description=\"Ask a question about literature and get a brief response from an expert.\"\n",
    ")\n",
    "\n",
    "# Launch the Gradio app\n",
    "demo.launch(server_name=\"0.0.0.0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d57bf3f4-63b5-45c7-943d-432e25376dd4",
   "metadata": {},
   "source": [
    "# A few more examples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e79366c-678e-4f95-9bc7-520296fd9d54",
   "metadata": {
    "id": "Fz1IIJXajf77"
   },
   "source": [
    "## Compare with different sized Llama chat models\n",
    "\n",
    "By default, the llama31 call uses the Llama 3.1 8B Instruct model. To see how the 70B or 405B model responds, simply pass 70 or 405 after prompt. For example, let compare how the 3 different sized Llama 3.1 chat models respond to the question about a quote in Hemingway's classic The Old Man and the Sea."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b420f914-9100-404d-bb66-22bc362774ac",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "height": 149,
    "id": "f_WT9xQwaFbw",
    "outputId": "e40261ab-2c49-43fe-ec55-e1e5f658b26a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "The quote you're referring to is from Ernest Hemingway's novel \"The Old Man and the Sea\". The full quote is:\n",
      "\n",
      "\"Everything about him was old except his eyes and they were the same color as the sea and they seemed to hold the moon on their dark blue waters.\"\n",
      "\n",
      "This quote is from the first chapter of the novel, and it sets the tone for the rest of the story, which is a poignant and introspective exploration of the old man's life, his struggles, and his connection to the natural world.\n"
     ]
    }
   ],
   "source": [
    "question = \"The quote that starts with 'Everything about him is old' in The Old Man and the Sea\"\n",
    "\n",
    "prompt = f\"\"\"<|begin_of_text|><|start_header_id|>user<|end_header_id|>\n",
    "\n",
    "{question}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\"\"\"\n",
    "\n",
    "response = llama31(prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8019206b-dc6d-4a10-92d1-21b7dd30f528",
   "metadata": {
    "id": "OzlDWmD9koc9"
   },
   "source": [
    "If you like Hemingway, you know Llama 3.1 8B is hallucinating a bit, and \"old before they were young\"? Now try to see if 70B does better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7d73d605-872a-417a-ab06-4854670f2966",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "height": 47,
    "id": "_pZ80oupaFeX",
    "outputId": "ea30acaf-8c39-49fb-b9ba-34cc1f08610b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "The quote you're thinking of is:\n",
      "\n",
      "\"Everything about him was old except his eyes and they seemed the same color as the sea and were cheerful and undefeated.\"\n",
      "\n",
      "- Ernest Hemingway, The Old Man and the Sea\n"
     ]
    }
   ],
   "source": [
    "response = llama31(prompt, 70)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d5812ce-a099-47fc-b57b-e233c40e9498",
   "metadata": {
    "id": "Rnyukk3GlHAX"
   },
   "source": [
    "The response makes more sense, but definitely not Hemingway would've written. In fact, it'd hurt the eyes of his fans. Let's hope 405 will cure the pain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5ad1bace-7b20-429f-bb56-5b542c8a6567",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "height": 47,
    "id": "dIHVm7ralEo0",
    "outputId": "b72f8dc8-6142-46a2-f25c-73c6d5f168a2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "You're thinking of the famous quote from Ernest Hemingway's \"The Old Man and the Sea\"!\n",
      "\n",
      "The full quote is:\n",
      "\n",
      "\"Everything about him was old except his eyes and they were the same color as the sea and were cheerful and undefeated.\"\n",
      "\n",
      "Isn't that a beautiful and poignant description of the old man, Santiago?\n"
     ]
    }
   ],
   "source": [
    "response = llama31(prompt, 405)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9b25935-0914-485b-86dc-3d9f11398a98",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
