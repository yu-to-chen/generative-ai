{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d48c1722-7475-4be8-9ad0-cb540d44e556",
   "metadata": {},
   "source": [
    "# Getting Started With Text Embeddings w/ Mistral"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a3e2a239-970d-4c4a-b56a-260e4cd030b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "#!{sys.executable} -m pip install -U google-generativeai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5a011fc3-ef11-4305-94ee-7fbfc93161bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import google.generativeai as palm\n",
    "import os\n",
    "import pprint\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "#palm.configure(api_key=os.environ['API_KEY'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f144881f-2986-40f2-a9fa-b22c61061962",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv, find_dotenv     \n",
    "\n",
    "from mistralai.client import MistralClient\n",
    "from mistralai.models.chat_completion import ChatMessage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0268f7b6-c4a3-470c-9907-2cfd38056e0a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nfor model in palm.list_models():\\n  if 'embedText' in model.supported_generation_methods:\\n    print(model.name)\\n\""
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "for model in palm.list_models():\n",
    "  if 'embedText' in model.supported_generation_methods:\n",
    "    print(model.name)\n",
    "\"\"\"   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0d7dcaa4-fcbb-4051-b193-e29322c23828",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "\n",
    "from mistralai.client import MistralClient\n",
    "\n",
    "def get_text_embedding(txt):\n",
    "    #client = MistralClient(api_key=api_key, endpoint=dlai_endpoint)\n",
    "    client = MistralClient(api_key=os.getenv(\"MISTRAL_API_KEY\"))\n",
    "    embeddings_batch_response = client.embeddings(model=\"mistral-embed\", input=txt)\n",
    "    return embeddings_batch_response.data[0].embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d97abe5-5fe7-44af-b3a6-e19d45def32b",
   "metadata": {},
   "source": [
    "#### Use the embeddings model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b0250444-f1ae-459a-bbbb-f7d8f54640ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length = 1024\n",
      "[-0.04510498046875, 0.0157623291015625, 0.0043182373046875, -0.0145416259765625, 0.0033359527587890625, 0.00618743896484375, 0.033355712890625, -0.00968170166015625, -0.000301361083984375, -0.0233306884765625]\n"
     ]
    }
   ],
   "source": [
    "x = 'life'\n",
    "close_to_x = 'What is the meaning of life?'\n",
    "\n",
    "\"\"\"\n",
    "model = \"models/embedding-gecko-001\"\n",
    "\n",
    "# Create an embedding\n",
    "embedding_x = palm.generate_embeddings(model=model, text=x)\n",
    "embedding_close_to_x = palm.generate_embeddings(model=model, text=close_to_x)\n",
    "\n",
    "vector = embedding_x['embedding']\n",
    "print(f\"Length = {len(vector)}\")\n",
    "print(vector[:10])\n",
    "\"\"\"\n",
    "\n",
    "#text_embeddings = np.array([get_text_embedding(chunk) for chunk in chunks])\n",
    "embedding_x = get_text_embedding(x)\n",
    "embedding_close_to_x = get_text_embedding(close_to_x)\n",
    "print(f\"Length = {len(embedding_x)}\")\n",
    "print(embedding_x[:10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "840ce865-0ac7-4458-8a16-f596cdc7b730",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length = 1024\n",
      "[-0.03485107421875, 0.027435302734375, 0.0308685302734375, 0.0004267692565917969, 0.0021762847900390625, -0.0298919677734375, 0.05352783203125, -0.0126953125, -0.00418853759765625, -0.035736083984375]\n"
     ]
    }
   ],
   "source": [
    "print(f\"Length = {len(embedding_close_to_x)}\")\n",
    "print(embedding_close_to_x[:10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bba0c9b2-ee63-4670-a546-9ecdf8b585bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7719042309013275\n"
     ]
    }
   ],
   "source": [
    "#similar_measure = np.dot(embedding_x['embedding'], embedding_close_to_x['embedding'])\n",
    "similar_measure = np.dot(embedding_x, embedding_close_to_x)\n",
    "print(similar_measure)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "201a29a6-d2c8-4adb-926b-d8af5544e641",
   "metadata": {},
   "source": [
    "#### Similarity\n",
    "\n",
    "- Calculate the similarity between two sentences as a number between 0 and 1.\n",
    "- Try out your own sentences and check if the similarity calculations match your intuition.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "973df968-d32a-4c11-ae1b-cbba4f91af83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length = 1024\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "emb_1 = embedding_model.get_embeddings(\n",
    "    [\"What is the meaning of life?\"]) # 42!\n",
    "\n",
    "emb_2 = embedding_model.get_embeddings(\n",
    "    [\"How does one spend their time well on Earth?\"])\n",
    "\n",
    "emb_3 = embedding_model.get_embeddings(\n",
    "    [\"Would you like a salad?\"])\n",
    "\n",
    "vec_1 = [emb_1[0].values]\n",
    "vec_2 = [emb_2[0].values]\n",
    "vec_3 = [emb_3[0].values]\n",
    "\n",
    "emb_1 = palm.generate_embeddings(model=model, text=[\"What is the meaning of life?\"])\n",
    "emb_2 = palm.generate_embeddings(model=model, text=[\"How does one spend their time well on Earth?\"])\n",
    "emb_3 = palm.generate_embeddings(model=model, text=[\"Would you like a salad?\"])\n",
    "\n",
    "vec_1 = emb_1['embedding']\n",
    "vec_2 = emb_2['embedding']\n",
    "vec_3 = emb_3['embedding']\n",
    "\"\"\"\n",
    "\n",
    "text1=\"What is the meaning of life?\"\n",
    "text2=\"How does one spend their time well on Earth?\"\n",
    "text3=\"Would you like a salad?\"\n",
    "\n",
    "vec_1 = get_text_embedding(text1)\n",
    "vec_2 = get_text_embedding(text2)\n",
    "vec_3 = get_text_embedding(text3)\n",
    "print(f\"Length = {len(vec_1)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0809ddf-29b2-4506-83ca-74ae6c4f12ec",
   "metadata": {},
   "source": [
    "- Note: the reason we wrap the embeddings (a Python list) in another list is because the `cosine_similarity` function expects either a 2D numpy array or a list of lists.\n",
    "```Python\n",
    "vec_1 = [emb_1[0].values]\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f5adb4a9-9ad2-44d8-9899-c93769f13b7e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.        ],\n",
       "       [0.57735027, 0.81649658]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "X = [[0, 0, 0], [1, 1, 1]]\n",
    "Y = [[1, 0, 0], [1, 1, 0]]\n",
    "cosine_similarity(X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "948057e3-6b32-4d97-b7e1-3d9539c9645d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7440927744532928\n",
      "0.6568434339229725\n",
      "0.6933973190670031\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "print(cosine_similarity(vec_1, vec_2)) \n",
    "print(cosine_similarity(vec_2, vec_3))\n",
    "print(cosine_similarity(vec_1, vec_3])\n",
    "\"\"\"\n",
    "print(np.dot(vec_1, vec_2)) \n",
    "print(np.dot(vec_2, vec_3)) \n",
    "print(np.dot(vec_1, vec_3)) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8963fc82-6f51-414a-ad5b-1caef073e741",
   "metadata": {},
   "source": [
    "#### From word to sentence embeddings\n",
    "- One possible way to calculate sentence embeddings from word embeddings is to take the average of the word embeddings.\n",
    "- This ignores word order and context, so two sentences with different meanings, but the same set of words will end up with the same sentence embedding.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c364ab94-bb0f-4fc2-ab28-82a726a3b75c",
   "metadata": {},
   "outputs": [],
   "source": [
    "in_1 = \"The kids play in the park.\"\n",
    "in_2 = \"The play was for kids in the park.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c51aee0e-b87b-4db4-824c-860db1beaa1f",
   "metadata": {},
   "source": [
    "- Remove stop words like [\"the\", \"in\", \"for\", \"an\", \"is\"] and punctuation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "30212746-6f77-4c8b-b80e-90f383518836",
   "metadata": {},
   "outputs": [],
   "source": [
    "in_pp_1 = [\"kids\", \"play\", \"park\"]\n",
    "in_pp_2 = [\"play\", \"kids\", \"park\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50350ca7-144a-4522-aad8-64156101d62b",
   "metadata": {},
   "source": [
    "- Generate one embedding for each word.  So this is a list of three lists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "db90651c-f6c9-451a-a220-c8cf0c39a04a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "(3, 768)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "embeddings_1 = [palm.generate_embeddings(model=model, text=s)['embedding'] for s in in_pp_1]\n",
    "print(len(embeddings_1))\n",
    "\n",
    "emb_array_1 = np.stack(embeddings_1)\n",
    "print(emb_array_1.shape)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "25bc6f81-3449-40fc-9e8d-f0ced6cc47b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "(3, 1024)\n"
     ]
    }
   ],
   "source": [
    "embeddings_1 = np.array([get_text_embedding(s) for s in in_pp_1])\n",
    "print(len(embeddings_1))\n",
    "\n",
    "emb_array_1 = np.stack(embeddings_1)\n",
    "print(emb_array_1.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66569e55-231e-4b9c-9b02-6d3b0f55dc07",
   "metadata": {},
   "source": [
    "- Use numpy to convert this list of lists into a 2D array of 3 rows and 768 columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a323436f-ea40-49c8-bca8-cba0417deb95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "(3, 1024)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "embeddings_2 = [palm.generate_embeddings(model=model, text=s)['embedding'] for s in in_pp_2]\n",
    "emb_array_2 = np.stack(embeddings_2)\n",
    "print(emb_array_1.shape)\n",
    "\"\"\"\n",
    "\n",
    "embeddings_2 = np.array([get_text_embedding(s) for s in in_pp_2])\n",
    "print(len(embeddings_2))\n",
    "\n",
    "emb_array_2 = np.stack(embeddings_2)\n",
    "print(emb_array_2.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c75efd28-69dc-41af-8f21-4584aa3c7f3e",
   "metadata": {},
   "source": [
    "- Take the average embedding across the 3 word embeddings \n",
    "- You'll get a single embedding of length 768.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8c899bf8-dbb4-4828-88ef-af91bf2fae23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1024,)\n"
     ]
    }
   ],
   "source": [
    "emb_1_mean = emb_array_1.mean(axis = 0) \n",
    "print(emb_1_mean.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "bb1104f9-3c9f-4df0-b0c5-662292000ef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_2_mean = emb_array_2.mean(axis = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b56a7828-b010-4333-93d2-dde544a20040",
   "metadata": {},
   "source": [
    "- Check to see that taking an average of word embeddings results in two sentence embeddings that are identical."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "8530ce68-cc0e-4360-bf1e-1a1f89c43c7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.03903707  0.02631632  0.01692963 -0.00691509]\n",
      "[-0.03903707  0.02631632  0.01692963 -0.00691509]\n"
     ]
    }
   ],
   "source": [
    "print(emb_1_mean[:4])\n",
    "print(emb_2_mean[:4])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af539fbf-79ce-45db-b55f-2e695a046409",
   "metadata": {},
   "source": [
    "#### Get sentence embeddings from the model.\n",
    "- These sentence embeddings account for word order and context.\n",
    "- Verify that the sentence embeddings are not the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f2980be4-56f1-404e-b144-5f6e768932b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The kids play in the park.\n",
      "The play was for kids in the park.\n"
     ]
    }
   ],
   "source": [
    "print(in_1)\n",
    "print(in_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2be678d2-15a6-493b-bd14-5fe2c8dfd422",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "embedding_1 = palm.generate_embeddings(model=model, text=in_1)\n",
    "embedding_2 = palm.generate_embeddings(model=model, text=in_2)\n",
    "vector_1 = embedding_1['embedding']\n",
    "print(vector_1[:4])\n",
    "vector_2 = embedding_2['embedding']\n",
    "print(vector_2[:4])\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f1a311af-a957-4789-9e0a-9834c9423f20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0040104063, -0.02063808, -0.0028912085, -0.007481416]\n",
      "[-0.0154303685, -0.012839607, 0.012309532, -0.00071919535]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "vector_1 = embedding_1['embedding']\n",
    "print(vector_1[:4])\n",
    "vector_2 = embedding_2['embedding']\n",
    "print(vector_2[:4])\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "6a2ee802-9620-4db7-9294-965d7d6843f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.0149078369140625, 0.00988006591796875, 0.041595458984375, -0.027313232421875]\n",
      "[-0.052001953125, 0.032867431640625, 0.054046630859375, -0.007633209228515625]\n"
     ]
    }
   ],
   "source": [
    "vector_1 = get_text_embedding(in_1)\n",
    "vector_2 = get_text_embedding(in_2)\n",
    "print(vector_1[:4])\n",
    "print(vector_2[:4])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
