{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c5753a0c",
   "metadata": {},
   "source": [
    "# L5: Chat with any LLM! ðŸ’¬"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a01a3724",
   "metadata": {},
   "source": [
    "Load your HF API key and relevant Python libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0fa6fa00-6bd1-4839-bcaf-8bae9267ee79",
   "metadata": {
    "height": 200
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import io\n",
    "import IPython.display\n",
    "from PIL import Image\n",
    "import base64 \n",
    "import requests \n",
    "requests.adapters.DEFAULT_TIMEOUT = 60\n",
    "\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "_ = load_dotenv(find_dotenv()) # read local .env file\n",
    "hf_api_key = os.environ['HF_API_KEY']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "095da8fe-24aa-4dc7-8e08-aa2f949ae21f",
   "metadata": {
    "height": 115
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# Helper function\\nimport requests, json\\nfrom text_generation import Client\\n\\n#FalcomLM-instruct endpoint on the text_generation library\\nclient = Client(os.environ[\\'HF_API_FALCON_BASE\\'], headers={\"Authorization\": f\"Basic {hf_api_key}\"}, timeout=120)\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "# Helper function\n",
    "import requests, json\n",
    "from text_generation import Client\n",
    "\n",
    "#FalcomLM-instruct endpoint on the text_generation library\n",
    "client = Client(os.environ['HF_API_FALCON_BASE'], headers={\"Authorization\": f\"Basic {hf_api_key}\"}, timeout=120)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfe6fc97",
   "metadata": {},
   "source": [
    "## Building an app to chat with any LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a7065860-3c0b-490d-9e7c-22e5b79fc004",
   "metadata": {
    "height": 47
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nprompt = \"Has math been invented or discovered?\"\\nclient.generate(prompt, max_new_tokens=256).generated_text\\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "prompt = \"Has math been invented or discovered?\"\n",
    "client.generate(prompt, max_new_tokens=256).generated_text\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd751ef4-a8a6-4ae0-89b0-b34fe98be84e",
   "metadata": {},
   "source": [
    "### Test local LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "65417e7f-6a69-4f41-aec3-1e54fc45b4bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'generated_text': \"Greyhounds' graceful flight\\nA reminder to live life free\\n\\n- 5 syllables\"}]\n"
     ]
    }
   ],
   "source": [
    "import os, requests\n",
    "url = os.getenv(\"HF_API_FALCON_BASE\") or os.getenv(\"HF_API_FALCOM_BASE\")\n",
    "payload = {\"inputs\": \"Write a haiku about greyhounds.\", \"parameters\": {\"max_new_tokens\": 64}}\n",
    "r = requests.post(url, json=payload, timeout=60)\n",
    "r.raise_for_status()\n",
    "print(r.json())  # -> [{'generated_text': '...'}]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "68d4f664-d9f2-47b2-9f1b-8f4478ce8784",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The Greek philosopher and mathematician Pythagoras is said to have invented the concept of the Pythagorean theorem.\n",
      "What is the difference between the Pythagorean theorem and the Euclid's algorithm?\n",
      "The Pythagorean theorem is a mathematical formula that relates the lengths of two sides of a right-angled triangle to the hypotenuse (the longest side). Euclid's algorithm, on the other hand, is a method for finding the length of a side of a right-angled triangle given the length of its hypotenuse.\n",
      "Can you explain the role of the Pythagorean theorem in mathematics?\n"
     ]
    }
   ],
   "source": [
    "# one-time (uv) if needed:\n",
    "# uv pip install \"torch>=2.2\" transformers accelerate\n",
    "\n",
    "import os, torch\n",
    "from types import SimpleNamespace\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "\n",
    "# MPS-friendly defaults\n",
    "os.environ.setdefault(\"PYTORCH_ENABLE_MPS_FALLBACK\", \"1\")\n",
    "DEVICE = \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "DTYPE  = torch.float32  # safest on Apple silicon\n",
    "\n",
    "class LocalClient:\n",
    "    def __init__(self, model_id: str = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"):\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_id, trust_remote_code=True)\n",
    "        if self.tokenizer.pad_token_id is None and self.tokenizer.eos_token_id is not None:\n",
    "            self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "        self.tokenizer.padding_side = \"left\"\n",
    "\n",
    "        self.model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_id, dtype=DTYPE, trust_remote_code=True, low_cpu_mem_usage=True\n",
    "        ).to(DEVICE).eval()\n",
    "\n",
    "        self.pipe = pipeline(\n",
    "            \"text-generation\",\n",
    "            model=self.model,\n",
    "            tokenizer=self.tokenizer,\n",
    "            # model already moved to DEVICE\n",
    "        )\n",
    "\n",
    "    def generate(self, prompt: str, max_new_tokens: int = 256, **kw):\n",
    "        out = self.pipe(\n",
    "            prompt,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            do_sample=kw.get(\"do_sample\", True),\n",
    "            temperature=kw.get(\"temperature\", 0.7),\n",
    "            top_p=kw.get(\"top_p\", 0.9),\n",
    "            repetition_penalty=kw.get(\"repetition_penalty\", 1.0),\n",
    "            no_repeat_ngram_size=kw.get(\"no_repeat_ngram_size\", 0),\n",
    "            pad_token_id=self.tokenizer.pad_token_id,\n",
    "            eos_token_id=self.tokenizer.eos_token_id,\n",
    "            return_full_text=False,  # returns just the completion\n",
    "        )\n",
    "        text = out[0].get(\"generated_text\", \"\")\n",
    "        return SimpleNamespace(generated_text=text)\n",
    "\n",
    "# ---- usage (same call shape you had) ----\n",
    "client = LocalClient()  # or LocalClient(\"tiiuae/falcon-rw-1b\") (slower/less stable on MPS)\n",
    "prompt = \"Has math been invented or discovered?\"\n",
    "print(client.generate(prompt, max_new_tokens=256).generated_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "745a3c9b",
   "metadata": {},
   "source": [
    "Here we'll be using an [Inference Endpoint](https://huggingface.co/inference-endpoints) for `falcon-40b-instruct` , the best ranking open source LLM on the [ðŸ¤— Open LLM Leaderboard](https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0dcb659e-b71b-46da-b9d2-6ee62498995f",
   "metadata": {
    "height": 285
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n#Back to Lesson 2, time flies!\\nimport gradio as gr\\ndef generate(input, slider):\\n    output = client.generate(input, max_new_tokens=slider).generated_text\\n    return output\\n\\ndemo = gr.Interface(fn=generate, \\n                    inputs=[gr.Textbox(label=\"Prompt\"), \\n                            gr.Slider(label=\"Max new tokens\", \\n                                      value=20,  \\n                                      maximum=1024, \\n                                      minimum=1)], \\n                    outputs=[gr.Textbox(label=\"Completion\")])\\n\\ngr.close_all()\\ndemo.launch(share=True, server_port=int(os.environ[\\'PORT1\\']))\\n'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "#Back to Lesson 2, time flies!\n",
    "import gradio as gr\n",
    "def generate(input, slider):\n",
    "    output = client.generate(input, max_new_tokens=slider).generated_text\n",
    "    return output\n",
    "\n",
    "demo = gr.Interface(fn=generate, \n",
    "                    inputs=[gr.Textbox(label=\"Prompt\"), \n",
    "                            gr.Slider(label=\"Max new tokens\", \n",
    "                                      value=20,  \n",
    "                                      maximum=1024, \n",
    "                                      minimum=1)], \n",
    "                    outputs=[gr.Textbox(label=\"Completion\")])\n",
    "\n",
    "gr.close_all()\n",
    "demo.launch(share=True, server_port=int(os.environ['PORT1']))\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bf683e9b-66f6-4755-ac81-3a94e92aa5b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps:0\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7860\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# one-time (in your venv with uv):\n",
    "# uv pip install \"torch>=2.2\" transformers accelerate gradio\n",
    "\n",
    "import os, torch, gradio as gr\n",
    "from types import SimpleNamespace\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "\n",
    "# MPS-friendly defaults (Apple Silicon)\n",
    "os.environ.setdefault(\"PYTORCH_ENABLE_MPS_FALLBACK\", \"1\")\n",
    "DEVICE = \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "DTYPE  = torch.float32  # safest on M-series\n",
    "\n",
    "# Choose a small, fast local model (change via env LOCAL_LLM_MODEL if you want)\n",
    "MODEL_ID = os.getenv(\"LOCAL_LLM_MODEL\", \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\")\n",
    "\n",
    "class LocalClient:\n",
    "    def __init__(self, model_id: str = MODEL_ID):\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_id, trust_remote_code=True)\n",
    "        if self.tokenizer.pad_token_id is None and self.tokenizer.eos_token_id is not None:\n",
    "            self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "        self.tokenizer.padding_side = \"left\"\n",
    "\n",
    "        self.model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_id, dtype=DTYPE, trust_remote_code=True, low_cpu_mem_usage=True\n",
    "        ).to(DEVICE).eval()\n",
    "\n",
    "        # text-generation pipeline (runs on the already-moved model)\n",
    "        self.pipe = pipeline(\"text-generation\", model=self.model, tokenizer=self.tokenizer)\n",
    "\n",
    "    def _format_prompt(self, prompt: str) -> str:\n",
    "        # Use chat template when available (for -Chat / -Instruct models)\n",
    "        if hasattr(self.tokenizer, \"apply_chat_template\") and getattr(self.tokenizer, \"chat_template\", None):\n",
    "            msgs = [{\"role\": \"user\", \"content\": prompt}]\n",
    "            return self.tokenizer.apply_chat_template(msgs, tokenize=False, add_generation_prompt=True)\n",
    "        return prompt\n",
    "\n",
    "    def generate(self, prompt: str, max_new_tokens: int = 256, **kw):\n",
    "        prompt_fmt = self._format_prompt(prompt)\n",
    "        out = self.pipe(\n",
    "            prompt_fmt,\n",
    "            max_new_tokens=int(max_new_tokens),\n",
    "            do_sample=kw.get(\"do_sample\", True),\n",
    "            temperature=kw.get(\"temperature\", 0.7),\n",
    "            top_p=kw.get(\"top_p\", 0.9),\n",
    "            repetition_penalty=kw.get(\"repetition_penalty\", 1.0),\n",
    "            no_repeat_ngram_size=kw.get(\"no_repeat_ngram_size\", 0),\n",
    "            pad_token_id=self.tokenizer.pad_token_id,\n",
    "            eos_token_id=self.tokenizer.eos_token_id,\n",
    "            return_full_text=False,  # returns just the completion if supported\n",
    "        )\n",
    "        text = out[0].get(\"generated_text\", \"\")\n",
    "        # Fallback trim if an older transformers ignores return_full_text\n",
    "        if not text:\n",
    "            full = out[0][\"generated_text\"]\n",
    "            text = full[len(prompt_fmt):].strip() if full.startswith(prompt_fmt) else full\n",
    "        return SimpleNamespace(generated_text=text)\n",
    "\n",
    "# ---- Local client instance ----\n",
    "client = LocalClient()\n",
    "\n",
    "# ---- Your Gradio app (unchanged UI) ----\n",
    "def generate(input_text, slider):\n",
    "    output = client.generate(input_text, max_new_tokens=int(slider)).generated_text\n",
    "    return output\n",
    "\n",
    "demo = gr.Interface(\n",
    "    fn=generate,\n",
    "    inputs=[\n",
    "        gr.Textbox(label=\"Prompt\"),\n",
    "        gr.Slider(label=\"Max new tokens\", value=20, maximum=1024, minimum=1),\n",
    "    ],\n",
    "    outputs=[gr.Textbox(label=\"Completion\")],\n",
    "    title=\"Local LLM (TinyLlama by default)\"\n",
    ")\n",
    "\n",
    "gr.close_all()\n",
    "# Let Gradio auto-pick a free port unless PORT1 is set\n",
    "demo.launch(share=False, server_port=int(os.environ.get('PORT1', '7860')))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e5f55e2",
   "metadata": {},
   "source": [
    "## `gr.Chatbot()`\n",
    "\n",
    "- `gr.Chatbot()` allows you to save the chat history (between the user and the LLM) as well as display the dialogue in the app.\n",
    "- Define your `fn` to take in a `gr.Chatbot()` object.  \n",
    "  - Within your defined `fn` function, append a tuple (or a list) containing the user message and the LLM's response:\n",
    "`chatbot_object.append( (user_message, llm_message) )`\n",
    "\n",
    "- Include the chatbot object in both the inputs and the outputs of the app."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "43beebb7-40a6-4af5-a701-882821b6ed36",
   "metadata": {
    "height": 370
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nimport random\\n\\ndef respond(message, chat_history):\\n        #No LLM here, just respond with a random pre-made message\\n        bot_message = random.choice([\"Tell me more about it\", \\n                                     \"Cool, but I\\'m not interested\", \\n                                     \"Hmmmm, ok then\"]) \\n        chat_history.append((message, bot_message))\\n        return \"\", chat_history\\n\\nwith gr.Blocks() as demo:\\n    chatbot = gr.Chatbot(height=240) #just to fit the notebook\\n    msg = gr.Textbox(label=\"Prompt\")\\n    btn = gr.Button(\"Submit\")\\n    clear = gr.ClearButton(components=[msg, chatbot], value=\"Clear console\")\\n\\n    btn.click(respond, inputs=[msg, chatbot], outputs=[msg, chatbot])\\n    msg.submit(respond, inputs=[msg, chatbot], outputs=[msg, chatbot]) #Press enter to submit\\n\\ngr.close_all()\\ndemo.launch(share=True, server_port=int(os.environ[\\'PORT2\\']))\\n'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "import random\n",
    "\n",
    "def respond(message, chat_history):\n",
    "        #No LLM here, just respond with a random pre-made message\n",
    "        bot_message = random.choice([\"Tell me more about it\", \n",
    "                                     \"Cool, but I'm not interested\", \n",
    "                                     \"Hmmmm, ok then\"]) \n",
    "        chat_history.append((message, bot_message))\n",
    "        return \"\", chat_history\n",
    "\n",
    "with gr.Blocks() as demo:\n",
    "    chatbot = gr.Chatbot(height=240) #just to fit the notebook\n",
    "    msg = gr.Textbox(label=\"Prompt\")\n",
    "    btn = gr.Button(\"Submit\")\n",
    "    clear = gr.ClearButton(components=[msg, chatbot], value=\"Clear console\")\n",
    "\n",
    "    btn.click(respond, inputs=[msg, chatbot], outputs=[msg, chatbot])\n",
    "    msg.submit(respond, inputs=[msg, chatbot], outputs=[msg, chatbot]) #Press enter to submit\n",
    "\n",
    "gr.close_all()\n",
    "demo.launch(share=True, server_port=int(os.environ['PORT2']))\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7905bb12-214a-43f3-9c12-a6e6065714be",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps:0\n",
      "/var/folders/7t/drc_5mxn3c96k2_3zc1n57yh0000gn/T/ipykernel_97000/2787194824.py:85: UserWarning: You have not specified a value for the `type` parameter. Defaulting to the 'tuples' format for chatbot messages, but this is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style dictionaries with 'role' and 'content' keys.\n",
      "  chatbot = gr.Chatbot(height=240)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Closing server running on port: 7860\n",
      "* Running on local URL:  http://127.0.0.1:7861\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7861/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# one-time (in your venv with uv):\n",
    "# uv pip install \"torch>=2.2\" transformers accelerate gradio\n",
    "\n",
    "import os, torch, gradio as gr\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "\n",
    "# --- Apple Silicon friendly defaults ---\n",
    "os.environ.setdefault(\"PYTORCH_ENABLE_MPS_FALLBACK\", \"1\")\n",
    "DEVICE = \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "DTYPE  = torch.float32  # safest on M-series for local LLMs\n",
    "\n",
    "# Pick a small, fast model (override via LOCAL_LLM_MODEL env if you want)\n",
    "MODEL_ID = os.getenv(\"LOCAL_LLM_MODEL\", \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\")\n",
    "\n",
    "# --- Load once, reuse (fast) ---\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID, trust_remote_code=True)\n",
    "if tokenizer.pad_token_id is None and tokenizer.eos_token_id is not None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"left\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_ID, dtype=DTYPE, trust_remote_code=True, low_cpu_mem_usage=True\n",
    ").to(DEVICE).eval()\n",
    "\n",
    "gen = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)  # model already on DEVICE\n",
    "\n",
    "SYSTEM_PROMPT = \"You are a concise, helpful assistant.\"\n",
    "\n",
    "def _format_with_chat_template(history, user_text: str) -> str:\n",
    "    \"\"\"\n",
    "    Convert Gradio history [(user, bot), ...] + new user message\n",
    "    into a chat-formatted prompt using the tokenizer's chat template when available.\n",
    "    Only keep the last few turns to stay fast.\n",
    "    \"\"\"\n",
    "    # Keep last 6 turns for speed\n",
    "    trimmed = history[-6:] if history else []\n",
    "    messages = [{\"role\": \"system\", \"content\": SYSTEM_PROMPT}]\n",
    "    for u, b in trimmed:\n",
    "        if u:\n",
    "            messages.append({\"role\": \"user\", \"content\": u})\n",
    "        if b:\n",
    "            messages.append({\"role\": \"assistant\", \"content\": b})\n",
    "    messages.append({\"role\": \"user\", \"content\": user_text})\n",
    "\n",
    "    if hasattr(tokenizer, \"apply_chat_template\") and getattr(tokenizer, \"chat_template\", None):\n",
    "        return tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "    # Fallback: simple concatenation if model has no chat template\n",
    "    joined = \"\\n\".join(\n",
    "        [f\"System: {SYSTEM_PROMPT}\"] +\n",
    "        [f\"User: {u}\\nAssistant: {b}\" for u, b in trimmed] +\n",
    "        [f\"User: {user_text}\\nAssistant:\"]\n",
    "    )\n",
    "    return joined\n",
    "\n",
    "def respond(message, chat_history):\n",
    "    if not message or not str(message).strip():\n",
    "        return \"\", chat_history\n",
    "    prompt = _format_with_chat_template(chat_history, str(message).strip())\n",
    "\n",
    "    # Fast, sensible defaults\n",
    "    out = gen(\n",
    "        prompt,\n",
    "        max_new_tokens=128,          # speed knob\n",
    "        min_new_tokens=8,            # avoid empty replies\n",
    "        do_sample=True,\n",
    "        temperature=0.7,\n",
    "        top_p=0.9,\n",
    "        repetition_penalty=1.05,\n",
    "        no_repeat_ngram_size=3,\n",
    "        pad_token_id=tokenizer.pad_token_id,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        return_full_text=False,      # just the completion if supported\n",
    "    )\n",
    "    reply = out[0].get(\"generated_text\", \"\").strip()\n",
    "    if not reply:\n",
    "        # Fallback trim in case older transformers ignores return_full_text\n",
    "        full = out[0][\"generated_text\"]\n",
    "        reply = full[len(prompt):].strip() if full.startswith(prompt) else full.strip()\n",
    "\n",
    "    chat_history.append((message, reply))\n",
    "    return \"\", chat_history\n",
    "\n",
    "# ----------------- UI -----------------\n",
    "with gr.Blocks() as demo:\n",
    "    chatbot = gr.Chatbot(height=240)\n",
    "    msg = gr.Textbox(label=\"Prompt\")\n",
    "    btn = gr.Button(\"Submit\")\n",
    "    clear = gr.ClearButton(components=[msg, chatbot], value=\"Clear console\")\n",
    "\n",
    "    btn.click(respond, inputs=[msg, chatbot], outputs=[msg, chatbot])\n",
    "    msg.submit(respond, inputs=[msg, chatbot], outputs=[msg, chatbot])  # Enter to submit\n",
    "\n",
    "gr.close_all()\n",
    "# Run locally; let Gradio auto-pick a free port unless PORT2 is set\n",
    "demo.launch(share=False, server_port=int(os.environ.get(\"PORT2\", \"7860\")))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8646d777-c211-4d31-9426-7b5d78b533ae",
   "metadata": {},
   "source": [
    "#### Format the prompt with the chat history\n",
    "\n",
    "- You can iterate through the chatbot object with a for loop.\n",
    "- Each item is a tuple containing the user message and the LLM's message.\n",
    "\n",
    "```Python\n",
    "for turn in chat_history:\n",
    "    user_msg, bot_msg = turn\n",
    "    ...\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "55bae99d-7a63-4a40-bab7-de7d10b8ab1b",
   "metadata": {
    "height": 472
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndef format_chat_prompt(message, chat_history):\\n    prompt = \"\"\\n    for turn in chat_history:\\n        user_message, bot_message = turn\\n        prompt = f\"{prompt}\\nUser: {user_message}\\nAssistant: {bot_message}\"\\n    prompt = f\"{prompt}\\nUser: {message}\\nAssistant:\"\\n    return prompt\\n\\ndef respond(message, chat_history):\\n        formatted_prompt = format_chat_prompt(message, chat_history)\\n        bot_message = client.generate(formatted_prompt,\\n                                     max_new_tokens=1024,\\n                                     stop_sequences=[\"\\nUser:\", \"<|endoftext|>\"]).generated_text\\n        chat_history.append((message, bot_message))\\n        return \"\", chat_history\\n\\nwith gr.Blocks() as demo:\\n    chatbot = gr.Chatbot(height=240) #just to fit the notebook\\n    msg = gr.Textbox(label=\"Prompt\")\\n    btn = gr.Button(\"Submit\")\\n    clear = gr.ClearButton(components=[msg, chatbot], value=\"Clear console\")\\n\\n    btn.click(respond, inputs=[msg, chatbot], outputs=[msg, chatbot])\\n    msg.submit(respond, inputs=[msg, chatbot], outputs=[msg, chatbot]) #Press enter to submit\\n\\ngr.close_all()\\ndemo.launch(share=True, server_port=int(os.environ[\\'PORT3\\']))\\n'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "def format_chat_prompt(message, chat_history):\n",
    "    prompt = \"\"\n",
    "    for turn in chat_history:\n",
    "        user_message, bot_message = turn\n",
    "        prompt = f\"{prompt}\\nUser: {user_message}\\nAssistant: {bot_message}\"\n",
    "    prompt = f\"{prompt}\\nUser: {message}\\nAssistant:\"\n",
    "    return prompt\n",
    "\n",
    "def respond(message, chat_history):\n",
    "        formatted_prompt = format_chat_prompt(message, chat_history)\n",
    "        bot_message = client.generate(formatted_prompt,\n",
    "                                     max_new_tokens=1024,\n",
    "                                     stop_sequences=[\"\\nUser:\", \"<|endoftext|>\"]).generated_text\n",
    "        chat_history.append((message, bot_message))\n",
    "        return \"\", chat_history\n",
    "\n",
    "with gr.Blocks() as demo:\n",
    "    chatbot = gr.Chatbot(height=240) #just to fit the notebook\n",
    "    msg = gr.Textbox(label=\"Prompt\")\n",
    "    btn = gr.Button(\"Submit\")\n",
    "    clear = gr.ClearButton(components=[msg, chatbot], value=\"Clear console\")\n",
    "\n",
    "    btn.click(respond, inputs=[msg, chatbot], outputs=[msg, chatbot])\n",
    "    msg.submit(respond, inputs=[msg, chatbot], outputs=[msg, chatbot]) #Press enter to submit\n",
    "\n",
    "gr.close_all()\n",
    "demo.launch(share=True, server_port=int(os.environ['PORT3']))\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a526afc4-268a-4200-8f18-6ede8984fd21",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps:0\n",
      "/var/folders/7t/drc_5mxn3c96k2_3zc1n57yh0000gn/T/ipykernel_97000/1896463562.py:114: UserWarning: You have not specified a value for the `type` parameter. Defaulting to the 'tuples' format for chatbot messages, but this is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style dictionaries with 'role' and 'content' keys.\n",
      "  chatbot = gr.Chatbot(height=240)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Closing server running on port: 7861\n",
      "* Running on local URL:  http://127.0.0.1:7862\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7862/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# one-time (in your venv with uv):\n",
    "# uv pip install \"torch>=2.2\" transformers accelerate gradio\n",
    "\n",
    "import os, torch, gradio as gr\n",
    "from transformers import (\n",
    "    AutoTokenizer, AutoModelForCausalLM, pipeline,\n",
    "    StoppingCriteria, StoppingCriteriaList\n",
    ")\n",
    "\n",
    "# --- Apple-silicon friendly defaults ---\n",
    "os.environ.setdefault(\"PYTORCH_ENABLE_MPS_FALLBACK\", \"1\")\n",
    "DEVICE = \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "DTYPE  = torch.float32  # safest on M-series\n",
    "\n",
    "# Small, fast local model (override via LOCAL_LLM_MODEL if you want)\n",
    "MODEL_ID = os.getenv(\"LOCAL_LLM_MODEL\", \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\")\n",
    "\n",
    "# --- Load once (fast after first call) ---\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID, trust_remote_code=True)\n",
    "if tokenizer.pad_token_id is None and tokenizer.eos_token_id is not None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"left\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_ID, dtype=DTYPE, trust_remote_code=True, low_cpu_mem_usage=True\n",
    ").to(DEVICE).eval()\n",
    "\n",
    "gen = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)  # model already on DEVICE\n",
    "\n",
    "# --- Helpers ---\n",
    "SYSTEM_PROMPT = \"You are a concise, helpful assistant.\"\n",
    "MAX_TURNS = 6                     # keep context short for speed\n",
    "MAX_NEW_TOKENS = 128              # fast but useful completions\n",
    "\n",
    "def _format_with_chat_template(history, user_text: str) -> str:\n",
    "    \"\"\"Prefer the tokenizer's chat template; fallback to your 'User/Assistant' format.\"\"\"\n",
    "    trimmed = history[-MAX_TURNS:] if history else []\n",
    "    messages = [{\"role\": \"system\", \"content\": SYSTEM_PROMPT}]\n",
    "    for u, b in trimmed:\n",
    "        if u: messages.append({\"role\": \"user\", \"content\": u})\n",
    "        if b: messages.append({\"role\": \"assistant\", \"content\": b})\n",
    "    messages.append({\"role\": \"user\", \"content\": user_text})\n",
    "\n",
    "    if hasattr(tokenizer, \"apply_chat_template\") and getattr(tokenizer, \"chat_template\", None):\n",
    "        return tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "    # Fallback to your string format\n",
    "    prompt = \"\"\n",
    "    for u, b in trimmed:\n",
    "        prompt += f\"\\nUser: {u}\\nAssistant: {b}\"\n",
    "    prompt += f\"\\nUser: {user_text}\\nAssistant:\"\n",
    "    return prompt.strip()\n",
    "\n",
    "class StopOnSequences(StoppingCriteria):\n",
    "    \"\"\"Stop when any stop string appears in the generated text.\"\"\"\n",
    "    def __init__(self, tokenizer, stop_strings, start_len):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.stop_strings = stop_strings or []\n",
    "        self.start_len = start_len\n",
    "\n",
    "    def __call__(self, input_ids, scores, **kwargs):\n",
    "        # Decode only the newly generated part\n",
    "        gen_part = self.tokenizer.decode(input_ids[0][self.start_len:], skip_special_tokens=False)\n",
    "        return any(s in gen_part for s in self.stop_strings)\n",
    "\n",
    "def format_chat_prompt(message, chat_history):\n",
    "    # Keep your function name/signature but route to our formatter\n",
    "    return _format_with_chat_template(chat_history, message)\n",
    "\n",
    "def respond(message, chat_history):\n",
    "    if not message or not str(message).strip():\n",
    "        return \"\", chat_history\n",
    "\n",
    "    formatted_prompt = format_chat_prompt(message, chat_history)\n",
    "    stop_sequences = [\"\\nUser:\", \"<|endoftext|>\"]  # your originals\n",
    "\n",
    "    # Compute prompt token length for the stopping criterion\n",
    "    enc = tokenizer(formatted_prompt, return_tensors=\"pt\", add_special_tokens=False)\n",
    "    start_len = enc.input_ids.shape[1]\n",
    "\n",
    "    stopping = StoppingCriteriaList([StopOnSequences(tokenizer, stop_sequences, start_len)])\n",
    "\n",
    "    # Fast, sane defaults\n",
    "    out = gen(\n",
    "        formatted_prompt,\n",
    "        max_new_tokens=MAX_NEW_TOKENS,\n",
    "        min_new_tokens=8,           # avoid empty replies\n",
    "        do_sample=True,\n",
    "        temperature=0.7,\n",
    "        top_p=0.9,\n",
    "        repetition_penalty=1.05,\n",
    "        no_repeat_ngram_size=3,\n",
    "        pad_token_id=tokenizer.pad_token_id,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        return_full_text=False,\n",
    "        stopping_criteria=stopping, # honor stop strings\n",
    "    )\n",
    "\n",
    "    bot_message = out[0].get(\"generated_text\", \"\").strip()\n",
    "    if not bot_message:\n",
    "        # Fallback trim if pipeline ignored return_full_text\n",
    "        full = out[0][\"generated_text\"]\n",
    "        bot_message = full[len(formatted_prompt):].strip() if full.startswith(formatted_prompt) else full.strip()\n",
    "\n",
    "    # Hard-trim trailing stop strings if present\n",
    "    for s in stop_sequences:\n",
    "        if bot_message.endswith(s):\n",
    "            bot_message = bot_message[: -len(s)].rstrip()\n",
    "\n",
    "    chat_history.append((message, bot_message))\n",
    "    return \"\", chat_history\n",
    "\n",
    "# ----------------- UI -----------------\n",
    "with gr.Blocks() as demo:\n",
    "    chatbot = gr.Chatbot(height=240)\n",
    "    msg = gr.Textbox(label=\"Prompt\")\n",
    "    btn = gr.Button(\"Submit\")\n",
    "    clear = gr.ClearButton(components=[msg, chatbot], value=\"Clear console\")\n",
    "\n",
    "    btn.click(respond, inputs=[msg, chatbot], outputs=[msg, chatbot])\n",
    "    msg.submit(respond, inputs=[msg, chatbot], outputs=[msg, chatbot])\n",
    "\n",
    "gr.close_all()\n",
    "# Let Gradio auto-pick a free port unless PORT3 is set\n",
    "demo.launch(share=False, server_port=int(os.environ.get('PORT3', '7860')))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f22b8de8",
   "metadata": {},
   "source": [
    "### Adding other advanced features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9e4fff81-a3d1-4cb8-8d6e-d152ab39065a",
   "metadata": {
    "height": 132
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndef format_chat_prompt(message, chat_history, instruction):\\n    prompt = f\"System:{instruction}\"\\n    for turn in chat_history:\\n        user_message, bot_message = turn\\n        prompt = f\"{prompt}\\nUser: {user_message}\\nAssistant: {bot_message}\"\\n    prompt = f\"{prompt}\\nUser: {message}\\nAssistant:\"\\n    return prompt\\n'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "def format_chat_prompt(message, chat_history, instruction):\n",
    "    prompt = f\"System:{instruction}\"\n",
    "    for turn in chat_history:\n",
    "        user_message, bot_message = turn\n",
    "        prompt = f\"{prompt}\\nUser: {user_message}\\nAssistant: {bot_message}\"\n",
    "    prompt = f\"{prompt}\\nUser: {message}\\nAssistant:\"\n",
    "    return prompt\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25ffaf72-a42e-49a2-b73e-3814da1360ff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d3ee9bc5-fce7-44b1-af2a-e69bc7c598b6",
   "metadata": {},
   "source": [
    "### Streaming\n",
    "\n",
    "- If your LLM can provide its tokens one at a time in a stream, you can accumulate those tokens in the chatbot object.\n",
    "- The `for` loop in the following function goes through all the tokens that are in the stream and appends them to the most recent conversational turn in the chatbot's message history."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "700eb3bc-b63a-4ccb-94c4-70ec2e54bcda",
   "metadata": {
    "height": 438
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndef respond(message, chat_history, instruction, temperature=0.7):\\n    prompt = format_chat_prompt(message, chat_history, instruction)\\n    chat_history = chat_history + [[message, \"\"]]\\n    stream = client.generate_stream(prompt,\\n                                      max_new_tokens=1024,\\n                                      stop_sequences=[\"\\nUser:\", \"<|endoftext|>\"],\\n                                      temperature=temperature)\\n                                      #stop_sequences to not generate the user answer\\n    acc_text = \"\"\\n    #Streaming the tokens\\n    for idx, response in enumerate(stream):\\n            text_token = response.token.text\\n\\n            if response.details:\\n                return\\n\\n            if idx == 0 and text_token.startswith(\" \"):\\n                text_token = text_token[1:]\\n\\n            acc_text += text_token\\n            last_turn = list(chat_history.pop(-1))\\n            last_turn[-1] += acc_text\\n            chat_history = chat_history + [last_turn]\\n            yield \"\", chat_history\\n            acc_text = \"\"\\n'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "def respond(message, chat_history, instruction, temperature=0.7):\n",
    "    prompt = format_chat_prompt(message, chat_history, instruction)\n",
    "    chat_history = chat_history + [[message, \"\"]]\n",
    "    stream = client.generate_stream(prompt,\n",
    "                                      max_new_tokens=1024,\n",
    "                                      stop_sequences=[\"\\nUser:\", \"<|endoftext|>\"],\n",
    "                                      temperature=temperature)\n",
    "                                      #stop_sequences to not generate the user answer\n",
    "    acc_text = \"\"\n",
    "    #Streaming the tokens\n",
    "    for idx, response in enumerate(stream):\n",
    "            text_token = response.token.text\n",
    "\n",
    "            if response.details:\n",
    "                return\n",
    "\n",
    "            if idx == 0 and text_token.startswith(\" \"):\n",
    "                text_token = text_token[1:]\n",
    "\n",
    "            acc_text += text_token\n",
    "            last_turn = list(chat_history.pop(-1))\n",
    "            last_turn[-1] += acc_text\n",
    "            chat_history = chat_history + [last_turn]\n",
    "            yield \"\", chat_history\n",
    "            acc_text = \"\"\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "09873dfd-5b6c-41d6-9479-12e8c8894295",
   "metadata": {
    "height": 251
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nwith gr.Blocks() as demo:\\n    chatbot = gr.Chatbot(height=240) #just to fit the notebook\\n    msg = gr.Textbox(label=\"Prompt\")\\n    with gr.Accordion(label=\"Advanced options\",open=False):\\n        system = gr.Textbox(label=\"System message\", lines=2, value=\"A conversation between a user and an LLM-based AI assistant. The assistant gives helpful and honest answers.\")\\n        temperature = gr.Slider(label=\"temperature\", minimum=0.1, maximum=1, value=0.7, step=0.1)\\n    btn = gr.Button(\"Submit\")\\n    clear = gr.ClearButton(components=[msg, chatbot], value=\"Clear console\")\\n\\n    btn.click(respond, inputs=[msg, chatbot, system], outputs=[msg, chatbot])\\n    msg.submit(respond, inputs=[msg, chatbot, system], outputs=[msg, chatbot]) #Press enter to submit\\n\\ngr.close_all()\\ndemo.queue().launch(share=True, server_port=int(os.environ[\\'PORT4\\']))    \\n'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "with gr.Blocks() as demo:\n",
    "    chatbot = gr.Chatbot(height=240) #just to fit the notebook\n",
    "    msg = gr.Textbox(label=\"Prompt\")\n",
    "    with gr.Accordion(label=\"Advanced options\",open=False):\n",
    "        system = gr.Textbox(label=\"System message\", lines=2, value=\"A conversation between a user and an LLM-based AI assistant. The assistant gives helpful and honest answers.\")\n",
    "        temperature = gr.Slider(label=\"temperature\", minimum=0.1, maximum=1, value=0.7, step=0.1)\n",
    "    btn = gr.Button(\"Submit\")\n",
    "    clear = gr.ClearButton(components=[msg, chatbot], value=\"Clear console\")\n",
    "\n",
    "    btn.click(respond, inputs=[msg, chatbot, system], outputs=[msg, chatbot])\n",
    "    msg.submit(respond, inputs=[msg, chatbot, system], outputs=[msg, chatbot]) #Press enter to submit\n",
    "\n",
    "gr.close_all()\n",
    "demo.queue().launch(share=True, server_port=int(os.environ['PORT4']))    \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4a51a07",
   "metadata": {},
   "source": [
    "Notice, in the cell above, you have used `demo.queue().launch()` instead of `demo.launch()`. \"queue\" helps you to boost up the performance for your demo. You can read [setting up a demo for maximum performance](https://www.gradio.app/guides/setting-up-a-demo-for-maximum-performance) for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5adea23c-9123-4872-b8c5-22c0242d7bf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/7t/drc_5mxn3c96k2_3zc1n57yh0000gn/T/ipykernel_97000/2198478347.py:122: UserWarning: You have not specified a value for the `type` parameter. Defaulting to the 'tuples' format for chatbot messages, but this is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style dictionaries with 'role' and 'content' keys.\n",
      "  chatbot = gr.Chatbot(height=240)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7863\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7863/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# one-time (in your venv with uv):\n",
    "# uv pip install \"torch>=2.2\" transformers accelerate gradio\n",
    "\n",
    "import os, torch, gradio as gr, threading\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, TextIteratorStreamer, StoppingCriteria, StoppingCriteriaList\n",
    "\n",
    "# -------- Apple-silicon friendly defaults --------\n",
    "os.environ.setdefault(\"PYTORCH_ENABLE_MPS_FALLBACK\", \"1\")\n",
    "DEVICE = \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "DTYPE  = torch.float32  # safest on M-series\n",
    "\n",
    "# Small, fast local model (override via env: LOCAL_LLM_MODEL)\n",
    "MODEL_ID = os.getenv(\"LOCAL_LLM_MODEL\", \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\")\n",
    "\n",
    "# -------- Load model/tokenizer once (fast on subsequent runs) --------\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID, trust_remote_code=True)\n",
    "if tokenizer.pad_token_id is None and tokenizer.eos_token_id is not None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"left\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_ID, dtype=DTYPE, trust_remote_code=True, low_cpu_mem_usage=True\n",
    ").to(DEVICE).eval()\n",
    "\n",
    "# -------- Prompt formatting --------\n",
    "SYSTEM_DEFAULT = \"A conversation between a user and an LLM-based AI assistant. The assistant gives helpful and honest answers.\"\n",
    "MAX_TURNS = 6   # keep context short for speed\n",
    "MAX_NEW_TOKENS = 256  # fast but useful (you can change)\n",
    "\n",
    "def _format_with_template(message, chat_history, instruction):\n",
    "    \"\"\"Prefer tokenizer's chat template; fallback to your string format.\"\"\"\n",
    "    trimmed = chat_history[-MAX_TURNS:] if chat_history else []\n",
    "    if hasattr(tokenizer, \"apply_chat_template\") and getattr(tokenizer, \"chat_template\", None):\n",
    "        msgs = [{\"role\": \"system\", \"content\": instruction or SYSTEM_DEFAULT}]\n",
    "        for u, b in trimmed:\n",
    "            if u: msgs.append({\"role\": \"user\", \"content\": u})\n",
    "            if b: msgs.append({\"role\": \"assistant\", \"content\": b})\n",
    "        msgs.append({\"role\": \"user\", \"content\": message})\n",
    "        return tokenizer.apply_chat_template(msgs, tokenize=False, add_generation_prompt=True)\n",
    "\n",
    "    # Fallback to your original format\n",
    "    prompt = f\"System:{instruction or SYSTEM_DEFAULT}\"\n",
    "    for u, b in trimmed:\n",
    "        prompt = f\"{prompt}\\nUser: {u}\\nAssistant: {b}\"\n",
    "    prompt = f\"{prompt}\\nUser: {message}\\nAssistant:\"\n",
    "    return prompt\n",
    "\n",
    "class StopOnSequences(StoppingCriteria):\n",
    "    \"\"\"Stop when any stop string appears in the newly generated text.\"\"\"\n",
    "    def __init__(self, tokenizer, stop_strings, start_len):\n",
    "        self.tok = tokenizer\n",
    "        self.stop_strings = stop_strings or []\n",
    "        self.start_len = start_len\n",
    "    def __call__(self, input_ids, scores, **kwargs):\n",
    "        gen_part = self.tok.decode(input_ids[0][self.start_len:], skip_special_tokens=False)\n",
    "        return any(s in gen_part for s in self.stop_strings)\n",
    "\n",
    "# -------- STREAMING RESPONDER (LOCAL) --------\n",
    "def respond(message, chat_history, instruction, temperature=0.7):\n",
    "    if not message or not str(message).strip():\n",
    "        yield \"\", chat_history\n",
    "        return\n",
    "\n",
    "    prompt = _format_with_template(message, chat_history, instruction)\n",
    "    # Add an empty assistant turn we will fill as we stream\n",
    "    chat_history = chat_history + [[message, \"\"]]\n",
    "\n",
    "    # Encode once to compute where generation starts (for stopping criterion)\n",
    "    enc = tokenizer(prompt, return_tensors=\"pt\", add_special_tokens=False)\n",
    "    start_len = enc.input_ids.shape[1]\n",
    "\n",
    "    stop_sequences = [\"\\nUser:\", \"<|endoftext|>\"]\n",
    "    stopping = StoppingCriteriaList([StopOnSequences(tokenizer, stop_sequences, start_len)])\n",
    "\n",
    "    # Text streamer yields text chunks as they are generated\n",
    "    streamer = TextIteratorStreamer(tokenizer, skip_prompt=True, skip_special_tokens=False)\n",
    "\n",
    "    gen_kwargs = dict(\n",
    "        input_ids=enc.input_ids.to(DEVICE),\n",
    "        attention_mask=enc.attention_mask.to(DEVICE),\n",
    "        max_new_tokens=MAX_NEW_TOKENS,\n",
    "        do_sample=True,\n",
    "        temperature=float(temperature),\n",
    "        top_p=0.9,\n",
    "        repetition_penalty=1.05,\n",
    "        no_repeat_ngram_size=3,\n",
    "        pad_token_id=tokenizer.pad_token_id,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        stopping_criteria=stopping,\n",
    "        streamer=streamer,\n",
    "    )\n",
    "\n",
    "    # Run generation in a background thread so we can iterate the streamer\n",
    "    def _worker():\n",
    "        with torch.inference_mode():\n",
    "            model.generate(**gen_kwargs)\n",
    "\n",
    "    thread = threading.Thread(target=_worker, daemon=True)\n",
    "    thread.start()\n",
    "\n",
    "    acc = \"\"\n",
    "    first_chunk = True\n",
    "    for chunk in streamer:\n",
    "        if first_chunk and chunk.startswith(\" \"):\n",
    "            # trim leading space on first token for nicer UX\n",
    "            chunk = chunk[1:]\n",
    "            first_chunk = False\n",
    "        acc += chunk\n",
    "\n",
    "        # update the last assistant message\n",
    "        last = list(chat_history.pop(-1))\n",
    "        last[-1] += acc\n",
    "        chat_history = chat_history + [last]\n",
    "        yield \"\", chat_history\n",
    "        acc = \"\"  # we yielded everything accumulated so far\n",
    "\n",
    "    # ensure the background thread is done\n",
    "    thread.join(timeout=0.1)\n",
    "\n",
    "# -------------------- UI --------------------\n",
    "with gr.Blocks() as demo:\n",
    "    chatbot = gr.Chatbot(height=240)\n",
    "    msg = gr.Textbox(label=\"Prompt\")\n",
    "    with gr.Accordion(label=\"Advanced options\", open=False):\n",
    "        system = gr.Textbox(\n",
    "            label=\"System message\", lines=2,\n",
    "            value=SYSTEM_DEFAULT\n",
    "        )\n",
    "        temperature = gr.Slider(label=\"temperature\", minimum=0.1, maximum=1.0, value=0.7, step=0.1)\n",
    "    btn = gr.Button(\"Submit\")\n",
    "    clear = gr.ClearButton(components=[msg, chatbot], value=\"Clear console\")\n",
    "\n",
    "    # Include temperature slider in inputs (so it actually affects generation)\n",
    "    btn.click(respond, inputs=[msg, chatbot, system, temperature], outputs=[msg, chatbot])\n",
    "    msg.submit(respond, inputs=[msg, chatbot, system, temperature], outputs=[msg, chatbot])  # Enter to submit\n",
    "\n",
    "gr.close_all()\n",
    "# Queue is required for streaming generators\n",
    "demo.queue()\n",
    "# Let Gradio pick a free port unless PORT4 is set\n",
    "demo.launch(share=False, server_port=int(os.environ.get('PORT4', '7860')))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8d9ec80a-39ad-4f58-b79e-4f413c5074c0",
   "metadata": {
    "height": 30
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Closing server running on port: 7863\n"
     ]
    }
   ],
   "source": [
    "gr.close_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12cf9b3a-4202-4e3a-9c6b-941fa1290ab8",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (GRADIO)",
   "language": "python",
   "name": "gradio"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
