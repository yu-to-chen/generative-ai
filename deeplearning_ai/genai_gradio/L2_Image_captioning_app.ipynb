{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "adf933f7",
   "metadata": {},
   "source": [
    "# L2: Image captioning app 🖼️📝"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b8996b4",
   "metadata": {},
   "source": [
    "Load your HF API key and relevant Python libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3471d1ed-41a0-473c-b3c7-99a9e14dffaf",
   "metadata": {
    "height": 149
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import io\n",
    "import IPython.display\n",
    "from PIL import Image\n",
    "import base64 \n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "_ = load_dotenv(find_dotenv()) # read local .env file\n",
    "hf_api_key = os.environ['HF_API_KEY']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b55a0f7c-25ab-446c-972b-fbebf3ca37a8",
   "metadata": {},
   "source": [
    "## See .env for \"Serverless Inference API (no endpoint to manage)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "05896028-3b43-408e-a899-109bde9625e5",
   "metadata": {
    "height": 302
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# Helper functions\n",
    "import requests, json\n",
    "\n",
    "#Image-to-text endpoint\n",
    "def get_completion(inputs, parameters=None, ENDPOINT_URL=os.environ['HF_API_ITT_BASE']):\n",
    "    headers = {\n",
    "      \"Authorization\": f\"Bearer {hf_api_key}\",\n",
    "      \"Content-Type\": \"application/json\"\n",
    "    }\n",
    "    data = { \"inputs\": inputs }\n",
    "    if parameters is not None:\n",
    "        data.update({\"parameters\": parameters})\n",
    "    response = requests.request(\"POST\",\n",
    "                                ENDPOINT_URL,\n",
    "                                headers=headers,\n",
    "                                data=json.dumps(data))\n",
    "    return json.loads(response.content.decode(\"utf-8\"))\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "97fe5b9b-eddb-4fcf-9b98-74f58b5d47c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, json, base64, requests\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "def _is_url(x: str) -> bool:\n",
    "    try:\n",
    "        s = urlparse(x)\n",
    "        return s.scheme in (\"http\", \"https\") and bool(s.netloc)\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "def _to_image_bytes(inp) -> bytes:\n",
    "    if isinstance(inp, (bytes, bytearray)):\n",
    "        return bytes(inp)\n",
    "    if isinstance(inp, str) and _is_url(inp):\n",
    "        r = requests.get(inp, timeout=30)\n",
    "        r.raise_for_status()\n",
    "        return r.content\n",
    "    if isinstance(inp, str) and os.path.exists(inp):\n",
    "        with open(inp, \"rb\") as f:\n",
    "            return f.read()\n",
    "    raise ValueError(\"inputs must be an image URL, local path, or bytes\")\n",
    "\n",
    "def _auth_header(token: str | None) -> dict:\n",
    "    return {\"Authorization\": f\"Bearer {token}\"} if token else {}\n",
    "\n",
    "def _explain_and_raise(resp: requests.Response):\n",
    "    ct = resp.headers.get(\"Content-Type\", \"\")\n",
    "    body = None\n",
    "    try:\n",
    "        body = resp.json() if \"application/json\" in ct else resp.text\n",
    "    except Exception:\n",
    "        body = resp.text\n",
    "    msg = f\"HTTP {resp.status_code} from {resp.url}\\nContent-Type: {ct}\\nBody (truncated): {str(body)[:800]}\"\n",
    "    # Common hints\n",
    "    if resp.status_code in (401, 403):\n",
    "        msg += \"\\nHint: missing/invalid token or endpoint is protected.\"\n",
    "    elif resp.status_code == 404:\n",
    "        msg += \"\\nHint: check ENDPOINT_URL is correct (model path or endpoint base).\"\n",
    "    elif resp.status_code == 415:\n",
    "        msg += \"\\nHint: wrong Content-Type for this model/endpoint.\"\n",
    "    elif resp.status_code == 422:\n",
    "        msg += \"\\nHint: payload schema is wrong for this model.\"\n",
    "    raise RuntimeError(msg)\n",
    "\n",
    "def get_completion(inputs, parameters=None, ENDPOINT_URL=None, token=None, timeout=60):\n",
    "    \"\"\"\n",
    "    Image-to-text request that is robust across:\n",
    "      - HF serverless (api-inference.huggingface.co/models/<owner>/<model>)\n",
    "      - HF dedicated endpoints (*.endpoints.huggingface.cloud)\n",
    "    Strategy:\n",
    "      1) Send image BYTES with Content-Type: application/octet-stream (most vision models).\n",
    "      2) If 400/415, and 'inputs' was a URL, RETRY as JSON: {\"inputs\": \"<image_url>\", \"parameters\": {...}}\n",
    "    \"\"\"\n",
    "    url = ENDPOINT_URL or os.getenv(\"HF_API_ITT_BASE\")\n",
    "    if not url:\n",
    "        raise RuntimeError(\"HF_API_ITT_BASE not set; export it or pass ENDPOINT_URL.\")\n",
    "\n",
    "    tok = token or os.getenv(\"HF_API_TOKEN\") or os.getenv(\"HUGGINGFACEHUB_API_TOKEN\")\n",
    "\n",
    "    # --- Attempt 1: binary bytes (universal for vision) ---\n",
    "    try:\n",
    "        img_bytes = _to_image_bytes(inputs)\n",
    "    except Exception as e:\n",
    "        # If we can't turn it into bytes but it's a URL, we'll try JSON below\n",
    "        img_bytes = None\n",
    "\n",
    "    if img_bytes is not None:\n",
    "        headers = {\"Accept\": \"application/json\", \"Content-Type\": \"application/octet-stream\", **_auth_header(tok)}\n",
    "        resp = requests.post(url, headers=headers, data=img_bytes, timeout=timeout)\n",
    "        if 200 <= resp.status_code < 300:\n",
    "            # Parse JSON if possible; otherwise return raw\n",
    "            if \"application/json\" in resp.headers.get(\"Content-Type\", \"\"):\n",
    "                try:\n",
    "                    return resp.json()\n",
    "                except Exception:\n",
    "                    return resp.text\n",
    "            return resp.content\n",
    "        # If the failure looks like a format issue, try JSON fallback when possible\n",
    "        if resp.status_code in (400, 415) and isinstance(inputs, str) and _is_url(inputs):\n",
    "            pass  # fall through to JSON retry below\n",
    "        else:\n",
    "            _explain_and_raise(resp)\n",
    "\n",
    "    # --- Attempt 2: JSON with URL (some models/endpoints prefer this) ---\n",
    "    if isinstance(inputs, str) and _is_url(inputs):\n",
    "        payload = {\"inputs\": inputs, \"parameters\": parameters or {}}\n",
    "        headers = {\"Accept\": \"application/json\", \"Content-Type\": \"application/json\", **_auth_header(tok)}\n",
    "        resp = requests.post(url, headers=headers, json=payload, timeout=timeout)\n",
    "        if 200 <= resp.status_code < 300:\n",
    "            try:\n",
    "                return resp.json()\n",
    "            except Exception:\n",
    "                return resp.text\n",
    "        _explain_and_raise(resp)\n",
    "\n",
    "    # --- Optional Attempt 3: JSON with base64 (rarely needed, but here if you want it) ---\n",
    "    if img_bytes is not None:\n",
    "        b64 = base64.b64encode(img_bytes).decode(\"utf-8\")\n",
    "        payload = {\"inputs\": b64, \"parameters\": parameters or {}}\n",
    "        headers = {\"Accept\": \"application/json\", \"Content-Type\": \"application/json\", **_auth_header(tok)}\n",
    "        resp = requests.post(url, headers=headers, json=payload, timeout=timeout)\n",
    "        if 200 <= resp.status_code < 300:\n",
    "            try:\n",
    "                return resp.json()\n",
    "            except Exception:\n",
    "                return resp.text\n",
    "        _explain_and_raise(resp)\n",
    "\n",
    "    raise RuntimeError(\"Could not prepare a valid payload for the image input.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2541e21e",
   "metadata": {},
   "source": [
    "## Building an image captioning app "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1ca462d",
   "metadata": {},
   "source": [
    "Here we'll be using an [Inference Endpoint](https://huggingface.co/inference-endpoints) for `Salesforce/blip-image-captioning-base` a 14M parameter captioning model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b98cf3f7",
   "metadata": {},
   "source": [
    "The free images are available on: https://free-images.com/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "eb7154b9-fa1c-416e-8ee2-635c27720539",
   "metadata": {
    "height": 64
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://free-images.com/sm/9596/dog_animal_greyhound_983023.jpg\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "image_url = \"https://free-images.com/sm/9596/dog_animal_greyhound_983023.jpg\"\n",
    "display(IPython.display.Image(url=image_url))\n",
    "\n",
    "#get_completion(image_url)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "280ea502-04a9-445b-b14e-4e3e0cae84ea",
   "metadata": {},
   "source": [
    "## Need to set up cloud endpoint to use the following code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "67bebf1f-50fd-40ac-bedf-d5b03b97253f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nimage_url = \"https://free-images.com/sm/9596/dog_animal_greyhound_983023.jpg\"\\nresult = get_completion(\\n    image_url,\\n    ENDPOINT_URL=\"https://api-inference.huggingface.co/models/nlpconnect/vit-gpt2-image-captioning\",\\n    token=os.getenv(\"HF_API_TOKEN\"),\\n)\\nprint(result)\\n# Typical shape: [{\\'generated_text\\': \\'...\\'}]\\nif isinstance(result, list) and result and isinstance(result[0], dict):\\n    print(\"Caption:\", result[0].get(\"generated_text\"))\\n'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "image_url = \"https://free-images.com/sm/9596/dog_animal_greyhound_983023.jpg\"\n",
    "result = get_completion(\n",
    "    image_url,\n",
    "    ENDPOINT_URL=\"https://api-inference.huggingface.co/models/nlpconnect/vit-gpt2-image-captioning\",\n",
    "    token=os.getenv(\"HF_API_TOKEN\"),\n",
    ")\n",
    "print(result)\n",
    "# Typical shape: [{'generated_text': '...'}]\n",
    "if isinstance(result, list) and result and isinstance(result[0], dict):\n",
    "    print(\"Caption:\", result[0].get(\"generated_text\"))\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ce9dcdeb-1416-4c89-bdc5-2b134b63e9ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nBLIP_URL = \"https://api-inference.huggingface.co/models/Salesforce/blip-image-captioning-base\"\\nres = get_completion(image_url, ENDPOINT_URL=BLIP_URL, token=os.getenv(\"HF_API_TOKEN\"))\\nprint(res)\\n'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "BLIP_URL = \"https://api-inference.huggingface.co/models/Salesforce/blip-image-captioning-base\"\n",
    "res = get_completion(image_url, ENDPOINT_URL=BLIP_URL, token=os.getenv(\"HF_API_TOKEN\"))\n",
    "print(res)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90da616d",
   "metadata": {},
   "source": [
    "## Captioning with `gr.Interface()`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3bee33c-5e85-4677-b25a-fcee2b36703a",
   "metadata": {},
   "source": [
    "#### gr.Image()\n",
    "- The `type` parameter is the format that the `fn` function expects to receive as its input.  If `type` is `numpy` or `pil`, `gr.Image()` will convert the uploaded file to this format before sending it to the `fn` function.\n",
    "- If `type` is `filepath`, `gr.Image()` will temporarily store the image and provide a string path to that image location as input to the `fn` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8021fac0-9300-44d5-adb1-6ac7111f38f6",
   "metadata": {
    "height": 404
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/site-packages/gradio_client/documentation.py:105: UserWarning: Could not get documentation group for <class 'gradio.mix.Parallel'>: No known documentation group for module 'gradio.mix'\n",
      "  warnings.warn(f\"Could not get documentation group for {cls}: {exc}\")\n",
      "/usr/local/lib/python3.9/site-packages/gradio_client/documentation.py:105: UserWarning: Could not get documentation group for <class 'gradio.mix.Series'>: No known documentation group for module 'gradio.mix'\n",
      "  warnings.warn(f\"Could not get documentation group for {cls}: {exc}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  https://0.0.0.0:7860\n",
      "IMPORTANT: You are using gradio version 3.37.0, however version 4.44.1 is available, please upgrade.\n",
      "--------\n",
      "\n",
      "Could not create share link. Missing file: /usr/local/lib/python3.9/site-packages/gradio/frpc_linux_amd64_v0.2. \n",
      "\n",
      "Please check your internet connection. This can happen if your antivirus software blocks the download of this file. You can install manually by following these steps: \n",
      "\n",
      "1. Download this file: https://cdn-media.huggingface.co/frpc-gradio-0.2/frpc_linux_amd64\n",
      "2. Rename the downloaded file to: frpc_linux_amd64_v0.2\n",
      "3. Move the file to this location: /usr/local/lib/python3.9/site-packages/gradio\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://s172-29-9-197p7860.lab-aws-production.deeplearning.ai/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gradio as gr \n",
    "\n",
    "def image_to_base64_str(pil_image):\n",
    "    byte_arr = io.BytesIO()\n",
    "    pil_image.save(byte_arr, format='PNG')\n",
    "    byte_arr = byte_arr.getvalue()\n",
    "    return str(base64.b64encode(byte_arr).decode('utf-8'))\n",
    "\n",
    "def captioner(image):\n",
    "    base64_image = image_to_base64_str(image)\n",
    "    result = get_completion(base64_image)\n",
    "    return result[0]['generated_text']\n",
    "\n",
    "gr.close_all()\n",
    "demo = gr.Interface(fn=captioner,\n",
    "                    inputs=[gr.Image(label=\"Upload image\", type=\"pil\")],\n",
    "                    outputs=[gr.Textbox(label=\"Caption\")],\n",
    "                    title=\"Image Captioning with BLIP\",\n",
    "                    description=\"Caption any image using the BLIP model\",\n",
    "                    allow_flagging=\"never\",\n",
    "                    examples=[\"christmas_dog.jpeg\", \"bird_flight.jpeg\", \"cow.jpeg\"])\n",
    "\n",
    "demo.launch(share=True, server_port=int(os.environ['PORT1']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "505950a7-c516-4dcc-bb50-a0628e7b57b3",
   "metadata": {
    "height": 30
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Closing server running on port: 7860\n"
     ]
    }
   ],
   "source": [
    "gr.close_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "43cf5022",
   "metadata": {
    "height": 30
   },
   "outputs": [
    {
     "ename": "HTTPError",
     "evalue": "404 Client Error: Not Found for url: https://api-inference.huggingface.co/models/nlpconnect/vit-gpt2-image-captioning",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mHTTPError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      3\u001b[39m headers = {\u001b[33m\"\u001b[39m\u001b[33mAccept\u001b[39m\u001b[33m\"\u001b[39m:\u001b[33m\"\u001b[39m\u001b[33mapplication/json\u001b[39m\u001b[33m\"\u001b[39m,\u001b[33m\"\u001b[39m\u001b[33mContent-Type\u001b[39m\u001b[33m\"\u001b[39m:\u001b[33m\"\u001b[39m\u001b[33mapplication/octet-stream\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m      4\u001b[39m            \u001b[33m\"\u001b[39m\u001b[33mAuthorization\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mBearer \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mos.environ[\u001b[33m'\u001b[39m\u001b[33mHF_API_TOKEN\u001b[39m\u001b[33m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m}\n\u001b[32m      5\u001b[39m r = requests.post(os.environ[\u001b[33m\"\u001b[39m\u001b[33mHF_API_ITT_BASE\u001b[39m\u001b[33m\"\u001b[39m], headers=headers, data=img, timeout=\u001b[32m60\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m \u001b[43mr\u001b[49m\u001b[43m.\u001b[49m\u001b[43mraise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[38;5;28mprint\u001b[39m(r.json())  \u001b[38;5;66;03m# typically: [{\"generated_text\": \"...\"}]\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/experimental/GenAIGradio/.venv/lib/python3.11/site-packages/requests/models.py:1026\u001b[39m, in \u001b[36mResponse.raise_for_status\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1021\u001b[39m     http_error_msg = (\n\u001b[32m   1022\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.status_code\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m Server Error: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mreason\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m for url: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.url\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m   1023\u001b[39m     )\n\u001b[32m   1025\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m http_error_msg:\n\u001b[32m-> \u001b[39m\u001b[32m1026\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m HTTPError(http_error_msg, response=\u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[31mHTTPError\u001b[39m: 404 Client Error: Not Found for url: https://api-inference.huggingface.co/models/nlpconnect/vit-gpt2-image-captioning"
     ]
    }
   ],
   "source": [
    "import os, requests\n",
    "img = requests.get(\"https://free-images.com/sm/9596/dog_animal_greyhound_983023.jpg\", timeout=30).content\n",
    "headers = {\"Accept\":\"application/json\",\"Content-Type\":\"application/octet-stream\",\n",
    "           \"Authorization\": f\"Bearer {os.environ['HF_API_TOKEN']}\"}\n",
    "r = requests.post(os.environ[\"HF_API_ITT_BASE\"], headers=headers, data=img, timeout=60)\n",
    "r.raise_for_status()\n",
    "print(r.json())  # typically: [{\"generated_text\": \"...\"}]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89a5d429-b26d-43fd-b521-0268af3bac76",
   "metadata": {},
   "source": [
    "## Quick local captioning (ViT-GPT2: small & reliable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d7b1b92e-8294-428c-ba56-042bbfe9751d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "03e3a228c1634433a096f044ae28c5a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ytchen/Documents/experimental/GenAIGradio/.venv/lib/python3.11/site-packages/transformers/models/auto/modeling_auto.py:2242: FutureWarning: The class `AutoModelForVision2Seq` is deprecated and will be removed in v5.0. Please use `AutoModelForImageTextToText` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "06f94f6b0bfe4bafb89a5fe7b131b4a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/982M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "29ddda87613640f5a520d9600bd80ead",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/982M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a131b09ce35e465e803a545508cb1c9b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/241 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e1e8e45d1b744d79b3cad3370ebc1602",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "963d8ee5361148d8beea034401a59b45",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e6c4aad43257453bbd48f1eea4f6ba78",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c23e753d47b34625880d5d6d87192f43",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/120 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e7153e76f9df422cb3cc974715c9b8c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "preprocessor_config.json:   0%|          | 0.00/228 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n",
      "Device set to use mps\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "We strongly recommend passing in an `attention_mask` since your input_ids may be padded. See https://huggingface.co/docs/transformers/troubleshooting#incorrect-output-when-padding-tokens-arent-masked.\n",
      "You may ignore this warning if your `pad_token_id` (50256) is identical to the `bos_token_id` (50256), `eos_token_id` (50256), or the `sep_token_id` (None), and your input is not padded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a dog wearing a red hat and a red bow tie \n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "pipe = pipeline(\"image-to-text\", model=\"nlpconnect/vit-gpt2-image-captioning\", device_map=\"auto\")\n",
    "\n",
    "img_url = \"https://free-images.com/sm/9596/dog_animal_greyhound_983023.jpg\"\n",
    "out = pipe(img_url, max_new_tokens=32)\n",
    "print(out[0][\"generated_text\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ad3b804c-a58a-4510-9668-8c9509eb6380",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://free-images.com/sm/9596/dog_animal_greyhound_983023.jpg\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(IPython.display.Image(url=img_url))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b99aa9a-5e7a-4169-a80b-3cb332bdcfef",
   "metadata": {},
   "source": [
    "## Fastest working captioner (ViT-GPT2) — minimal RAM, great on M-series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9d80ae26-4912-4b61-9120-5e7a2c43b4be",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<img src=\"https://free-images.com/sm/9596/dog_animal_greyhound_983023.jpg\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a dog wearing a red hat and a red bow tie \n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "import torch\n",
    "\n",
    "device = \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "\n",
    "# Simple + reliable\n",
    "pipe = pipeline(\n",
    "    \"image-to-text\",\n",
    "    model=\"nlpconnect/vit-gpt2-image-captioning\",\n",
    "    device=device,               # uses MPS on Apple Silicon\n",
    "    # model_kwargs={\"torch_dtype\": \"auto\"}  # optional\n",
    ")\n",
    "\n",
    "img_url = \"https://free-images.com/sm/9596/dog_animal_greyhound_983023.jpg\"\n",
    "display(IPython.display.Image(url=img_url))\n",
    "\n",
    "out = pipe(img_url, max_new_tokens=32)\n",
    "print(out[0][\"generated_text\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "217b0b92-2d01-4b2c-9085-deaac37f2d02",
   "metadata": {},
   "source": [
    "## Higher-quality captions (BLIP base) — use float16 on MPS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9f35bd93-24ec-44e7-856d-a2d6ab221497",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://free-images.com/sm/9596/dog_animal_greyhound_983023.jpg\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a dog wearing a santa hat and a red scarf\n"
     ]
    }
   ],
   "source": [
    "import io, requests, torch\n",
    "from PIL import Image\n",
    "from transformers import BlipProcessor, BlipForConditionalGeneration\n",
    "\n",
    "use_mps = torch.backends.mps.is_available()\n",
    "device = torch.device(\"mps\" if use_mps else \"cpu\")\n",
    "\n",
    "processor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
    "model = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
    "\n",
    "# MPS tip: half precision saves memory & speeds up on M-series\n",
    "if use_mps:\n",
    "    model = model.to(torch.float16)\n",
    "model = model.to(device).eval()\n",
    "\n",
    "img_url = \"https://free-images.com/sm/9596/dog_animal_greyhound_983023.jpg\"\n",
    "display(IPython.display.Image(url=img_url))\n",
    "\n",
    "img = Image.open(io.BytesIO(requests.get(img_url, timeout=30).content)).convert(\"RGB\")\n",
    "\n",
    "inputs = processor(images=img, return_tensors=\"pt\").to(device)\n",
    "if use_mps:\n",
    "    # keep input dtype consistent with model for MPS\n",
    "    inputs = {k: (v.half() if v.dtype == torch.float32 else v) for k, v in inputs.items()}\n",
    "\n",
    "with torch.inference_mode():\n",
    "    out_ids = model.generate(**inputs, max_new_tokens=32)\n",
    "caption = processor.decode(out_ids[0], skip_special_tokens=True)\n",
    "print(caption)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "586409f6-99c8-4d34-a649-9f6e89740227",
   "metadata": {},
   "source": [
    "## Tiny local Gradio app (runs on your Mac)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4c43cb1d-92ed-4b00-8dda-4cc921197f52",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ytchen/Documents/experimental/GenAIGradio/.venv/lib/python3.11/site-packages/transformers/models/auto/modeling_auto.py:2242: FutureWarning: The class `AutoModelForVision2Seq` is deprecated and will be removed in v5.0. Please use `AutoModelForImageTextToText` instead.\n",
      "  warnings.warn(\n",
      "Device set to use mps\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7860\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gradio as gr\n",
    "from transformers import pipeline\n",
    "import torch\n",
    "\n",
    "device = \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "pipe = pipeline(\"image-to-text\", model=\"nlpconnect/vit-gpt2-image-captioning\", device=device)\n",
    "\n",
    "def caption(img):\n",
    "    return pipe(img, max_new_tokens=32)[0][\"generated_text\"]\n",
    "\n",
    "demo = gr.Interface(fn=caption, inputs=gr.Image(type=\"pil\"), outputs=\"text\",\n",
    "                    title=\"Local Image Captioning (MPS)\")\n",
    "demo.launch()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dac66bd0-8144-45ae-ae2d-744f5189654d",
   "metadata": {},
   "source": [
    "## local first, no HF endpoint needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dd94aef6-046c-4254-bf7e-64eaa59ac112",
   "metadata": {},
   "outputs": [],
   "source": [
    "# local_itt.py\n",
    "import torch\n",
    "from transformers import pipeline\n",
    "\n",
    "_PIPE = None\n",
    "\n",
    "def get_completion_local(image, model_name=\"nlpconnect/vit-gpt2-image-captioning\", max_new_tokens=32):\n",
    "    global _PIPE\n",
    "    if _PIPE is None:\n",
    "        device = \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "        _PIPE = pipeline(\"image-to-text\", model=model_name, device=device)\n",
    "    out = _PIPE(image, max_new_tokens=max_new_tokens)\n",
    "    # standardize shape similar to HF serverless\n",
    "    return [{\"generated_text\": out[0][\"generated_text\"]}]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "70621203-c9fb-4ac9-8187-0b4d33fc4d6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Closing server running on port: 7860\n"
     ]
    }
   ],
   "source": [
    "gr.close_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcf49260-28c4-4257-8741-68bb57e6b1f2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (GRADIO)",
   "language": "python",
   "name": "gradio"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
