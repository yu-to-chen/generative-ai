{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6652a472",
   "metadata": {},
   "source": [
    "# L4: Describe-and-Generate game üñçÔ∏è"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b21aa74",
   "metadata": {},
   "source": [
    "Load your HF API key and relevant Python libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "92f16527-fcab-41e0-bce8-634fae58b1b8",
   "metadata": {
    "height": 166
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import io\n",
    "from IPython.display import Image, display, HTML\n",
    "from PIL import Image\n",
    "import base64 \n",
    "\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "_ = load_dotenv(find_dotenv()) # read local .env file\n",
    "hf_api_key = os.environ['HF_API_KEY']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ac9eab7f-4013-42ec-b34e-b413b0f8adb6",
   "metadata": {
    "height": 302
   },
   "outputs": [],
   "source": [
    "#### Helper function\n",
    "import requests, json\n",
    "\n",
    "#Here we are going to call multiple endpoints!\n",
    "def get_completion(inputs, parameters=None, ENDPOINT_URL=\"\"):\n",
    "    headers = {\n",
    "      \"Authorization\": f\"Bearer {hf_api_key}\",\n",
    "      \"Content-Type\": \"application/json\"\n",
    "    }   \n",
    "    data = { \"inputs\": inputs }\n",
    "    if parameters is not None:\n",
    "        data.update({\"parameters\": parameters})\n",
    "    response = requests.request(\"POST\",\n",
    "                                ENDPOINT_URL,\n",
    "                                headers=headers,\n",
    "                                data=json.dumps(data))\n",
    "    return json.loads(response.content.decode(\"utf-8\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "52cb1837",
   "metadata": {
    "height": 81
   },
   "outputs": [],
   "source": [
    "#text-to-image\n",
    "TTI_ENDPOINT = os.environ['HF_API_TTI_BASE']\n",
    "#image-to-text\n",
    "ITT_ENDPOINT = os.environ['HF_API_ITT_BASE']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3f9c4d4",
   "metadata": {},
   "source": [
    "## Building your game with `gr.Blocks()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "76170d93-0de0-4359-8f13-3d1bcc41f86a",
   "metadata": {
    "height": 387
   },
   "outputs": [],
   "source": [
    "#Bringing the functions from lessons 3 and 4!\n",
    "def image_to_base64_str(pil_image):\n",
    "    byte_arr = io.BytesIO()\n",
    "    pil_image.save(byte_arr, format='PNG')\n",
    "    byte_arr = byte_arr.getvalue()\n",
    "    return str(base64.b64encode(byte_arr).decode('utf-8'))\n",
    "\n",
    "def base64_to_pil(img_base64):\n",
    "    base64_decoded = base64.b64decode(img_base64)\n",
    "    byte_stream = io.BytesIO(base64_decoded)\n",
    "    pil_image = Image.open(byte_stream)\n",
    "    return pil_image\n",
    "\n",
    "def captioner(image):\n",
    "    base64_image = image_to_base64_str(image)\n",
    "    result = get_completion(base64_image, None, ITT_ENDPOINT)\n",
    "    return result[0]['generated_text']\n",
    "\n",
    "def generate(prompt):\n",
    "    output = get_completion(prompt, None, TTI_ENDPOINT)\n",
    "    result_image = base64_to_pil(output)\n",
    "    return result_image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd1f3485",
   "metadata": {},
   "source": [
    "### First attempt, just captioning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "64e403a9-bfcb-47e8-b741-743d5f4980fd",
   "metadata": {
    "height": 200
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nimport gradio as gr \\nwith gr.Blocks() as demo:\\n    gr.Markdown(\"# Describe-and-Generate game üñçÔ∏è\")\\n    image_upload = gr.Image(label=\"Your first image\",type=\"pil\")\\n    btn_caption = gr.Button(\"Generate caption\")\\n    caption = gr.Textbox(label=\"Generated caption\")\\n\\n    btn_caption.click(fn=captioner, inputs=[image_upload], outputs=[caption])\\n\\ngr.close_all()\\ndemo.launch(share=True, server_port=int(os.environ[\\'PORT1\\']))\\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "import gradio as gr \n",
    "with gr.Blocks() as demo:\n",
    "    gr.Markdown(\"# Describe-and-Generate game üñçÔ∏è\")\n",
    "    image_upload = gr.Image(label=\"Your first image\",type=\"pil\")\n",
    "    btn_caption = gr.Button(\"Generate caption\")\n",
    "    caption = gr.Textbox(label=\"Generated caption\")\n",
    "    \n",
    "    btn_caption.click(fn=captioner, inputs=[image_upload], outputs=[caption])\n",
    "\n",
    "gr.close_all()\n",
    "demo.launch(share=True, server_port=int(os.environ['PORT1']))\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "85c1113c-853f-4d49-ba69-9ced8d3092cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Closing server running on port: 7860\n",
      "* Running on local URL:  http://127.0.0.1:7860\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ytchen/Documents/experimental/GenAIGradio/.venv/lib/python3.11/site-packages/transformers/models/auto/modeling_auto.py:2242: FutureWarning: The class `AutoModelForVision2Seq` is deprecated and will be removed in v5.0. Please use `AutoModelForImageTextToText` instead.\n",
      "  warnings.warn(\n",
      "Device set to use mps\n"
     ]
    }
   ],
   "source": [
    "# one-time (in your venv with uv):\n",
    "# uv pip install \"torch>=2.2\" torchvision accelerate transformers pillow gradio\n",
    "\n",
    "import os, torch, gradio as gr\n",
    "from transformers import pipeline\n",
    "\n",
    "# Use MPS on Apple silicon; fall back to CPU if unavailable\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "os.environ.setdefault(\"PYTORCH_ENABLE_MPS_FALLBACK\", \"1\")  # safer on MPS\n",
    "\n",
    "# Lazy-load a single global pipeline (fast on subsequent calls)\n",
    "_CAPTION_PIPE = None\n",
    "def get_pipe():\n",
    "    global _CAPTION_PIPE\n",
    "    if _CAPTION_PIPE is None:\n",
    "        try:\n",
    "            # Small & fast; great default for local captioning\n",
    "            _CAPTION_PIPE = pipeline(\n",
    "                \"image-to-text\",\n",
    "                model=\"nlpconnect/vit-gpt2-image-captioning\",\n",
    "                device=device,                # send to MPS/CPU\n",
    "            )\n",
    "        except TypeError:\n",
    "            # Older Transformers may not accept torch.device directly\n",
    "            _CAPTION_PIPE = pipeline(\n",
    "                \"image-to-text\",\n",
    "                model=\"nlpconnect/vit-gpt2-image-captioning\",\n",
    "                device=0 if device.type == \"cuda\" else -1\n",
    "            ).to(device)\n",
    "    return _CAPTION_PIPE\n",
    "\n",
    "def captioner(pil_image):\n",
    "    if pil_image is None:\n",
    "        return \"Please upload an image.\"\n",
    "    pipe = get_pipe()\n",
    "    with torch.inference_mode():\n",
    "        out = pipe(pil_image, max_new_tokens=24)   # small cap for speed\n",
    "    return out[0][\"generated_text\"]\n",
    "\n",
    "with gr.Blocks() as demo:\n",
    "    gr.Markdown(\"# Describe-and-Generate game üñçÔ∏è (Local, Fast)\")\n",
    "\n",
    "    with gr.Row():\n",
    "        with gr.Column(scale=4):\n",
    "            image_upload = gr.Image(label=\"Your first image\", type=\"pil\")\n",
    "        with gr.Column(scale=1, min_width=50):\n",
    "            btn_caption = gr.Button(\"Generate caption\")\n",
    "\n",
    "    caption = gr.Textbox(label=\"Generated caption\")\n",
    "\n",
    "    btn_caption.click(fn=captioner, inputs=[image_upload], outputs=[caption])\n",
    "\n",
    "gr.close_all()\n",
    "demo.launch(share=False, server_port=int(os.environ.get('PORT1', '7860')))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "966cd4e8",
   "metadata": {},
   "source": [
    "### Let's add generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c5ea9a93-9f97-43f8-86ca-2e8e4ea9dda8",
   "metadata": {
    "height": 217
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nwith gr.Blocks() as demo:\\n    gr.Markdown(\"# Describe-and-Generate game üñçÔ∏è\")\\n    image_upload = gr.Image(label=\"Your first image\",type=\"pil\")\\n    btn_caption = gr.Button(\"Generate caption\")\\n    caption = gr.Textbox(label=\"Generated caption\")\\n    btn_image = gr.Button(\"Generate image\")\\n    image_output = gr.Image(label=\"Generated Image\")\\n    btn_caption.click(fn=captioner, inputs=[image_upload], outputs=[caption])\\n    btn_image.click(fn=generate, inputs=[caption], outputs=[image_output])\\n\\ngr.close_all()\\ndemo.launch(share=True, server_port=int(os.environ[\\'PORT2\\']))\\n'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "with gr.Blocks() as demo:\n",
    "    gr.Markdown(\"# Describe-and-Generate game üñçÔ∏è\")\n",
    "    image_upload = gr.Image(label=\"Your first image\",type=\"pil\")\n",
    "    btn_caption = gr.Button(\"Generate caption\")\n",
    "    caption = gr.Textbox(label=\"Generated caption\")\n",
    "    btn_image = gr.Button(\"Generate image\")\n",
    "    image_output = gr.Image(label=\"Generated Image\")\n",
    "    btn_caption.click(fn=captioner, inputs=[image_upload], outputs=[caption])\n",
    "    btn_image.click(fn=generate, inputs=[caption], outputs=[image_output])\n",
    "\n",
    "gr.close_all()\n",
    "demo.launch(share=True, server_port=int(os.environ['PORT2']))\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "84417848-3c8a-47ee-8d51-6d3fe64b04ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Closing server running on port: 7860\n",
      "* Running on local URL:  http://127.0.0.1:7861\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7861/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ytchen/Documents/experimental/GenAIGradio/.venv/lib/python3.11/site-packages/transformers/models/auto/modeling_auto.py:2242: FutureWarning: The class `AutoModelForVision2Seq` is deprecated and will be removed in v5.0. Please use `AutoModelForImageTextToText` instead.\n",
      "  warnings.warn(\n",
      "Device set to use mps:0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3151634ba3ad4b31a3ece56e34fd966c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading pipeline components...:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    }
   ],
   "source": [
    "# one-time installs (inside your venv with uv):\n",
    "# uv pip install \"torch>=2.2\" torchvision accelerate transformers diffusers safetensors pillow gradio\n",
    "\n",
    "import os, torch, gradio as gr\n",
    "from transformers import pipeline\n",
    "from diffusers import AutoPipelineForText2Image\n",
    "from PIL import Image\n",
    "\n",
    "# ---- MPS-friendly defaults for Apple silicon ----\n",
    "os.environ.setdefault(\"PYTORCH_ENABLE_MPS_FALLBACK\", \"1\")\n",
    "os.environ.setdefault(\"PYTORCH_MPS_HIGH_WATERMARK_RATIO\", \"0.0\")\n",
    "\n",
    "DEVICE = \"mps\" if torch.backends.mps.is_available() else (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "DTYPE  = torch.float32 if DEVICE == \"mps\" else (torch.float16 if DEVICE == \"cuda\" else torch.float32)\n",
    "\n",
    "# ---- Lazy-load local captioner (small & fast) ----\n",
    "_CAPTION_PIPE = None\n",
    "def _get_caption_pipe():\n",
    "    global _CAPTION_PIPE\n",
    "    if _CAPTION_PIPE is None:\n",
    "        _CAPTION_PIPE = pipeline(\"image-to-text\", model=\"nlpconnect/vit-gpt2-image-captioning\")\n",
    "        # move to MPS/CUDA when possible\n",
    "        try:\n",
    "            if DEVICE != \"cpu\":\n",
    "                _CAPTION_PIPE.model.to(DEVICE)\n",
    "        except Exception:\n",
    "            pass\n",
    "    return _CAPTION_PIPE\n",
    "\n",
    "# ---- Lazy-load local T2I (SDXL-Turbo = super fast drafts) ----\n",
    "_TTI_PIPE = None\n",
    "def _get_tti_pipe():\n",
    "    global _TTI_PIPE\n",
    "    if _TTI_PIPE is None:\n",
    "        _TTI_PIPE = AutoPipelineForText2Image.from_pretrained(\n",
    "            \"stabilityai/sdxl-turbo\",\n",
    "            torch_dtype=DTYPE,              # keep f32 on MPS to avoid NaNs/black frames\n",
    "            # variant=\"fp16\"  # (only if you force CUDA fp16; skip on MPS)\n",
    "        ).to(DEVICE)\n",
    "        if hasattr(_TTI_PIPE, \"set_progress_bar_config\"):\n",
    "            _TTI_PIPE.set_progress_bar_config(disable=True)\n",
    "        if hasattr(_TTI_PIPE, \"enable_attention_slicing\"):\n",
    "            _TTI_PIPE.enable_attention_slicing()\n",
    "        if hasattr(_TTI_PIPE, \"enable_vae_tiling\"):\n",
    "            _TTI_PIPE.enable_vae_tiling()\n",
    "    return _TTI_PIPE\n",
    "\n",
    "def _snap8(x: int) -> int:\n",
    "    return int(max(8, round(int(x) / 8) * 8))\n",
    "\n",
    "# ---- Handlers ----\n",
    "def captioner(pil_image: Image.Image):\n",
    "    if pil_image is None:\n",
    "        return \"Please upload an image.\"\n",
    "    pipe = _get_caption_pipe()\n",
    "    with torch.inference_mode():\n",
    "        out = pipe(pil_image, max_new_tokens=24)  # short = fast\n",
    "    return out[0][\"generated_text\"]\n",
    "\n",
    "def generate(prompt: str):\n",
    "    if not prompt or not prompt.strip():\n",
    "        raise gr.Error(\"No prompt. Use the generated caption or type your own.\")\n",
    "    pipe = _get_tti_pipe()\n",
    "    with torch.inference_mode():\n",
    "        img = pipe(\n",
    "            prompt.strip(),\n",
    "            num_inference_steps=2,   # SDXL-Turbo sweet spot: 1‚Äì4\n",
    "            guidance_scale=0.0,      # works best at 0.0\n",
    "            width=_snap8(512),\n",
    "            height=_snap8(512),\n",
    "        ).images[0]\n",
    "    return img\n",
    "\n",
    "# ---- Your UI (unchanged layout) ----\n",
    "with gr.Blocks() as demo:\n",
    "    gr.Markdown(\"# Describe-and-Generate game üñçÔ∏è\")\n",
    "    image_upload = gr.Image(label=\"Your first image\", type=\"pil\")\n",
    "    btn_caption = gr.Button(\"Generate caption\")\n",
    "    caption = gr.Textbox(label=\"Generated caption\")\n",
    "    btn_image = gr.Button(\"Generate image\")\n",
    "    image_output = gr.Image(label=\"Generated Image\", type=\"pil\")\n",
    "\n",
    "    btn_caption.click(fn=captioner, inputs=[image_upload], outputs=[caption])\n",
    "    btn_image.click(fn=generate, inputs=[caption], outputs=[image_output])\n",
    "\n",
    "gr.close_all()\n",
    "# Let Gradio pick a free port for reliability/speed\n",
    "_port = os.environ.get(\"PORT2\")\n",
    "if _port:\n",
    "    demo.launch(share=False, server_port=int(_port))\n",
    "else:\n",
    "    demo.launch(share=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22da53d1",
   "metadata": {},
   "source": [
    "### Doing it all at once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "024a4529-7f38-41fb-aab3-c38525b61abe",
   "metadata": {
    "height": 285
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndef caption_and_generate(image):\\n    caption = captioner(image)\\n    image = generate(caption)\\n    return [caption, image]\\n\\nwith gr.Blocks() as demo:\\n    gr.Markdown(\"# Describe-and-Generate game üñçÔ∏è\")\\n    image_upload = gr.Image(label=\"Your first image\",type=\"pil\")\\n    btn_all = gr.Button(\"Caption and generate\")\\n    caption = gr.Textbox(label=\"Generated caption\")\\n    image_output = gr.Image(label=\"Generated Image\")\\n\\n    btn_all.click(fn=caption_and_generate, inputs=[image_upload], outputs=[caption, image_output])\\n\\ngr.close_all()\\ndemo.launch(share=True, server_port=int(os.environ[\\'PORT3\\']))\\n'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "def caption_and_generate(image):\n",
    "    caption = captioner(image)\n",
    "    image = generate(caption)\n",
    "    return [caption, image]\n",
    "\n",
    "with gr.Blocks() as demo:\n",
    "    gr.Markdown(\"# Describe-and-Generate game üñçÔ∏è\")\n",
    "    image_upload = gr.Image(label=\"Your first image\",type=\"pil\")\n",
    "    btn_all = gr.Button(\"Caption and generate\")\n",
    "    caption = gr.Textbox(label=\"Generated caption\")\n",
    "    image_output = gr.Image(label=\"Generated Image\")\n",
    "\n",
    "    btn_all.click(fn=caption_and_generate, inputs=[image_upload], outputs=[caption, image_output])\n",
    "\n",
    "gr.close_all()\n",
    "demo.launch(share=True, server_port=int(os.environ['PORT3']))\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7e5b5202-752c-448f-805d-deebf2506c93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Closing server running on port: 7861\n",
      "* Running on local URL:  http://127.0.0.1:7862\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7862/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ytchen/Documents/experimental/GenAIGradio/.venv/lib/python3.11/site-packages/transformers/models/auto/modeling_auto.py:2242: FutureWarning: The class `AutoModelForVision2Seq` is deprecated and will be removed in v5.0. Please use `AutoModelForImageTextToText` instead.\n",
      "  warnings.warn(\n",
      "Device set to use mps:0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02902b4282d949a9bb4546209bc951f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading pipeline components...:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# one-time installs (inside your venv with uv):\n",
    "# uv pip install \"torch>=2.2\" torchvision accelerate transformers diffusers safetensors pillow gradio\n",
    "\n",
    "import os, torch, gradio as gr\n",
    "from transformers import pipeline\n",
    "from diffusers import AutoPipelineForText2Image\n",
    "from PIL import Image\n",
    "\n",
    "# --- MPS-friendly env ---\n",
    "os.environ.setdefault(\"PYTORCH_ENABLE_MPS_FALLBACK\", \"1\")\n",
    "os.environ.setdefault(\"PYTORCH_MPS_HIGH_WATERMARK_RATIO\", \"0.0\")\n",
    "\n",
    "DEVICE = \"mps\" if torch.backends.mps.is_available() else (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "DTYPE  = torch.float32 if DEVICE == \"mps\" else (torch.float16 if DEVICE == \"cuda\" else torch.float32)\n",
    "\n",
    "# --- Lazy global pipelines (load once, reuse = fast) ---\n",
    "_CAPTION_PIPE = None\n",
    "_TTI_PIPE = None\n",
    "\n",
    "def _get_caption_pipe():\n",
    "    global _CAPTION_PIPE\n",
    "    if _CAPTION_PIPE is None:\n",
    "        _CAPTION_PIPE = pipeline(\"image-to-text\", model=\"nlpconnect/vit-gpt2-image-captioning\")\n",
    "        try:\n",
    "            if DEVICE != \"cpu\":\n",
    "                _CAPTION_PIPE.model.to(DEVICE)\n",
    "        except Exception:\n",
    "            pass\n",
    "    return _CAPTION_PIPE\n",
    "\n",
    "def _get_tti_pipe():\n",
    "    global _TTI_PIPE\n",
    "    if _TTI_PIPE is None:\n",
    "        _TTI_PIPE = AutoPipelineForText2Image.from_pretrained(\n",
    "            \"stabilityai/sdxl-turbo\",\n",
    "            torch_dtype=DTYPE,            # keep f32 on MPS for stability\n",
    "        ).to(DEVICE)\n",
    "        if hasattr(_TTI_PIPE, \"set_progress_bar_config\"): _TTI_PIPE.set_progress_bar_config(disable=True)\n",
    "        if hasattr(_TTI_PIPE, \"enable_attention_slicing\"): _TTI_PIPE.enable_attention_slicing()\n",
    "        if hasattr(_TTI_PIPE, \"enable_vae_tiling\"): _TTI_PIPE.enable_vae_tiling()\n",
    "    return _TTI_PIPE\n",
    "\n",
    "def captioner(pil_image: Image.Image) -> str:\n",
    "    if pil_image is None:\n",
    "        return \"Please upload an image.\"\n",
    "    pipe = _get_caption_pipe()\n",
    "    with torch.inference_mode():\n",
    "        out = pipe(pil_image, max_new_tokens=24)  # short = fast\n",
    "    # typical: [{'generated_text': '...'}]\n",
    "    return out[0].get(\"generated_text\", str(out[0]))\n",
    "\n",
    "def generate(prompt: str) -> Image.Image:\n",
    "    if not prompt or not prompt.strip():\n",
    "        raise gr.Error(\"No prompt. Use the generated caption or type your own.\")\n",
    "    pipe = _get_tti_pipe()\n",
    "    with torch.inference_mode():\n",
    "        img = pipe(\n",
    "            prompt.strip(),\n",
    "            num_inference_steps=2,   # SDXL-Turbo sweet spot: 1‚Äì4\n",
    "            guidance_scale=0.0,      # SDXL-Turbo works best at 0.0\n",
    "            width=512, height=512,   # fast and stable; feel free to change\n",
    "        ).images[0]\n",
    "    return img\n",
    "\n",
    "def caption_and_generate(image: Image.Image):\n",
    "    cap = captioner(image)\n",
    "    img = generate(cap)\n",
    "    return [cap, img]\n",
    "\n",
    "# ---------------- UI ----------------\n",
    "with gr.Blocks() as demo:\n",
    "    gr.Markdown(\"# Describe-and-Generate game üñçÔ∏è (Local & Fast)\")\n",
    "    image_upload = gr.Image(label=\"Your first image\", type=\"pil\")\n",
    "    btn_all = gr.Button(\"Caption and generate\", variant=\"primary\")\n",
    "    caption = gr.Textbox(label=\"Generated caption\")\n",
    "    image_output = gr.Image(label=\"Generated Image\", type=\"pil\")\n",
    "\n",
    "    btn_all.click(fn=caption_and_generate, inputs=[image_upload], outputs=[caption, image_output])\n",
    "\n",
    "gr.close_all()\n",
    "# Let Gradio auto-pick a free port; use PORT3 if you really want a fixed port\n",
    "_port = os.environ.get(\"PORT3\")\n",
    "demo.launch(share=False, server_port=int(_port) if _port else None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ba8bd128-fea1-4c92-8363-cdfba40aed8e",
   "metadata": {
    "height": 30
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Closing server running on port: 7862\n"
     ]
    }
   ],
   "source": [
    "gr.close_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dafaaa3-e48a-4625-a324-934fc37eaa58",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (GRADIO)",
   "language": "python",
   "name": "gradio"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
