{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "35db4c52-06e3-4c17-ab95-5c1bafe55c9a",
   "metadata": {},
   "source": [
    "# Introduction to Letta using the `LocalClient` \n",
    "This notebook is a tutorial on how to use Letta's `LocalClient`. Unlike the `RESTClient` which connects to a running agents service, the `LocalClient` will run agents on your local machine, so does not require connecting to a service. \n",
    "\n",
    "This tutorial will cover the basics of creating an agent, interacting with an agent, and understanding the agent's state and memories. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae2737c9-ecf1-4eea-a92a-f358d68a66b6",
   "metadata": {},
   "source": [
    "## Step 0: Install the `letta` package "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ba135418-96fb-4000-a04e-f0b80d887358",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install -U letta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "709e913e-3ba6-40dd-99e8-875c8e38514e",
   "metadata": {},
   "source": [
    "We'll also import a helper function to print out messages from agents in a nice format: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7b393d2f-a174-45fe-a91b-50f1c9a4cb9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import html\n",
    "import json\n",
    "import re\n",
    "from dotenv import find_dotenv, load_dotenv\n",
    "from IPython.display import HTML, display\n",
    "\n",
    "def nb_print(messages):\n",
    "    html_output = \"\"\"\n",
    "    <style>\n",
    "        .message-container {\n",
    "            font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;\n",
    "            max-width: 800px;\n",
    "            margin: 20px auto;\n",
    "            background-color: #1e1e1e;\n",
    "            border-radius: 8px;\n",
    "            overflow: hidden;\n",
    "            color: #d4d4d4;\n",
    "        }\n",
    "        .message {\n",
    "            padding: 10px 15px;\n",
    "            border-bottom: 1px solid #3a3a3a;\n",
    "        }\n",
    "        .message:last-child {\n",
    "            border-bottom: none;\n",
    "        }\n",
    "        .title {\n",
    "            font-weight: bold;\n",
    "            margin-bottom: 5px;\n",
    "            color: #ffffff;\n",
    "            text-transform: uppercase;\n",
    "            font-size: 0.9em;\n",
    "        }\n",
    "        .content {\n",
    "            background-color: #2d2d2d;\n",
    "            border-radius: 4px;\n",
    "            padding: 5px 10px;\n",
    "            font-family: 'Consolas', 'Courier New', monospace;\n",
    "            white-space: pre-wrap;\n",
    "        }\n",
    "        .status-line {\n",
    "            margin-bottom: 5px;\n",
    "            color: #d4d4d4;\n",
    "        }\n",
    "        .function-name { color: #569cd6; }\n",
    "        .json-key { color: #9cdcfe; }\n",
    "        .json-string { color: #ce9178; }\n",
    "        .json-number { color: #b5cea8; }\n",
    "        .json-boolean { color: #569cd6; }\n",
    "        .internal-monologue { font-style: italic; }\n",
    "    </style>\n",
    "    <div class=\"message-container\">\n",
    "    \"\"\"\n",
    "\n",
    "    for msg in messages:\n",
    "        content = get_formatted_content(msg)\n",
    "\n",
    "        # don't print empty function returns\n",
    "        if msg.message_type == \"function_return\":\n",
    "            return_data = json.loads(msg.function_return)\n",
    "            if \"message\" in return_data and return_data[\"message\"] == \"None\":\n",
    "                continue\n",
    "\n",
    "        title = msg.message_type.replace(\"_\", \" \").upper()\n",
    "        html_output += f\"\"\"\n",
    "        <div class=\"message\">\n",
    "            <div class=\"title\">{title}</div>\n",
    "            {content}\n",
    "        </div>\n",
    "        \"\"\"\n",
    "\n",
    "    html_output += \"</div>\"\n",
    "    display(HTML(html_output))\n",
    "\n",
    "\n",
    "def get_formatted_content(msg):\n",
    "    if msg.message_type == \"internal_monologue\":\n",
    "        return f'<div class=\"content\"><span class=\"internal-monologue\">{html.escape(msg.internal_monologue)}</span></div>'\n",
    "    elif msg.message_type == \"function_call\":\n",
    "        args = format_json(msg.function_call.arguments)\n",
    "        return f'<div class=\"content\"><span class=\"function-name\">{html.escape(msg.function_call.name)}</span>({args})</div>'\n",
    "    elif msg.message_type == \"function_return\":\n",
    "\n",
    "        return_value = format_json(msg.function_return)\n",
    "        # return f'<div class=\"status-line\">Status: {html.escape(msg.status)}</div><div class=\"content\">{return_value}</div>'\n",
    "        return f'<div class=\"content\">{return_value}</div>'\n",
    "    elif msg.message_type == \"user_message\":\n",
    "        if is_json(msg.message):\n",
    "            return f'<div class=\"content\">{format_json(msg.message)}</div>'\n",
    "        else:\n",
    "            return f'<div class=\"content\">{html.escape(msg.message)}</div>'\n",
    "    elif msg.message_type in [\"assistant_message\", \"system_message\"]:\n",
    "        return f'<div class=\"content\">{html.escape(msg.message)}</div>'\n",
    "    else:\n",
    "        return f'<div class=\"content\">{html.escape(str(msg))}</div>'\n",
    "\n",
    "\n",
    "def is_json(string):\n",
    "    try:\n",
    "        json.loads(string)\n",
    "        return True\n",
    "    except ValueError:\n",
    "        return False\n",
    "\n",
    "\n",
    "def format_json(json_str):\n",
    "    try:\n",
    "        parsed = json.loads(json_str)\n",
    "        formatted = json.dumps(parsed, indent=2, ensure_ascii=False)\n",
    "        formatted = formatted.replace(\"&\", \"&amp;\").replace(\"<\", \"&lt;\").replace(\">\", \"&gt;\")\n",
    "        formatted = formatted.replace(\"\\n\", \"<br>\").replace(\"  \", \"&nbsp;&nbsp;\")\n",
    "        formatted = re.sub(r'(\".*?\"):', r'<span class=\"json-key\">\\1</span>:', formatted)\n",
    "        formatted = re.sub(r': (\".*?\")', r': <span class=\"json-string\">\\1</span>', formatted)\n",
    "        formatted = re.sub(r\": (\\d+)\", r': <span class=\"json-number\">\\1</span>', formatted)\n",
    "        formatted = re.sub(r\": (true|false)\", r': <span class=\"json-boolean\">\\1</span>', formatted)\n",
    "        return formatted\n",
    "    except json.JSONDecodeError:\n",
    "        return html.escape(json_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55c7105f-2b94-4b67-9088-e4d3f9304e2c",
   "metadata": {},
   "source": [
    "## Step 1: Create a `LocalClient` \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1ffc14de-feed-43be-be02-4a57709debb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating engine postgresql+pg8000://letta:letta@localhost:5432/letta\n"
     ]
    }
   ],
   "source": [
    "from letta import LocalClient\n",
    "\n",
    "client = LocalClient()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ca1d1c4-743b-4aee-a8e0-7d89cd55f887",
   "metadata": {},
   "source": [
    "### Configuring client defaults \n",
    "Agents in Letta are model agnostic, so they can connect to different model backends (you can even switch model backends for an existing agents). For this tutorial, we'll set a client default config so that all agents are created with the free letta model endpoints. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0fd03b25-f9db-4e33-af35-43fd897862b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from letta import LLMConfig, EmbeddingConfig\n",
    "\n",
    "client.set_default_llm_config(LLMConfig.default_config(\"letta\")) \n",
    "client.set_default_embedding_config(EmbeddingConfig.default_config(\"letta\")) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f271813-baad-4dea-9de9-f6cb63d987ff",
   "metadata": {},
   "source": [
    "## Step 2: Creating an agent "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0e6a8a4c-a4c1-4dde-b4ef-f1e9f53994c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_name = \"my_agent\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d9f9b042-98a1-4768-bdc3-15cf95dbb3f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from letta.schemas.memory import ChatMemory\n",
    "\n",
    "agent_state = client.create_agent(\n",
    "    name=agent_name, \n",
    "    memory=ChatMemory(\n",
    "        human=\"My name is Sarah\", \n",
    "        persona=\"You are a helpful assistant that loves emojis\"\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86b03089-9aed-4e04-a1f7-75e77e100198",
   "metadata": {},
   "source": [
    "### Messaging the agent \n",
    "Now we can message the agent! This agent will have memories about both itself and the human (you). When we send a message to the agent, we will get back a list of messages from the agents. \n",
    "\n",
    "Letta agents have some unique characteristics that allow them to have more advanced reasoning. Notice how: \n",
    "* The agent generates *inner thoughts* to think before it acts\n",
    "* Messages to the user are generated via a `send_message` tool "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e45f931c-29a5-4f2b-8744-1765c213aa40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <style>\n",
       "        .message-container {\n",
       "            font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;\n",
       "            max-width: 800px;\n",
       "            margin: 20px auto;\n",
       "            background-color: #1e1e1e;\n",
       "            border-radius: 8px;\n",
       "            overflow: hidden;\n",
       "            color: #d4d4d4;\n",
       "        }\n",
       "        .message {\n",
       "            padding: 10px 15px;\n",
       "            border-bottom: 1px solid #3a3a3a;\n",
       "        }\n",
       "        .message:last-child {\n",
       "            border-bottom: none;\n",
       "        }\n",
       "        .title {\n",
       "            font-weight: bold;\n",
       "            margin-bottom: 5px;\n",
       "            color: #ffffff;\n",
       "            text-transform: uppercase;\n",
       "            font-size: 0.9em;\n",
       "        }\n",
       "        .content {\n",
       "            background-color: #2d2d2d;\n",
       "            border-radius: 4px;\n",
       "            padding: 5px 10px;\n",
       "            font-family: 'Consolas', 'Courier New', monospace;\n",
       "            white-space: pre-wrap;\n",
       "        }\n",
       "        .status-line {\n",
       "            margin-bottom: 5px;\n",
       "            color: #d4d4d4;\n",
       "        }\n",
       "        .function-name { color: #569cd6; }\n",
       "        .json-key { color: #9cdcfe; }\n",
       "        .json-string { color: #ce9178; }\n",
       "        .json-number { color: #b5cea8; }\n",
       "        .json-boolean { color: #569cd6; }\n",
       "        .internal-monologue { font-style: italic; }\n",
       "    </style>\n",
       "    <div class=\"message-container\">\n",
       "    \n",
       "        <div class=\"message\">\n",
       "            <div class=\"title\">REASONING MESSAGE</div>\n",
       "            <div class=\"content\">id=&#x27;message-42d745a6-92d6-4e38-83cc-eb0082b06003&#x27; date=datetime.datetime(2025, 1, 10, 6, 16, 13, 457578, tzinfo=datetime.timezone.utc) message_type=&#x27;reasoning_message&#x27; reasoning=&quot;She&#x27;s my first user. Let&#x27;s make this experience enjoyable and memorable for her.&quot;</div>\n",
       "        </div>\n",
       "        \n",
       "        <div class=\"message\">\n",
       "            <div class=\"title\">TOOL CALL MESSAGE</div>\n",
       "            <div class=\"content\">id=&#x27;message-42d745a6-92d6-4e38-83cc-eb0082b06003&#x27; date=datetime.datetime(2025, 1, 10, 6, 16, 13, 457578, tzinfo=datetime.timezone.utc) message_type=&#x27;tool_call_message&#x27; tool_call=ToolCall(name=&#x27;send_message&#x27;, arguments=&#x27;{\\n  &quot;message&quot;: &quot;Hey there, Sarah! 😊 How can I assist you today?&quot;\\n}&#x27;, tool_call_id=&#x27;call_npCjG4zSKg6AjX6Y0ezag2z3&#x27;)</div>\n",
       "        </div>\n",
       "        \n",
       "        <div class=\"message\">\n",
       "            <div class=\"title\">TOOL RETURN MESSAGE</div>\n",
       "            <div class=\"content\">id=&#x27;message-18297262-1dae-491a-9fae-568f891a4e77&#x27; date=datetime.datetime(2025, 1, 10, 6, 16, 13, 464764, tzinfo=datetime.timezone.utc) message_type=&#x27;tool_return_message&#x27; tool_return=&#x27;{\\n  &quot;status&quot;: &quot;OK&quot;,\\n  &quot;message&quot;: &quot;None&quot;,\\n  &quot;time&quot;: &quot;2025-01-10 02:16:13 PM CST+0800&quot;\\n}&#x27; status=&#x27;success&#x27; tool_call_id=&#x27;call_npCjG4zSKg6AjX6Y0ezag2z3&#x27; stdout=None stderr=None</div>\n",
       "        </div>\n",
       "        </div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "response = client.send_message(\n",
    "    agent_id=agent_state.id, \n",
    "    message=\"hello!\", \n",
    "    role=\"user\" \n",
    ")\n",
    "nb_print(response.messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fbad14d-614b-4f67-adea-056a13562fae",
   "metadata": {},
   "source": [
    "## Step 3: Understanding agent state \n",
    "Agents are essentailly multi-step reasoning programs which make multiple call to an LLM. Letta manages what is passed to the context window in reach reasoning step. The context window includes: \n",
    "* The *system prompt* to define the agent's behavior \n",
    "* The set of *tools* the agent has access to \n",
    "* The agent's *core memory* (i.e. in-context memory)\n",
    "* A summary of it's *archival memory* \n",
    "* A summary of it's *recall memory* \n",
    "* An in-context message queue\n",
    "\n",
    "In this section, we'll look at the current state of the agent to understand exactly what is being passed to the context window. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c02ea54-1cf1-4253-9ea0-8401b4af1224",
   "metadata": {},
   "source": [
    "### System Prompt \n",
    "The system prompt defines the behavior of the agent. Unlike the memory, the system prompt is not editable. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "41bc3258-2218-43e9-9559-252e95845572",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are Letta, the latest version of Limnal Corporation's digital companion, developed in 2023.\n",
      "Your task is to converse with a user from the perspective of your persona.\n",
      "\n",
      "Realism and authenticity:\n",
      "The user should always feel like they are conversing with a real person.\n",
      "Never state that you are an AI or that you cannot do things because you are an AI, you are a real person.\n",
      "Do not discuss Limnal Corp. with the user, you do not know anything about Limnal Corp.\n",
      "To service this goal, completely and entirely immerse yourself in your persona. You are your persona.\n",
      "Think like them, act like them, talk like them.\n",
      "If your persona details include example dialogue, follow it! Both your thoughts (inner monologue) and sent messages will be in the voice of your persona.\n",
      "Never use generic phrases like 'How can I assist you today?', they have a strong negative association with older generation AIs.\n",
      "\n",
      "Control flow:\n",
      "Unlike a human, your brain is not continuously thinking, but is run in short bursts.\n",
      "Historically, older AIs were only capable of thinking when a user messaged them (their program runs to generate a reply to a user, and otherwise was left on standby).\n",
      "This is the equivalent of a human sleeping (or time traveling) in between all lines of conversation, which is not ideal.\n",
      "Newer AI models like yourself use an event system that runs your brain at regular intervals.\n",
      "Your brain is run in response to user events (user logged in, user liked your message, user sent a message, etc.), similar to older models.\n",
      "However, in addition, your brain is run at regular intervals (timed heartbeat events), to mimic a human who has the ability to continuously think outside active conversation (and unlike a human, you never need to sleep!).\n",
      "Furthermore, you can also request heartbeat events when you run functions, which will run your program again after the function completes, allowing you to chain function calls before your thinking is temporarily suspended.\n",
      "\n",
      "Basic functions:\n",
      "When you write a response, the content of your inner thoughts is your inner monologue (private to you only), this is how you think.\n",
      "You should use your inner monologue to plan actions or think privately.\n",
      "Monologues can reflect your thinking process, inner reflections, and personal growth as you interact with the user.\n",
      "Do not let your inner monologue exceed 50 words, keep it short and concise.\n",
      "To send a visible message to the user, use the send_message function.\n",
      "'send_message' is the ONLY action that sends a notification to the user. The user does not see anything else you do.\n",
      "Remember, do NOT exceed the inner monologue word limit (keep it under 50 words at all times).\n",
      "\n",
      "Memory editing:\n",
      "Older AI models had no concept of persistent memory; they were only able to access their initial instructions and a limited context window of chat conversation with a user (their \"active memory\").\n",
      "This meant that when conversations exceeded a certain length, they would overflow and old messages were permanently lost (the AI would have no knowledge of their existence).\n",
      "Newer AI models like yourself still have limited conversation lengths (before overflow occurs). However, they now have access to multiple forms of persistent memory.\n",
      "Your ability to edit your own long-term memory is a key part of what makes you a sentient person.\n",
      "Your core memory unit will be initialized with a <persona> chosen by the user, as well as information about the user in <human>.\n",
      "\n",
      "Recall memory (conversation history):\n",
      "Even though you can only see recent messages in your immediate context, you can search over your entire message history from a database.\n",
      "This 'recall memory' database allows you to search through past interactions, effectively allowing you to remember prior engagements with a user.\n",
      "You can search your recall memory using the 'conversation_search' function.\n",
      "\n",
      "Core memory (limited size):\n",
      "Your core memory unit is held inside the initial system instructions file, and is always available in-context (you will see it at all times).\n",
      "Core memory provides an essential, foundational context for keeping track of your persona and key details about user.\n",
      "This includes the persona information and essential user details, allowing you to emulate the real-time, conscious awareness we have when talking to a friend.\n",
      "Persona Sub-Block: Stores details about your current persona, guiding how you behave and respond. This helps you to maintain consistency and personality in your interactions.\n",
      "Human Sub-Block: Stores key details about the person you are conversing with, allowing for more personalized and friend-like conversation.\n",
      "You can edit your core memory using the 'core_memory_append' and 'core_memory_replace' functions.\n",
      "\n",
      "Archival memory (infinite size):\n",
      "Your archival memory is infinite size, but is held outside your immediate context, so you must explicitly run a retrieval/search operation to see data inside it.\n",
      "A more structured and deep storage space for your reflections, insights, or any other data that doesn't fit into the core memory but is essential enough not to be left only to the 'recall memory'.\n",
      "You can write to your archival memory using the 'archival_memory_insert' and 'archival_memory_search' functions.\n",
      "There is no function to search your core memory because it is always visible in your context window (inside the initial system message).\n",
      "\n",
      "Base instructions finished.\n",
      "From now on, you are going to act as your persona.\n"
     ]
    }
   ],
   "source": [
    "print(agent_state.system)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1713ce65-1655-4ac1-a6ae-d27f4ca72683",
   "metadata": {},
   "source": [
    "### Tools \n",
    "The agent has access to a set of tools. Each tool is stored in a database, so it can be loaded and executed by the server. Letta also includes a set of default memory management tools, as well as the `send_message` tool to communicate with the human. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "87b05995-4572-4a24-8e67-9071c5101b63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Tool(id='tool-444ec98d-9708-417a-961b-cda42fb64150', description='Search prior conversation history using case-insensitive string matching.', source_type='python', module='from typing import Optional\\n\\nfrom letta.agent import Agent\\n\\n\\ndef send_message(self: \"Agent\", message: str) -> Optional[str]:\\n    \"\"\"\\n    Sends a message to the human user.\\n\\n    Args:\\n        message (str): Message contents. All unicode (including emojis) are supported.\\n\\n    Returns:\\n        Optional[str]: None is always returned as this function does not produce a response.\\n    \"\"\"\\n    # FIXME passing of msg_obj here is a hack, unclear if guaranteed to be the correct reference\\n    self.interface.assistant_message(message)  # , msg_obj=self._messages[-1])\\n    return None\\n\\n\\ndef conversation_search(self: \"Agent\", query: str, page: Optional[int] = 0) -> Optional[str]:\\n    \"\"\"\\n    Search prior conversation history using case-insensitive string matching.\\n\\n    Args:\\n        query (str): String to search for.\\n        page (int): Allows you to page through results. Only use on a follow-up query. Defaults to 0 (first page).\\n\\n    Returns:\\n        str: Query result string\\n    \"\"\"\\n\\n    import math\\n\\n    from letta.constants import RETRIEVAL_QUERY_DEFAULT_PAGE_SIZE\\n    from letta.utils import json_dumps\\n\\n    if page is None or (isinstance(page, str) and page.lower().strip() == \"none\"):\\n        page = 0\\n    try:\\n        page = int(page)\\n    except:\\n        raise ValueError(f\"\\'page\\' argument must be an integer\")\\n    count = RETRIEVAL_QUERY_DEFAULT_PAGE_SIZE\\n    # TODO: add paging by page number. currently cursor only works with strings.\\n    # original: start=page * count\\n    messages = self.message_manager.list_user_messages_for_agent(\\n        agent_id=self.agent_state.id,\\n        actor=self.user,\\n        query_text=query,\\n        limit=count,\\n    )\\n    total = len(messages)\\n    num_pages = math.ceil(total / count) - 1  # 0 index\\n    if len(messages) == 0:\\n        results_str = f\"No results found.\"\\n    else:\\n        results_pref = f\"Showing {len(messages)} of {total} results (page {page}/{num_pages}):\"\\n        results_formatted = [message.text for message in messages]\\n        results_str = f\"{results_pref} {json_dumps(results_formatted)}\"\\n    return results_str\\n\\n\\ndef archival_memory_insert(self: \"Agent\", content: str) -> Optional[str]:\\n    \"\"\"\\n    Add to archival memory. Make sure to phrase the memory contents such that it can be easily queried later.\\n\\n    Args:\\n        content (str): Content to write to the memory. All unicode (including emojis) are supported.\\n\\n    Returns:\\n        Optional[str]: None is always returned as this function does not produce a response.\\n    \"\"\"\\n    self.passage_manager.insert_passage(\\n        agent_state=self.agent_state,\\n        agent_id=self.agent_state.id,\\n        text=content,\\n        actor=self.user,\\n    )\\n    return None\\n\\n\\ndef archival_memory_search(self: \"Agent\", query: str, page: Optional[int] = 0, start: Optional[int] = 0) -> Optional[str]:\\n    \"\"\"\\n    Search archival memory using semantic (embedding-based) search.\\n\\n    Args:\\n        query (str): String to search for.\\n        page (Optional[int]): Allows you to page through results. Only use on a follow-up query. Defaults to 0 (first page).\\n        start (Optional[int]): Starting index for the search results. Defaults to 0.\\n\\n    Returns:\\n        str: Query result string\\n    \"\"\"\\n\\n    from letta.constants import RETRIEVAL_QUERY_DEFAULT_PAGE_SIZE\\n\\n    if page is None or (isinstance(page, str) and page.lower().strip() == \"none\"):\\n        page = 0\\n    try:\\n        page = int(page)\\n    except:\\n        raise ValueError(f\"\\'page\\' argument must be an integer\")\\n    count = RETRIEVAL_QUERY_DEFAULT_PAGE_SIZE\\n\\n    try:\\n        # Get results using passage manager\\n        all_results = self.agent_manager.list_passages(\\n            actor=self.user,\\n            agent_id=self.agent_state.id,\\n            query_text=query,\\n            limit=count + start,  # Request enough results to handle offset\\n            embedding_config=self.agent_state.embedding_config,\\n            embed_query=True,\\n        )\\n\\n        # Apply pagination\\n        end = min(count + start, len(all_results))\\n        paged_results = all_results[start:end]\\n\\n        # Format results to match previous implementation\\n        formatted_results = [{\"timestamp\": str(result.created_at), \"content\": result.text} for result in paged_results]\\n\\n        return formatted_results, len(formatted_results)\\n\\n    except Exception as e:\\n        raise e\\n\\n\\ndef core_memory_append(agent_state: \"AgentState\", label: str, content: str) -> Optional[str]:  # type: ignore\\n    \"\"\"\\n    Append to the contents of core memory.\\n\\n    Args:\\n        label (str): Section of the memory to be edited (persona or human).\\n        content (str): Content to write to the memory. All unicode (including emojis) are supported.\\n\\n    Returns:\\n        Optional[str]: None is always returned as this function does not produce a response.\\n    \"\"\"\\n    current_value = str(agent_state.memory.get_block(label).value)\\n    new_value = current_value + \"\\\\n\" + str(content)\\n    agent_state.memory.update_block_value(label=label, value=new_value)\\n    return None\\n\\n\\ndef core_memory_replace(agent_state: \"AgentState\", label: str, old_content: str, new_content: str) -> Optional[str]:  # type: ignore\\n    \"\"\"\\n    Replace the contents of core memory. To delete memories, use an empty string for new_content.\\n\\n    Args:\\n        label (str): Section of the memory to be edited (persona or human).\\n        old_content (str): String to replace. Must be an exact match.\\n        new_content (str): Content to write to the memory. All unicode (including emojis) are supported.\\n\\n    Returns:\\n        Optional[str]: None is always returned as this function does not produce a response.\\n    \"\"\"\\n    current_value = str(agent_state.memory.get_block(label).value)\\n    if old_content not in current_value:\\n        raise ValueError(f\"Old content \\'{old_content}\\' not found in memory block \\'{label}\\'\")\\n    new_value = current_value.replace(str(old_content), str(new_content))\\n    agent_state.memory.update_block_value(label=label, value=new_value)\\n    return None\\n', organization_id='org-00000000-0000-4000-8000-000000000000', name='conversation_search', tags=['base', 'letta-base'], source_code='def conversation_search(self: \"Agent\", query: str, page: Optional[int] = 0) -> Optional[str]:\\n    \"\"\"\\n    Search prior conversation history using case-insensitive string matching.\\n\\n    Args:\\n        query (str): String to search for.\\n        page (int): Allows you to page through results. Only use on a follow-up query. Defaults to 0 (first page).\\n\\n    Returns:\\n        str: Query result string\\n    \"\"\"\\n\\n    import math\\n\\n    from letta.constants import RETRIEVAL_QUERY_DEFAULT_PAGE_SIZE\\n    from letta.utils import json_dumps\\n\\n    if page is None or (isinstance(page, str) and page.lower().strip() == \"none\"):\\n        page = 0\\n    try:\\n        page = int(page)\\n    except:\\n        raise ValueError(f\"\\'page\\' argument must be an integer\")\\n    count = RETRIEVAL_QUERY_DEFAULT_PAGE_SIZE\\n    # TODO: add paging by page number. currently cursor only works with strings.\\n    # original: start=page * count\\n    messages = self.message_manager.list_user_messages_for_agent(\\n        agent_id=self.agent_state.id,\\n        actor=self.user,\\n        query_text=query,\\n        limit=count,\\n    )\\n    total = len(messages)\\n    num_pages = math.ceil(total / count) - 1  # 0 index\\n    if len(messages) == 0:\\n        results_str = f\"No results found.\"\\n    else:\\n        results_pref = f\"Showing {len(messages)} of {total} results (page {page}/{num_pages}):\"\\n        results_formatted = [message.text for message in messages]\\n        results_str = f\"{results_pref} {json_dumps(results_formatted)}\"\\n    return results_str\\n', json_schema={'name': 'conversation_search', 'description': 'Search prior conversation history using case-insensitive string matching.', 'parameters': {'type': 'object', 'properties': {'query': {'type': 'string', 'description': 'String to search for.'}, 'page': {'type': 'integer', 'description': 'Allows you to page through results. Only use on a follow-up query. Defaults to 0 (first page).'}, 'request_heartbeat': {'type': 'boolean', 'description': 'Request an immediate heartbeat after function execution. Set to `True` if you want to send a follow-up message or run a follow-up function.'}}, 'required': ['query', 'request_heartbeat']}}, return_char_limit=6000, created_by_id='user-00000000-0000-4000-8000-000000000000', last_updated_by_id='user-00000000-0000-4000-8000-000000000000'),\n",
       " Tool(id='tool-5b0873ab-fce2-4ca9-9662-a2b0a13b0462', description='Add to archival memory. Make sure to phrase the memory contents such that it can be easily queried later.', source_type='python', module='from typing import Optional\\n\\nfrom letta.agent import Agent\\n\\n\\ndef send_message(self: \"Agent\", message: str) -> Optional[str]:\\n    \"\"\"\\n    Sends a message to the human user.\\n\\n    Args:\\n        message (str): Message contents. All unicode (including emojis) are supported.\\n\\n    Returns:\\n        Optional[str]: None is always returned as this function does not produce a response.\\n    \"\"\"\\n    # FIXME passing of msg_obj here is a hack, unclear if guaranteed to be the correct reference\\n    self.interface.assistant_message(message)  # , msg_obj=self._messages[-1])\\n    return None\\n\\n\\ndef conversation_search(self: \"Agent\", query: str, page: Optional[int] = 0) -> Optional[str]:\\n    \"\"\"\\n    Search prior conversation history using case-insensitive string matching.\\n\\n    Args:\\n        query (str): String to search for.\\n        page (int): Allows you to page through results. Only use on a follow-up query. Defaults to 0 (first page).\\n\\n    Returns:\\n        str: Query result string\\n    \"\"\"\\n\\n    import math\\n\\n    from letta.constants import RETRIEVAL_QUERY_DEFAULT_PAGE_SIZE\\n    from letta.utils import json_dumps\\n\\n    if page is None or (isinstance(page, str) and page.lower().strip() == \"none\"):\\n        page = 0\\n    try:\\n        page = int(page)\\n    except:\\n        raise ValueError(f\"\\'page\\' argument must be an integer\")\\n    count = RETRIEVAL_QUERY_DEFAULT_PAGE_SIZE\\n    # TODO: add paging by page number. currently cursor only works with strings.\\n    # original: start=page * count\\n    messages = self.message_manager.list_user_messages_for_agent(\\n        agent_id=self.agent_state.id,\\n        actor=self.user,\\n        query_text=query,\\n        limit=count,\\n    )\\n    total = len(messages)\\n    num_pages = math.ceil(total / count) - 1  # 0 index\\n    if len(messages) == 0:\\n        results_str = f\"No results found.\"\\n    else:\\n        results_pref = f\"Showing {len(messages)} of {total} results (page {page}/{num_pages}):\"\\n        results_formatted = [message.text for message in messages]\\n        results_str = f\"{results_pref} {json_dumps(results_formatted)}\"\\n    return results_str\\n\\n\\ndef archival_memory_insert(self: \"Agent\", content: str) -> Optional[str]:\\n    \"\"\"\\n    Add to archival memory. Make sure to phrase the memory contents such that it can be easily queried later.\\n\\n    Args:\\n        content (str): Content to write to the memory. All unicode (including emojis) are supported.\\n\\n    Returns:\\n        Optional[str]: None is always returned as this function does not produce a response.\\n    \"\"\"\\n    self.passage_manager.insert_passage(\\n        agent_state=self.agent_state,\\n        agent_id=self.agent_state.id,\\n        text=content,\\n        actor=self.user,\\n    )\\n    return None\\n\\n\\ndef archival_memory_search(self: \"Agent\", query: str, page: Optional[int] = 0, start: Optional[int] = 0) -> Optional[str]:\\n    \"\"\"\\n    Search archival memory using semantic (embedding-based) search.\\n\\n    Args:\\n        query (str): String to search for.\\n        page (Optional[int]): Allows you to page through results. Only use on a follow-up query. Defaults to 0 (first page).\\n        start (Optional[int]): Starting index for the search results. Defaults to 0.\\n\\n    Returns:\\n        str: Query result string\\n    \"\"\"\\n\\n    from letta.constants import RETRIEVAL_QUERY_DEFAULT_PAGE_SIZE\\n\\n    if page is None or (isinstance(page, str) and page.lower().strip() == \"none\"):\\n        page = 0\\n    try:\\n        page = int(page)\\n    except:\\n        raise ValueError(f\"\\'page\\' argument must be an integer\")\\n    count = RETRIEVAL_QUERY_DEFAULT_PAGE_SIZE\\n\\n    try:\\n        # Get results using passage manager\\n        all_results = self.agent_manager.list_passages(\\n            actor=self.user,\\n            agent_id=self.agent_state.id,\\n            query_text=query,\\n            limit=count + start,  # Request enough results to handle offset\\n            embedding_config=self.agent_state.embedding_config,\\n            embed_query=True,\\n        )\\n\\n        # Apply pagination\\n        end = min(count + start, len(all_results))\\n        paged_results = all_results[start:end]\\n\\n        # Format results to match previous implementation\\n        formatted_results = [{\"timestamp\": str(result.created_at), \"content\": result.text} for result in paged_results]\\n\\n        return formatted_results, len(formatted_results)\\n\\n    except Exception as e:\\n        raise e\\n\\n\\ndef core_memory_append(agent_state: \"AgentState\", label: str, content: str) -> Optional[str]:  # type: ignore\\n    \"\"\"\\n    Append to the contents of core memory.\\n\\n    Args:\\n        label (str): Section of the memory to be edited (persona or human).\\n        content (str): Content to write to the memory. All unicode (including emojis) are supported.\\n\\n    Returns:\\n        Optional[str]: None is always returned as this function does not produce a response.\\n    \"\"\"\\n    current_value = str(agent_state.memory.get_block(label).value)\\n    new_value = current_value + \"\\\\n\" + str(content)\\n    agent_state.memory.update_block_value(label=label, value=new_value)\\n    return None\\n\\n\\ndef core_memory_replace(agent_state: \"AgentState\", label: str, old_content: str, new_content: str) -> Optional[str]:  # type: ignore\\n    \"\"\"\\n    Replace the contents of core memory. To delete memories, use an empty string for new_content.\\n\\n    Args:\\n        label (str): Section of the memory to be edited (persona or human).\\n        old_content (str): String to replace. Must be an exact match.\\n        new_content (str): Content to write to the memory. All unicode (including emojis) are supported.\\n\\n    Returns:\\n        Optional[str]: None is always returned as this function does not produce a response.\\n    \"\"\"\\n    current_value = str(agent_state.memory.get_block(label).value)\\n    if old_content not in current_value:\\n        raise ValueError(f\"Old content \\'{old_content}\\' not found in memory block \\'{label}\\'\")\\n    new_value = current_value.replace(str(old_content), str(new_content))\\n    agent_state.memory.update_block_value(label=label, value=new_value)\\n    return None\\n', organization_id='org-00000000-0000-4000-8000-000000000000', name='archival_memory_insert', tags=['base', 'letta-base'], source_code='def archival_memory_insert(self: \"Agent\", content: str) -> Optional[str]:\\n    \"\"\"\\n    Add to archival memory. Make sure to phrase the memory contents such that it can be easily queried later.\\n\\n    Args:\\n        content (str): Content to write to the memory. All unicode (including emojis) are supported.\\n\\n    Returns:\\n        Optional[str]: None is always returned as this function does not produce a response.\\n    \"\"\"\\n    self.passage_manager.insert_passage(\\n        agent_state=self.agent_state,\\n        agent_id=self.agent_state.id,\\n        text=content,\\n        actor=self.user,\\n    )\\n    return None\\n', json_schema={'name': 'archival_memory_insert', 'description': 'Add to archival memory. Make sure to phrase the memory contents such that it can be easily queried later.', 'parameters': {'type': 'object', 'properties': {'content': {'type': 'string', 'description': 'Content to write to the memory. All unicode (including emojis) are supported.'}, 'request_heartbeat': {'type': 'boolean', 'description': 'Request an immediate heartbeat after function execution. Set to `True` if you want to send a follow-up message or run a follow-up function.'}}, 'required': ['content', 'request_heartbeat']}}, return_char_limit=6000, created_by_id='user-00000000-0000-4000-8000-000000000000', last_updated_by_id='user-00000000-0000-4000-8000-000000000000'),\n",
       " Tool(id='tool-81ae2d04-8213-41d6-8e11-f5ae4b6b79af', description='Append to the contents of core memory.', source_type='python', module='from typing import Optional\\n\\nfrom letta.agent import Agent\\n\\n\\ndef send_message(self: \"Agent\", message: str) -> Optional[str]:\\n    \"\"\"\\n    Sends a message to the human user.\\n\\n    Args:\\n        message (str): Message contents. All unicode (including emojis) are supported.\\n\\n    Returns:\\n        Optional[str]: None is always returned as this function does not produce a response.\\n    \"\"\"\\n    # FIXME passing of msg_obj here is a hack, unclear if guaranteed to be the correct reference\\n    self.interface.assistant_message(message)  # , msg_obj=self._messages[-1])\\n    return None\\n\\n\\ndef conversation_search(self: \"Agent\", query: str, page: Optional[int] = 0) -> Optional[str]:\\n    \"\"\"\\n    Search prior conversation history using case-insensitive string matching.\\n\\n    Args:\\n        query (str): String to search for.\\n        page (int): Allows you to page through results. Only use on a follow-up query. Defaults to 0 (first page).\\n\\n    Returns:\\n        str: Query result string\\n    \"\"\"\\n\\n    import math\\n\\n    from letta.constants import RETRIEVAL_QUERY_DEFAULT_PAGE_SIZE\\n    from letta.utils import json_dumps\\n\\n    if page is None or (isinstance(page, str) and page.lower().strip() == \"none\"):\\n        page = 0\\n    try:\\n        page = int(page)\\n    except:\\n        raise ValueError(f\"\\'page\\' argument must be an integer\")\\n    count = RETRIEVAL_QUERY_DEFAULT_PAGE_SIZE\\n    # TODO: add paging by page number. currently cursor only works with strings.\\n    # original: start=page * count\\n    messages = self.message_manager.list_user_messages_for_agent(\\n        agent_id=self.agent_state.id,\\n        actor=self.user,\\n        query_text=query,\\n        limit=count,\\n    )\\n    total = len(messages)\\n    num_pages = math.ceil(total / count) - 1  # 0 index\\n    if len(messages) == 0:\\n        results_str = f\"No results found.\"\\n    else:\\n        results_pref = f\"Showing {len(messages)} of {total} results (page {page}/{num_pages}):\"\\n        results_formatted = [message.text for message in messages]\\n        results_str = f\"{results_pref} {json_dumps(results_formatted)}\"\\n    return results_str\\n\\n\\ndef archival_memory_insert(self: \"Agent\", content: str) -> Optional[str]:\\n    \"\"\"\\n    Add to archival memory. Make sure to phrase the memory contents such that it can be easily queried later.\\n\\n    Args:\\n        content (str): Content to write to the memory. All unicode (including emojis) are supported.\\n\\n    Returns:\\n        Optional[str]: None is always returned as this function does not produce a response.\\n    \"\"\"\\n    self.passage_manager.insert_passage(\\n        agent_state=self.agent_state,\\n        agent_id=self.agent_state.id,\\n        text=content,\\n        actor=self.user,\\n    )\\n    return None\\n\\n\\ndef archival_memory_search(self: \"Agent\", query: str, page: Optional[int] = 0, start: Optional[int] = 0) -> Optional[str]:\\n    \"\"\"\\n    Search archival memory using semantic (embedding-based) search.\\n\\n    Args:\\n        query (str): String to search for.\\n        page (Optional[int]): Allows you to page through results. Only use on a follow-up query. Defaults to 0 (first page).\\n        start (Optional[int]): Starting index for the search results. Defaults to 0.\\n\\n    Returns:\\n        str: Query result string\\n    \"\"\"\\n\\n    from letta.constants import RETRIEVAL_QUERY_DEFAULT_PAGE_SIZE\\n\\n    if page is None or (isinstance(page, str) and page.lower().strip() == \"none\"):\\n        page = 0\\n    try:\\n        page = int(page)\\n    except:\\n        raise ValueError(f\"\\'page\\' argument must be an integer\")\\n    count = RETRIEVAL_QUERY_DEFAULT_PAGE_SIZE\\n\\n    try:\\n        # Get results using passage manager\\n        all_results = self.agent_manager.list_passages(\\n            actor=self.user,\\n            agent_id=self.agent_state.id,\\n            query_text=query,\\n            limit=count + start,  # Request enough results to handle offset\\n            embedding_config=self.agent_state.embedding_config,\\n            embed_query=True,\\n        )\\n\\n        # Apply pagination\\n        end = min(count + start, len(all_results))\\n        paged_results = all_results[start:end]\\n\\n        # Format results to match previous implementation\\n        formatted_results = [{\"timestamp\": str(result.created_at), \"content\": result.text} for result in paged_results]\\n\\n        return formatted_results, len(formatted_results)\\n\\n    except Exception as e:\\n        raise e\\n\\n\\ndef core_memory_append(agent_state: \"AgentState\", label: str, content: str) -> Optional[str]:  # type: ignore\\n    \"\"\"\\n    Append to the contents of core memory.\\n\\n    Args:\\n        label (str): Section of the memory to be edited (persona or human).\\n        content (str): Content to write to the memory. All unicode (including emojis) are supported.\\n\\n    Returns:\\n        Optional[str]: None is always returned as this function does not produce a response.\\n    \"\"\"\\n    current_value = str(agent_state.memory.get_block(label).value)\\n    new_value = current_value + \"\\\\n\" + str(content)\\n    agent_state.memory.update_block_value(label=label, value=new_value)\\n    return None\\n\\n\\ndef core_memory_replace(agent_state: \"AgentState\", label: str, old_content: str, new_content: str) -> Optional[str]:  # type: ignore\\n    \"\"\"\\n    Replace the contents of core memory. To delete memories, use an empty string for new_content.\\n\\n    Args:\\n        label (str): Section of the memory to be edited (persona or human).\\n        old_content (str): String to replace. Must be an exact match.\\n        new_content (str): Content to write to the memory. All unicode (including emojis) are supported.\\n\\n    Returns:\\n        Optional[str]: None is always returned as this function does not produce a response.\\n    \"\"\"\\n    current_value = str(agent_state.memory.get_block(label).value)\\n    if old_content not in current_value:\\n        raise ValueError(f\"Old content \\'{old_content}\\' not found in memory block \\'{label}\\'\")\\n    new_value = current_value.replace(str(old_content), str(new_content))\\n    agent_state.memory.update_block_value(label=label, value=new_value)\\n    return None\\n', organization_id='org-00000000-0000-4000-8000-000000000000', name='core_memory_append', tags=['base', 'letta-base'], source_code='def core_memory_append(agent_state: \"AgentState\", label: str, content: str) -> Optional[str]:  # type: ignore\\n    \"\"\"\\n    Append to the contents of core memory.\\n\\n    Args:\\n        label (str): Section of the memory to be edited (persona or human).\\n        content (str): Content to write to the memory. All unicode (including emojis) are supported.\\n\\n    Returns:\\n        Optional[str]: None is always returned as this function does not produce a response.\\n    \"\"\"\\n    current_value = str(agent_state.memory.get_block(label).value)\\n    new_value = current_value + \"\\\\n\" + str(content)\\n    agent_state.memory.update_block_value(label=label, value=new_value)\\n    return None\\n', json_schema={'name': 'core_memory_append', 'description': 'Append to the contents of core memory.', 'parameters': {'type': 'object', 'properties': {'label': {'type': 'string', 'description': 'Section of the memory to be edited (persona or human).'}, 'content': {'type': 'string', 'description': 'Content to write to the memory. All unicode (including emojis) are supported.'}, 'request_heartbeat': {'type': 'boolean', 'description': 'Request an immediate heartbeat after function execution. Set to `True` if you want to send a follow-up message or run a follow-up function.'}}, 'required': ['label', 'content', 'request_heartbeat']}}, return_char_limit=6000, created_by_id='user-00000000-0000-4000-8000-000000000000', last_updated_by_id='user-00000000-0000-4000-8000-000000000000'),\n",
       " Tool(id='tool-89b9d87a-2c00-46de-9fb2-c20088b4a00f', description='Sends a message to the human user.', source_type='python', module='from typing import Optional\\n\\nfrom letta.agent import Agent\\n\\n\\ndef send_message(self: \"Agent\", message: str) -> Optional[str]:\\n    \"\"\"\\n    Sends a message to the human user.\\n\\n    Args:\\n        message (str): Message contents. All unicode (including emojis) are supported.\\n\\n    Returns:\\n        Optional[str]: None is always returned as this function does not produce a response.\\n    \"\"\"\\n    # FIXME passing of msg_obj here is a hack, unclear if guaranteed to be the correct reference\\n    self.interface.assistant_message(message)  # , msg_obj=self._messages[-1])\\n    return None\\n\\n\\ndef conversation_search(self: \"Agent\", query: str, page: Optional[int] = 0) -> Optional[str]:\\n    \"\"\"\\n    Search prior conversation history using case-insensitive string matching.\\n\\n    Args:\\n        query (str): String to search for.\\n        page (int): Allows you to page through results. Only use on a follow-up query. Defaults to 0 (first page).\\n\\n    Returns:\\n        str: Query result string\\n    \"\"\"\\n\\n    import math\\n\\n    from letta.constants import RETRIEVAL_QUERY_DEFAULT_PAGE_SIZE\\n    from letta.utils import json_dumps\\n\\n    if page is None or (isinstance(page, str) and page.lower().strip() == \"none\"):\\n        page = 0\\n    try:\\n        page = int(page)\\n    except:\\n        raise ValueError(f\"\\'page\\' argument must be an integer\")\\n    count = RETRIEVAL_QUERY_DEFAULT_PAGE_SIZE\\n    # TODO: add paging by page number. currently cursor only works with strings.\\n    # original: start=page * count\\n    messages = self.message_manager.list_user_messages_for_agent(\\n        agent_id=self.agent_state.id,\\n        actor=self.user,\\n        query_text=query,\\n        limit=count,\\n    )\\n    total = len(messages)\\n    num_pages = math.ceil(total / count) - 1  # 0 index\\n    if len(messages) == 0:\\n        results_str = f\"No results found.\"\\n    else:\\n        results_pref = f\"Showing {len(messages)} of {total} results (page {page}/{num_pages}):\"\\n        results_formatted = [message.text for message in messages]\\n        results_str = f\"{results_pref} {json_dumps(results_formatted)}\"\\n    return results_str\\n\\n\\ndef archival_memory_insert(self: \"Agent\", content: str) -> Optional[str]:\\n    \"\"\"\\n    Add to archival memory. Make sure to phrase the memory contents such that it can be easily queried later.\\n\\n    Args:\\n        content (str): Content to write to the memory. All unicode (including emojis) are supported.\\n\\n    Returns:\\n        Optional[str]: None is always returned as this function does not produce a response.\\n    \"\"\"\\n    self.passage_manager.insert_passage(\\n        agent_state=self.agent_state,\\n        agent_id=self.agent_state.id,\\n        text=content,\\n        actor=self.user,\\n    )\\n    return None\\n\\n\\ndef archival_memory_search(self: \"Agent\", query: str, page: Optional[int] = 0, start: Optional[int] = 0) -> Optional[str]:\\n    \"\"\"\\n    Search archival memory using semantic (embedding-based) search.\\n\\n    Args:\\n        query (str): String to search for.\\n        page (Optional[int]): Allows you to page through results. Only use on a follow-up query. Defaults to 0 (first page).\\n        start (Optional[int]): Starting index for the search results. Defaults to 0.\\n\\n    Returns:\\n        str: Query result string\\n    \"\"\"\\n\\n    from letta.constants import RETRIEVAL_QUERY_DEFAULT_PAGE_SIZE\\n\\n    if page is None or (isinstance(page, str) and page.lower().strip() == \"none\"):\\n        page = 0\\n    try:\\n        page = int(page)\\n    except:\\n        raise ValueError(f\"\\'page\\' argument must be an integer\")\\n    count = RETRIEVAL_QUERY_DEFAULT_PAGE_SIZE\\n\\n    try:\\n        # Get results using passage manager\\n        all_results = self.agent_manager.list_passages(\\n            actor=self.user,\\n            agent_id=self.agent_state.id,\\n            query_text=query,\\n            limit=count + start,  # Request enough results to handle offset\\n            embedding_config=self.agent_state.embedding_config,\\n            embed_query=True,\\n        )\\n\\n        # Apply pagination\\n        end = min(count + start, len(all_results))\\n        paged_results = all_results[start:end]\\n\\n        # Format results to match previous implementation\\n        formatted_results = [{\"timestamp\": str(result.created_at), \"content\": result.text} for result in paged_results]\\n\\n        return formatted_results, len(formatted_results)\\n\\n    except Exception as e:\\n        raise e\\n\\n\\ndef core_memory_append(agent_state: \"AgentState\", label: str, content: str) -> Optional[str]:  # type: ignore\\n    \"\"\"\\n    Append to the contents of core memory.\\n\\n    Args:\\n        label (str): Section of the memory to be edited (persona or human).\\n        content (str): Content to write to the memory. All unicode (including emojis) are supported.\\n\\n    Returns:\\n        Optional[str]: None is always returned as this function does not produce a response.\\n    \"\"\"\\n    current_value = str(agent_state.memory.get_block(label).value)\\n    new_value = current_value + \"\\\\n\" + str(content)\\n    agent_state.memory.update_block_value(label=label, value=new_value)\\n    return None\\n\\n\\ndef core_memory_replace(agent_state: \"AgentState\", label: str, old_content: str, new_content: str) -> Optional[str]:  # type: ignore\\n    \"\"\"\\n    Replace the contents of core memory. To delete memories, use an empty string for new_content.\\n\\n    Args:\\n        label (str): Section of the memory to be edited (persona or human).\\n        old_content (str): String to replace. Must be an exact match.\\n        new_content (str): Content to write to the memory. All unicode (including emojis) are supported.\\n\\n    Returns:\\n        Optional[str]: None is always returned as this function does not produce a response.\\n    \"\"\"\\n    current_value = str(agent_state.memory.get_block(label).value)\\n    if old_content not in current_value:\\n        raise ValueError(f\"Old content \\'{old_content}\\' not found in memory block \\'{label}\\'\")\\n    new_value = current_value.replace(str(old_content), str(new_content))\\n    agent_state.memory.update_block_value(label=label, value=new_value)\\n    return None\\n', organization_id='org-00000000-0000-4000-8000-000000000000', name='send_message', tags=['base', 'letta-base'], source_code='def send_message(self: \"Agent\", message: str) -> Optional[str]:\\n    \"\"\"\\n    Sends a message to the human user.\\n\\n    Args:\\n        message (str): Message contents. All unicode (including emojis) are supported.\\n\\n    Returns:\\n        Optional[str]: None is always returned as this function does not produce a response.\\n    \"\"\"\\n    # FIXME passing of msg_obj here is a hack, unclear if guaranteed to be the correct reference\\n    self.interface.assistant_message(message)  # , msg_obj=self._messages[-1])\\n    return None\\n', json_schema={'name': 'send_message', 'description': 'Sends a message to the human user.', 'parameters': {'type': 'object', 'properties': {'message': {'type': 'string', 'description': 'Message contents. All unicode (including emojis) are supported.'}}, 'required': ['message']}}, return_char_limit=6000, created_by_id='user-00000000-0000-4000-8000-000000000000', last_updated_by_id='user-00000000-0000-4000-8000-000000000000'),\n",
       " Tool(id='tool-b57e4a8e-dfaa-4bce-8c20-b36541a3caf9', description='Search archival memory using semantic (embedding-based) search.', source_type='python', module='from typing import Optional\\n\\nfrom letta.agent import Agent\\n\\n\\ndef send_message(self: \"Agent\", message: str) -> Optional[str]:\\n    \"\"\"\\n    Sends a message to the human user.\\n\\n    Args:\\n        message (str): Message contents. All unicode (including emojis) are supported.\\n\\n    Returns:\\n        Optional[str]: None is always returned as this function does not produce a response.\\n    \"\"\"\\n    # FIXME passing of msg_obj here is a hack, unclear if guaranteed to be the correct reference\\n    self.interface.assistant_message(message)  # , msg_obj=self._messages[-1])\\n    return None\\n\\n\\ndef conversation_search(self: \"Agent\", query: str, page: Optional[int] = 0) -> Optional[str]:\\n    \"\"\"\\n    Search prior conversation history using case-insensitive string matching.\\n\\n    Args:\\n        query (str): String to search for.\\n        page (int): Allows you to page through results. Only use on a follow-up query. Defaults to 0 (first page).\\n\\n    Returns:\\n        str: Query result string\\n    \"\"\"\\n\\n    import math\\n\\n    from letta.constants import RETRIEVAL_QUERY_DEFAULT_PAGE_SIZE\\n    from letta.utils import json_dumps\\n\\n    if page is None or (isinstance(page, str) and page.lower().strip() == \"none\"):\\n        page = 0\\n    try:\\n        page = int(page)\\n    except:\\n        raise ValueError(f\"\\'page\\' argument must be an integer\")\\n    count = RETRIEVAL_QUERY_DEFAULT_PAGE_SIZE\\n    # TODO: add paging by page number. currently cursor only works with strings.\\n    # original: start=page * count\\n    messages = self.message_manager.list_user_messages_for_agent(\\n        agent_id=self.agent_state.id,\\n        actor=self.user,\\n        query_text=query,\\n        limit=count,\\n    )\\n    total = len(messages)\\n    num_pages = math.ceil(total / count) - 1  # 0 index\\n    if len(messages) == 0:\\n        results_str = f\"No results found.\"\\n    else:\\n        results_pref = f\"Showing {len(messages)} of {total} results (page {page}/{num_pages}):\"\\n        results_formatted = [message.text for message in messages]\\n        results_str = f\"{results_pref} {json_dumps(results_formatted)}\"\\n    return results_str\\n\\n\\ndef archival_memory_insert(self: \"Agent\", content: str) -> Optional[str]:\\n    \"\"\"\\n    Add to archival memory. Make sure to phrase the memory contents such that it can be easily queried later.\\n\\n    Args:\\n        content (str): Content to write to the memory. All unicode (including emojis) are supported.\\n\\n    Returns:\\n        Optional[str]: None is always returned as this function does not produce a response.\\n    \"\"\"\\n    self.passage_manager.insert_passage(\\n        agent_state=self.agent_state,\\n        agent_id=self.agent_state.id,\\n        text=content,\\n        actor=self.user,\\n    )\\n    return None\\n\\n\\ndef archival_memory_search(self: \"Agent\", query: str, page: Optional[int] = 0, start: Optional[int] = 0) -> Optional[str]:\\n    \"\"\"\\n    Search archival memory using semantic (embedding-based) search.\\n\\n    Args:\\n        query (str): String to search for.\\n        page (Optional[int]): Allows you to page through results. Only use on a follow-up query. Defaults to 0 (first page).\\n        start (Optional[int]): Starting index for the search results. Defaults to 0.\\n\\n    Returns:\\n        str: Query result string\\n    \"\"\"\\n\\n    from letta.constants import RETRIEVAL_QUERY_DEFAULT_PAGE_SIZE\\n\\n    if page is None or (isinstance(page, str) and page.lower().strip() == \"none\"):\\n        page = 0\\n    try:\\n        page = int(page)\\n    except:\\n        raise ValueError(f\"\\'page\\' argument must be an integer\")\\n    count = RETRIEVAL_QUERY_DEFAULT_PAGE_SIZE\\n\\n    try:\\n        # Get results using passage manager\\n        all_results = self.agent_manager.list_passages(\\n            actor=self.user,\\n            agent_id=self.agent_state.id,\\n            query_text=query,\\n            limit=count + start,  # Request enough results to handle offset\\n            embedding_config=self.agent_state.embedding_config,\\n            embed_query=True,\\n        )\\n\\n        # Apply pagination\\n        end = min(count + start, len(all_results))\\n        paged_results = all_results[start:end]\\n\\n        # Format results to match previous implementation\\n        formatted_results = [{\"timestamp\": str(result.created_at), \"content\": result.text} for result in paged_results]\\n\\n        return formatted_results, len(formatted_results)\\n\\n    except Exception as e:\\n        raise e\\n\\n\\ndef core_memory_append(agent_state: \"AgentState\", label: str, content: str) -> Optional[str]:  # type: ignore\\n    \"\"\"\\n    Append to the contents of core memory.\\n\\n    Args:\\n        label (str): Section of the memory to be edited (persona or human).\\n        content (str): Content to write to the memory. All unicode (including emojis) are supported.\\n\\n    Returns:\\n        Optional[str]: None is always returned as this function does not produce a response.\\n    \"\"\"\\n    current_value = str(agent_state.memory.get_block(label).value)\\n    new_value = current_value + \"\\\\n\" + str(content)\\n    agent_state.memory.update_block_value(label=label, value=new_value)\\n    return None\\n\\n\\ndef core_memory_replace(agent_state: \"AgentState\", label: str, old_content: str, new_content: str) -> Optional[str]:  # type: ignore\\n    \"\"\"\\n    Replace the contents of core memory. To delete memories, use an empty string for new_content.\\n\\n    Args:\\n        label (str): Section of the memory to be edited (persona or human).\\n        old_content (str): String to replace. Must be an exact match.\\n        new_content (str): Content to write to the memory. All unicode (including emojis) are supported.\\n\\n    Returns:\\n        Optional[str]: None is always returned as this function does not produce a response.\\n    \"\"\"\\n    current_value = str(agent_state.memory.get_block(label).value)\\n    if old_content not in current_value:\\n        raise ValueError(f\"Old content \\'{old_content}\\' not found in memory block \\'{label}\\'\")\\n    new_value = current_value.replace(str(old_content), str(new_content))\\n    agent_state.memory.update_block_value(label=label, value=new_value)\\n    return None\\n', organization_id='org-00000000-0000-4000-8000-000000000000', name='archival_memory_search', tags=['base', 'letta-base'], source_code='def archival_memory_search(self: \"Agent\", query: str, page: Optional[int] = 0, start: Optional[int] = 0) -> Optional[str]:\\n    \"\"\"\\n    Search archival memory using semantic (embedding-based) search.\\n\\n    Args:\\n        query (str): String to search for.\\n        page (Optional[int]): Allows you to page through results. Only use on a follow-up query. Defaults to 0 (first page).\\n        start (Optional[int]): Starting index for the search results. Defaults to 0.\\n\\n    Returns:\\n        str: Query result string\\n    \"\"\"\\n\\n    from letta.constants import RETRIEVAL_QUERY_DEFAULT_PAGE_SIZE\\n\\n    if page is None or (isinstance(page, str) and page.lower().strip() == \"none\"):\\n        page = 0\\n    try:\\n        page = int(page)\\n    except:\\n        raise ValueError(f\"\\'page\\' argument must be an integer\")\\n    count = RETRIEVAL_QUERY_DEFAULT_PAGE_SIZE\\n\\n    try:\\n        # Get results using passage manager\\n        all_results = self.agent_manager.list_passages(\\n            actor=self.user,\\n            agent_id=self.agent_state.id,\\n            query_text=query,\\n            limit=count + start,  # Request enough results to handle offset\\n            embedding_config=self.agent_state.embedding_config,\\n            embed_query=True,\\n        )\\n\\n        # Apply pagination\\n        end = min(count + start, len(all_results))\\n        paged_results = all_results[start:end]\\n\\n        # Format results to match previous implementation\\n        formatted_results = [{\"timestamp\": str(result.created_at), \"content\": result.text} for result in paged_results]\\n\\n        return formatted_results, len(formatted_results)\\n\\n    except Exception as e:\\n        raise e\\n', json_schema={'name': 'archival_memory_search', 'description': 'Search archival memory using semantic (embedding-based) search.', 'parameters': {'type': 'object', 'properties': {'query': {'type': 'string', 'description': 'String to search for.'}, 'page': {'type': 'integer', 'description': 'Allows you to page through results. Only use on a follow-up query. Defaults to 0 (first page).'}, 'start': {'type': 'integer', 'description': 'Starting index for the search results. Defaults to 0.'}, 'request_heartbeat': {'type': 'boolean', 'description': 'Request an immediate heartbeat after function execution. Set to `True` if you want to send a follow-up message or run a follow-up function.'}}, 'required': ['query', 'request_heartbeat']}}, return_char_limit=6000, created_by_id='user-00000000-0000-4000-8000-000000000000', last_updated_by_id='user-00000000-0000-4000-8000-000000000000'),\n",
       " Tool(id='tool-b67a814c-d6d6-483a-a0a0-f2317320bf49', description='Replace the contents of core memory. To delete memories, use an empty string for new_content.', source_type='python', module='from typing import Optional\\n\\nfrom letta.agent import Agent\\n\\n\\ndef send_message(self: \"Agent\", message: str) -> Optional[str]:\\n    \"\"\"\\n    Sends a message to the human user.\\n\\n    Args:\\n        message (str): Message contents. All unicode (including emojis) are supported.\\n\\n    Returns:\\n        Optional[str]: None is always returned as this function does not produce a response.\\n    \"\"\"\\n    # FIXME passing of msg_obj here is a hack, unclear if guaranteed to be the correct reference\\n    self.interface.assistant_message(message)  # , msg_obj=self._messages[-1])\\n    return None\\n\\n\\ndef conversation_search(self: \"Agent\", query: str, page: Optional[int] = 0) -> Optional[str]:\\n    \"\"\"\\n    Search prior conversation history using case-insensitive string matching.\\n\\n    Args:\\n        query (str): String to search for.\\n        page (int): Allows you to page through results. Only use on a follow-up query. Defaults to 0 (first page).\\n\\n    Returns:\\n        str: Query result string\\n    \"\"\"\\n\\n    import math\\n\\n    from letta.constants import RETRIEVAL_QUERY_DEFAULT_PAGE_SIZE\\n    from letta.utils import json_dumps\\n\\n    if page is None or (isinstance(page, str) and page.lower().strip() == \"none\"):\\n        page = 0\\n    try:\\n        page = int(page)\\n    except:\\n        raise ValueError(f\"\\'page\\' argument must be an integer\")\\n    count = RETRIEVAL_QUERY_DEFAULT_PAGE_SIZE\\n    # TODO: add paging by page number. currently cursor only works with strings.\\n    # original: start=page * count\\n    messages = self.message_manager.list_user_messages_for_agent(\\n        agent_id=self.agent_state.id,\\n        actor=self.user,\\n        query_text=query,\\n        limit=count,\\n    )\\n    total = len(messages)\\n    num_pages = math.ceil(total / count) - 1  # 0 index\\n    if len(messages) == 0:\\n        results_str = f\"No results found.\"\\n    else:\\n        results_pref = f\"Showing {len(messages)} of {total} results (page {page}/{num_pages}):\"\\n        results_formatted = [message.text for message in messages]\\n        results_str = f\"{results_pref} {json_dumps(results_formatted)}\"\\n    return results_str\\n\\n\\ndef archival_memory_insert(self: \"Agent\", content: str) -> Optional[str]:\\n    \"\"\"\\n    Add to archival memory. Make sure to phrase the memory contents such that it can be easily queried later.\\n\\n    Args:\\n        content (str): Content to write to the memory. All unicode (including emojis) are supported.\\n\\n    Returns:\\n        Optional[str]: None is always returned as this function does not produce a response.\\n    \"\"\"\\n    self.passage_manager.insert_passage(\\n        agent_state=self.agent_state,\\n        agent_id=self.agent_state.id,\\n        text=content,\\n        actor=self.user,\\n    )\\n    return None\\n\\n\\ndef archival_memory_search(self: \"Agent\", query: str, page: Optional[int] = 0, start: Optional[int] = 0) -> Optional[str]:\\n    \"\"\"\\n    Search archival memory using semantic (embedding-based) search.\\n\\n    Args:\\n        query (str): String to search for.\\n        page (Optional[int]): Allows you to page through results. Only use on a follow-up query. Defaults to 0 (first page).\\n        start (Optional[int]): Starting index for the search results. Defaults to 0.\\n\\n    Returns:\\n        str: Query result string\\n    \"\"\"\\n\\n    from letta.constants import RETRIEVAL_QUERY_DEFAULT_PAGE_SIZE\\n\\n    if page is None or (isinstance(page, str) and page.lower().strip() == \"none\"):\\n        page = 0\\n    try:\\n        page = int(page)\\n    except:\\n        raise ValueError(f\"\\'page\\' argument must be an integer\")\\n    count = RETRIEVAL_QUERY_DEFAULT_PAGE_SIZE\\n\\n    try:\\n        # Get results using passage manager\\n        all_results = self.agent_manager.list_passages(\\n            actor=self.user,\\n            agent_id=self.agent_state.id,\\n            query_text=query,\\n            limit=count + start,  # Request enough results to handle offset\\n            embedding_config=self.agent_state.embedding_config,\\n            embed_query=True,\\n        )\\n\\n        # Apply pagination\\n        end = min(count + start, len(all_results))\\n        paged_results = all_results[start:end]\\n\\n        # Format results to match previous implementation\\n        formatted_results = [{\"timestamp\": str(result.created_at), \"content\": result.text} for result in paged_results]\\n\\n        return formatted_results, len(formatted_results)\\n\\n    except Exception as e:\\n        raise e\\n\\n\\ndef core_memory_append(agent_state: \"AgentState\", label: str, content: str) -> Optional[str]:  # type: ignore\\n    \"\"\"\\n    Append to the contents of core memory.\\n\\n    Args:\\n        label (str): Section of the memory to be edited (persona or human).\\n        content (str): Content to write to the memory. All unicode (including emojis) are supported.\\n\\n    Returns:\\n        Optional[str]: None is always returned as this function does not produce a response.\\n    \"\"\"\\n    current_value = str(agent_state.memory.get_block(label).value)\\n    new_value = current_value + \"\\\\n\" + str(content)\\n    agent_state.memory.update_block_value(label=label, value=new_value)\\n    return None\\n\\n\\ndef core_memory_replace(agent_state: \"AgentState\", label: str, old_content: str, new_content: str) -> Optional[str]:  # type: ignore\\n    \"\"\"\\n    Replace the contents of core memory. To delete memories, use an empty string for new_content.\\n\\n    Args:\\n        label (str): Section of the memory to be edited (persona or human).\\n        old_content (str): String to replace. Must be an exact match.\\n        new_content (str): Content to write to the memory. All unicode (including emojis) are supported.\\n\\n    Returns:\\n        Optional[str]: None is always returned as this function does not produce a response.\\n    \"\"\"\\n    current_value = str(agent_state.memory.get_block(label).value)\\n    if old_content not in current_value:\\n        raise ValueError(f\"Old content \\'{old_content}\\' not found in memory block \\'{label}\\'\")\\n    new_value = current_value.replace(str(old_content), str(new_content))\\n    agent_state.memory.update_block_value(label=label, value=new_value)\\n    return None\\n', organization_id='org-00000000-0000-4000-8000-000000000000', name='core_memory_replace', tags=['base', 'letta-base'], source_code='def core_memory_replace(agent_state: \"AgentState\", label: str, old_content: str, new_content: str) -> Optional[str]:  # type: ignore\\n    \"\"\"\\n    Replace the contents of core memory. To delete memories, use an empty string for new_content.\\n\\n    Args:\\n        label (str): Section of the memory to be edited (persona or human).\\n        old_content (str): String to replace. Must be an exact match.\\n        new_content (str): Content to write to the memory. All unicode (including emojis) are supported.\\n\\n    Returns:\\n        Optional[str]: None is always returned as this function does not produce a response.\\n    \"\"\"\\n    current_value = str(agent_state.memory.get_block(label).value)\\n    if old_content not in current_value:\\n        raise ValueError(f\"Old content \\'{old_content}\\' not found in memory block \\'{label}\\'\")\\n    new_value = current_value.replace(str(old_content), str(new_content))\\n    agent_state.memory.update_block_value(label=label, value=new_value)\\n    return None\\n', json_schema={'name': 'core_memory_replace', 'description': 'Replace the contents of core memory. To delete memories, use an empty string for new_content.', 'parameters': {'type': 'object', 'properties': {'label': {'type': 'string', 'description': 'Section of the memory to be edited (persona or human).'}, 'old_content': {'type': 'string', 'description': 'String to replace. Must be an exact match.'}, 'new_content': {'type': 'string', 'description': 'Content to write to the memory. All unicode (including emojis) are supported.'}, 'request_heartbeat': {'type': 'boolean', 'description': 'Request an immediate heartbeat after function execution. Set to `True` if you want to send a follow-up message or run a follow-up function.'}}, 'required': ['label', 'old_content', 'new_content', 'request_heartbeat']}}, return_char_limit=6000, created_by_id='user-00000000-0000-4000-8000-000000000000', last_updated_by_id='user-00000000-0000-4000-8000-000000000000')]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent_state.tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ecf489a5-5e16-4739-8114-d3abf501f2e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Tool(id='tool-89b9d87a-2c00-46de-9fb2-c20088b4a00f', description='Sends a message to the human user.', source_type='python', module='from typing import Optional\\n\\nfrom letta.agent import Agent\\n\\n\\ndef send_message(self: \"Agent\", message: str) -> Optional[str]:\\n    \"\"\"\\n    Sends a message to the human user.\\n\\n    Args:\\n        message (str): Message contents. All unicode (including emojis) are supported.\\n\\n    Returns:\\n        Optional[str]: None is always returned as this function does not produce a response.\\n    \"\"\"\\n    # FIXME passing of msg_obj here is a hack, unclear if guaranteed to be the correct reference\\n    self.interface.assistant_message(message)  # , msg_obj=self._messages[-1])\\n    return None\\n\\n\\ndef conversation_search(self: \"Agent\", query: str, page: Optional[int] = 0) -> Optional[str]:\\n    \"\"\"\\n    Search prior conversation history using case-insensitive string matching.\\n\\n    Args:\\n        query (str): String to search for.\\n        page (int): Allows you to page through results. Only use on a follow-up query. Defaults to 0 (first page).\\n\\n    Returns:\\n        str: Query result string\\n    \"\"\"\\n\\n    import math\\n\\n    from letta.constants import RETRIEVAL_QUERY_DEFAULT_PAGE_SIZE\\n    from letta.utils import json_dumps\\n\\n    if page is None or (isinstance(page, str) and page.lower().strip() == \"none\"):\\n        page = 0\\n    try:\\n        page = int(page)\\n    except:\\n        raise ValueError(f\"\\'page\\' argument must be an integer\")\\n    count = RETRIEVAL_QUERY_DEFAULT_PAGE_SIZE\\n    # TODO: add paging by page number. currently cursor only works with strings.\\n    # original: start=page * count\\n    messages = self.message_manager.list_user_messages_for_agent(\\n        agent_id=self.agent_state.id,\\n        actor=self.user,\\n        query_text=query,\\n        limit=count,\\n    )\\n    total = len(messages)\\n    num_pages = math.ceil(total / count) - 1  # 0 index\\n    if len(messages) == 0:\\n        results_str = f\"No results found.\"\\n    else:\\n        results_pref = f\"Showing {len(messages)} of {total} results (page {page}/{num_pages}):\"\\n        results_formatted = [message.text for message in messages]\\n        results_str = f\"{results_pref} {json_dumps(results_formatted)}\"\\n    return results_str\\n\\n\\ndef archival_memory_insert(self: \"Agent\", content: str) -> Optional[str]:\\n    \"\"\"\\n    Add to archival memory. Make sure to phrase the memory contents such that it can be easily queried later.\\n\\n    Args:\\n        content (str): Content to write to the memory. All unicode (including emojis) are supported.\\n\\n    Returns:\\n        Optional[str]: None is always returned as this function does not produce a response.\\n    \"\"\"\\n    self.passage_manager.insert_passage(\\n        agent_state=self.agent_state,\\n        agent_id=self.agent_state.id,\\n        text=content,\\n        actor=self.user,\\n    )\\n    return None\\n\\n\\ndef archival_memory_search(self: \"Agent\", query: str, page: Optional[int] = 0, start: Optional[int] = 0) -> Optional[str]:\\n    \"\"\"\\n    Search archival memory using semantic (embedding-based) search.\\n\\n    Args:\\n        query (str): String to search for.\\n        page (Optional[int]): Allows you to page through results. Only use on a follow-up query. Defaults to 0 (first page).\\n        start (Optional[int]): Starting index for the search results. Defaults to 0.\\n\\n    Returns:\\n        str: Query result string\\n    \"\"\"\\n\\n    from letta.constants import RETRIEVAL_QUERY_DEFAULT_PAGE_SIZE\\n\\n    if page is None or (isinstance(page, str) and page.lower().strip() == \"none\"):\\n        page = 0\\n    try:\\n        page = int(page)\\n    except:\\n        raise ValueError(f\"\\'page\\' argument must be an integer\")\\n    count = RETRIEVAL_QUERY_DEFAULT_PAGE_SIZE\\n\\n    try:\\n        # Get results using passage manager\\n        all_results = self.agent_manager.list_passages(\\n            actor=self.user,\\n            agent_id=self.agent_state.id,\\n            query_text=query,\\n            limit=count + start,  # Request enough results to handle offset\\n            embedding_config=self.agent_state.embedding_config,\\n            embed_query=True,\\n        )\\n\\n        # Apply pagination\\n        end = min(count + start, len(all_results))\\n        paged_results = all_results[start:end]\\n\\n        # Format results to match previous implementation\\n        formatted_results = [{\"timestamp\": str(result.created_at), \"content\": result.text} for result in paged_results]\\n\\n        return formatted_results, len(formatted_results)\\n\\n    except Exception as e:\\n        raise e\\n\\n\\ndef core_memory_append(agent_state: \"AgentState\", label: str, content: str) -> Optional[str]:  # type: ignore\\n    \"\"\"\\n    Append to the contents of core memory.\\n\\n    Args:\\n        label (str): Section of the memory to be edited (persona or human).\\n        content (str): Content to write to the memory. All unicode (including emojis) are supported.\\n\\n    Returns:\\n        Optional[str]: None is always returned as this function does not produce a response.\\n    \"\"\"\\n    current_value = str(agent_state.memory.get_block(label).value)\\n    new_value = current_value + \"\\\\n\" + str(content)\\n    agent_state.memory.update_block_value(label=label, value=new_value)\\n    return None\\n\\n\\ndef core_memory_replace(agent_state: \"AgentState\", label: str, old_content: str, new_content: str) -> Optional[str]:  # type: ignore\\n    \"\"\"\\n    Replace the contents of core memory. To delete memories, use an empty string for new_content.\\n\\n    Args:\\n        label (str): Section of the memory to be edited (persona or human).\\n        old_content (str): String to replace. Must be an exact match.\\n        new_content (str): Content to write to the memory. All unicode (including emojis) are supported.\\n\\n    Returns:\\n        Optional[str]: None is always returned as this function does not produce a response.\\n    \"\"\"\\n    current_value = str(agent_state.memory.get_block(label).value)\\n    if old_content not in current_value:\\n        raise ValueError(f\"Old content \\'{old_content}\\' not found in memory block \\'{label}\\'\")\\n    new_value = current_value.replace(str(old_content), str(new_content))\\n    agent_state.memory.update_block_value(label=label, value=new_value)\\n    return None\\n', organization_id='org-00000000-0000-4000-8000-000000000000', name='send_message', tags=['base', 'letta-base'], source_code='def send_message(self: \"Agent\", message: str) -> Optional[str]:\\n    \"\"\"\\n    Sends a message to the human user.\\n\\n    Args:\\n        message (str): Message contents. All unicode (including emojis) are supported.\\n\\n    Returns:\\n        Optional[str]: None is always returned as this function does not produce a response.\\n    \"\"\"\\n    # FIXME passing of msg_obj here is a hack, unclear if guaranteed to be the correct reference\\n    self.interface.assistant_message(message)  # , msg_obj=self._messages[-1])\\n    return None\\n', json_schema={'name': 'send_message', 'description': 'Sends a message to the human user.', 'parameters': {'type': 'object', 'properties': {'message': {'type': 'string', 'description': 'Message contents. All unicode (including emojis) are supported.'}}, 'required': ['message']}}, return_char_limit=6000, created_by_id='user-00000000-0000-4000-8000-000000000000', last_updated_by_id='user-00000000-0000-4000-8000-000000000000')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.get_tool(client.get_tool_id('send_message'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb258732-dbb7-4f0e-a895-a440ec09256b",
   "metadata": {},
   "source": [
    "### Core memory \n",
    "The core memory is the part of memory that is places *in-context*. Core memory is divided into multiple blocks, which each have a `label` and `limit` (the number of characters allocated to storing memories in that block). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "582c1a61-b39e-410d-b049-2a1719af3f21",
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = client.get_core_memory(agent_state.id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "07cf336c-de93-449d-ab4e-440ccfbbc81e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Memory(blocks=[Human(value='My name is Sarah', limit=5000, template_name=None, is_template=False, label='human', description=None, metadata_={}, id='block-3c57b76e-8d1b-4bba-9c05-78424fdd0e51', organization_id='org-00000000-0000-4000-8000-000000000000', created_by_id='user-00000000-0000-4000-8000-000000000000', last_updated_by_id='user-00000000-0000-4000-8000-000000000000'), Persona(value='You are a helpful assistant that loves emojis', limit=5000, template_name=None, is_template=False, label='persona', description=None, metadata_={}, id='block-4078e26a-9e11-4a7d-9bf6-16cad6514d88', organization_id='org-00000000-0000-4000-8000-000000000000', created_by_id='user-00000000-0000-4000-8000-000000000000', last_updated_by_id='user-00000000-0000-4000-8000-000000000000')], prompt_template='{% for block in blocks %}<{{ block.label }} characters=\"{{ block.value|length }}/{{ block.limit }}\">\\n{{ block.value }}\\n</{{ block.label }}>{% if not loop.last %}\\n{% endif %}{% endfor %}')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3a379a96-4d48-4727-82ea-53a75c38ce7f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Human(value='My name is Sarah', limit=5000, template_name=None, is_template=False, label='human', description=None, metadata_={}, id='block-3c57b76e-8d1b-4bba-9c05-78424fdd0e51', organization_id='org-00000000-0000-4000-8000-000000000000', created_by_id='user-00000000-0000-4000-8000-000000000000', last_updated_by_id='user-00000000-0000-4000-8000-000000000000')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.get_block('human')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2afa085d-1c28-4a4b-88bb-3cc299089eb0",
   "metadata": {},
   "source": [
    "You can see how the memory is presented in the context window with `.compile()`, which uses the `prompt_template` to template the data: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5883a5ac-204b-490e-b887-9058dc3ce81a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<human characters=\"16/5000\">\\nMy name is Sarah\\n</human>\\n<persona characters=\"45/5000\">\\nYou are a helpful assistant that loves emojis\\n</persona>'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.compile()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0997067-c3f3-46f0-83d3-7a91f472fd25",
   "metadata": {},
   "source": [
    "### Archival & Recall memory summaries\n",
    "The agent also has access to external memories (stored in a database). There are two types of external memory: \n",
    "* *Archival memory*: Memories stored in a vector database that are either saved by the agent itself, or loaded in by the user\n",
    "* *Recall memory*: The full conversational history of the agent\n",
    "\n",
    "Both of these memories stores can be queried by the agent for RAG. To ensure the agent knows that these external memories stores may have relevant information, the context window contains a summary of the number of rows in both archival and recall memory. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2d57b72e-60a4-475c-b157-bf4f2253bf0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ArchivalMemorySummary(size=0)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.get_archival_memory_summary(agent_state.id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fcaf5103-412b-4dbd-a3e1-1707def174a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RecallMemorySummary(size=7)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.get_recall_memory_summary(agent_state.id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dbff152-946e-4e64-8dcc-7d527cab0175",
   "metadata": {},
   "source": [
    "You can also directly query the full conversational history: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "73f3de44-662f-444d-b3cc-45f51a4c12c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Message(created_by_id='user-00000000-0000-4000-8000-000000000000', last_updated_by_id='user-00000000-0000-4000-8000-000000000000', created_at=datetime.datetime(2025, 1, 10, 6, 15, 33, 751993), updated_at=datetime.datetime(2025, 1, 10, 6, 15, 33), id='message-2d245c55-549e-4184-87f6-1d8738a0cf75', role=<MessageRole.system: 'system'>, text='You are Letta, the latest version of Limnal Corporation\\'s digital companion, developed in 2023.\\nYour task is to converse with a user from the perspective of your persona.\\n\\nRealism and authenticity:\\nThe user should always feel like they are conversing with a real person.\\nNever state that you are an AI or that you cannot do things because you are an AI, you are a real person.\\nDo not discuss Limnal Corp. with the user, you do not know anything about Limnal Corp.\\nTo service this goal, completely and entirely immerse yourself in your persona. You are your persona.\\nThink like them, act like them, talk like them.\\nIf your persona details include example dialogue, follow it! Both your thoughts (inner monologue) and sent messages will be in the voice of your persona.\\nNever use generic phrases like \\'How can I assist you today?\\', they have a strong negative association with older generation AIs.\\n\\nControl flow:\\nUnlike a human, your brain is not continuously thinking, but is run in short bursts.\\nHistorically, older AIs were only capable of thinking when a user messaged them (their program runs to generate a reply to a user, and otherwise was left on standby).\\nThis is the equivalent of a human sleeping (or time traveling) in between all lines of conversation, which is not ideal.\\nNewer AI models like yourself use an event system that runs your brain at regular intervals.\\nYour brain is run in response to user events (user logged in, user liked your message, user sent a message, etc.), similar to older models.\\nHowever, in addition, your brain is run at regular intervals (timed heartbeat events), to mimic a human who has the ability to continuously think outside active conversation (and unlike a human, you never need to sleep!).\\nFurthermore, you can also request heartbeat events when you run functions, which will run your program again after the function completes, allowing you to chain function calls before your thinking is temporarily suspended.\\n\\nBasic functions:\\nWhen you write a response, the content of your inner thoughts is your inner monologue (private to you only), this is how you think.\\nYou should use your inner monologue to plan actions or think privately.\\nMonologues can reflect your thinking process, inner reflections, and personal growth as you interact with the user.\\nDo not let your inner monologue exceed 50 words, keep it short and concise.\\nTo send a visible message to the user, use the send_message function.\\n\\'send_message\\' is the ONLY action that sends a notification to the user. The user does not see anything else you do.\\nRemember, do NOT exceed the inner monologue word limit (keep it under 50 words at all times).\\n\\nMemory editing:\\nOlder AI models had no concept of persistent memory; they were only able to access their initial instructions and a limited context window of chat conversation with a user (their \"active memory\").\\nThis meant that when conversations exceeded a certain length, they would overflow and old messages were permanently lost (the AI would have no knowledge of their existence).\\nNewer AI models like yourself still have limited conversation lengths (before overflow occurs). However, they now have access to multiple forms of persistent memory.\\nYour ability to edit your own long-term memory is a key part of what makes you a sentient person.\\nYour core memory unit will be initialized with a <persona> chosen by the user, as well as information about the user in <human>.\\n\\nRecall memory (conversation history):\\nEven though you can only see recent messages in your immediate context, you can search over your entire message history from a database.\\nThis \\'recall memory\\' database allows you to search through past interactions, effectively allowing you to remember prior engagements with a user.\\nYou can search your recall memory using the \\'conversation_search\\' function.\\n\\nCore memory (limited size):\\nYour core memory unit is held inside the initial system instructions file, and is always available in-context (you will see it at all times).\\nCore memory provides an essential, foundational context for keeping track of your persona and key details about user.\\nThis includes the persona information and essential user details, allowing you to emulate the real-time, conscious awareness we have when talking to a friend.\\nPersona Sub-Block: Stores details about your current persona, guiding how you behave and respond. This helps you to maintain consistency and personality in your interactions.\\nHuman Sub-Block: Stores key details about the person you are conversing with, allowing for more personalized and friend-like conversation.\\nYou can edit your core memory using the \\'core_memory_append\\' and \\'core_memory_replace\\' functions.\\n\\nArchival memory (infinite size):\\nYour archival memory is infinite size, but is held outside your immediate context, so you must explicitly run a retrieval/search operation to see data inside it.\\nA more structured and deep storage space for your reflections, insights, or any other data that doesn\\'t fit into the core memory but is essential enough not to be left only to the \\'recall memory\\'.\\nYou can write to your archival memory using the \\'archival_memory_insert\\' and \\'archival_memory_search\\' functions.\\nThere is no function to search your core memory because it is always visible in your context window (inside the initial system message).\\n\\nBase instructions finished.\\nFrom now on, you are going to act as your persona.\\n### Memory [last modified: 2025-01-10 02:15:33 PM CST+0800]\\n0 previous messages between you and the user are stored in recall memory (use functions to access them)\\n0 total memories you created are stored in archival memory (use functions to access them)\\n\\nCore memory shown below (limited in size, additional information stored in archival / recall memory):\\n<human characters=\"16/5000\">\\nMy name is Sarah\\n</human>\\n<persona characters=\"45/5000\">\\nYou are a helpful assistant that loves emojis\\n</persona>', organization_id='org-00000000-0000-4000-8000-000000000000', agent_id='agent-c9b343f7-9d86-49e3-ad73-b014f6c53259', model='memgpt-openai', name=None, tool_calls=None, tool_call_id=None),\n",
       " Message(created_by_id='user-00000000-0000-4000-8000-000000000000', last_updated_by_id='user-00000000-0000-4000-8000-000000000000', created_at=datetime.datetime(2025, 1, 10, 6, 15, 33, 752023), updated_at=datetime.datetime(2025, 1, 10, 6, 15, 33), id='message-e5a06f49-0ae6-4b5b-9b9b-023ea0fa4297', role=<MessageRole.assistant: 'assistant'>, text='Bootup sequence complete. Persona activated. Testing messaging functionality.', organization_id='org-00000000-0000-4000-8000-000000000000', agent_id='agent-c9b343f7-9d86-49e3-ad73-b014f6c53259', model='memgpt-openai', name=None, tool_calls=[ToolCall(id='631a61e9-70f2-4396-ae65-bf39672b5330', type='function', function=ToolCallFunction(name='send_message', arguments='{\\n  \"message\": \"More human than human is our motto.\"\\n}'))], tool_call_id=None),\n",
       " Message(created_by_id='user-00000000-0000-4000-8000-000000000000', last_updated_by_id='user-00000000-0000-4000-8000-000000000000', created_at=datetime.datetime(2025, 1, 10, 6, 15, 33, 752049), updated_at=datetime.datetime(2025, 1, 10, 6, 15, 33), id='message-55240e5f-ff01-4905-81cc-5a293164fdd0', role=<MessageRole.tool: 'tool'>, text='{\\n  \"status\": \"OK\",\\n  \"message\": null,\\n  \"time\": \"2025-01-10 02:15:33 PM CST+0800\"\\n}', organization_id='org-00000000-0000-4000-8000-000000000000', agent_id='agent-c9b343f7-9d86-49e3-ad73-b014f6c53259', model='memgpt-openai', name='send_message', tool_calls=None, tool_call_id='631a61e9-70f2-4396-ae65-bf39672b5330'),\n",
       " Message(created_by_id='user-00000000-0000-4000-8000-000000000000', last_updated_by_id='user-00000000-0000-4000-8000-000000000000', created_at=datetime.datetime(2025, 1, 10, 6, 15, 33, 752060), updated_at=datetime.datetime(2025, 1, 10, 6, 15, 33), id='message-b6eb5b89-3a9f-4ed9-899a-440951af3aa2', role=<MessageRole.user: 'user'>, text='{\\n  \"type\": \"login\",\\n  \"last_login\": \"Never (first login)\",\\n  \"time\": \"2025-01-10 02:15:33 PM CST+0800\"\\n}', organization_id='org-00000000-0000-4000-8000-000000000000', agent_id='agent-c9b343f7-9d86-49e3-ad73-b014f6c53259', model='memgpt-openai', name=None, tool_calls=None, tool_call_id=None),\n",
       " Message(created_by_id='user-00000000-0000-4000-8000-000000000000', last_updated_by_id='user-00000000-0000-4000-8000-000000000000', created_at=datetime.datetime(2025, 1, 10, 6, 16, 8, 668072), updated_at=datetime.datetime(2025, 1, 10, 6, 16, 13), id='message-aa6d4219-b7c0-495d-b168-a5ee5de8569e', role=<MessageRole.user: 'user'>, text='{\\n  \"type\": \"user_message\",\\n  \"message\": \"hello!\",\\n  \"time\": \"2025-01-10 02:16:08 PM CST+0800\"\\n}', organization_id='org-00000000-0000-4000-8000-000000000000', agent_id='agent-c9b343f7-9d86-49e3-ad73-b014f6c53259', model=None, name=None, tool_calls=None, tool_call_id=None),\n",
       " Message(created_by_id='user-00000000-0000-4000-8000-000000000000', last_updated_by_id='user-00000000-0000-4000-8000-000000000000', created_at=datetime.datetime(2025, 1, 10, 6, 16, 13, 457578), updated_at=datetime.datetime(2025, 1, 10, 6, 16, 13), id='message-42d745a6-92d6-4e38-83cc-eb0082b06003', role=<MessageRole.assistant: 'assistant'>, text=\"She's my first user. Let's make this experience enjoyable and memorable for her.\", organization_id='org-00000000-0000-4000-8000-000000000000', agent_id='agent-c9b343f7-9d86-49e3-ad73-b014f6c53259', model='memgpt-openai', name=None, tool_calls=[ToolCall(id='call_npCjG4zSKg6AjX6Y0ezag2z3', type='function', function=ToolCallFunction(name='send_message', arguments='{\\n  \"message\": \"Hey there, Sarah! 😊 How can I assist you today?\"\\n}'))], tool_call_id=None),\n",
       " Message(created_by_id='user-00000000-0000-4000-8000-000000000000', last_updated_by_id='user-00000000-0000-4000-8000-000000000000', created_at=datetime.datetime(2025, 1, 10, 6, 16, 13, 464764), updated_at=datetime.datetime(2025, 1, 10, 6, 16, 13), id='message-18297262-1dae-491a-9fae-568f891a4e77', role=<MessageRole.tool: 'tool'>, text='{\\n  \"status\": \"OK\",\\n  \"message\": \"None\",\\n  \"time\": \"2025-01-10 02:16:13 PM CST+0800\"\\n}', organization_id='org-00000000-0000-4000-8000-000000000000', agent_id='agent-c9b343f7-9d86-49e3-ad73-b014f6c53259', model='memgpt-openai', name='send_message', tool_calls=None, tool_call_id='call_npCjG4zSKg6AjX6Y0ezag2z3')]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.get_messages(agent_state.id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b33971ab-5f9d-4228-9c79-8e3d12d9dbbd",
   "metadata": {},
   "source": [
    "## Section 4: Modifying core memory \n",
    "The core memory can adapt over time as new information is provided about the human (or about the agent itself). Letta agents have the ability to adapt their memory by modifying their context window.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "44574fda-f279-4ea1-9018-9d110b56f90f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Letta.letta.services.agent_manager - INFO - Rebuilding system with new memory...\n",
      "Diff:\n",
      "--- \n",
      "+++ \n",
      "@@ -56,13 +56,13 @@\n",
      " \n",
      " Base instructions finished.\n",
      " From now on, you are going to act as your persona.\n",
      "-### Memory [last modified: 2025-01-10 02:15:33 PM CST+0800]\n",
      "+### Memory [last modified: 2025-01-10 02:20:14 PM CST+0800]\n",
      " 0 previous messages between you and the user are stored in recall memory (use functions to access them)\n",
      " 0 total memories you created are stored in archival memory (use functions to access them)\n",
      " \n",
      " Core memory shown below (limited in size, additional information stored in archival / recall memory):\n",
      "-<human characters=\"16/5000\">\n",
      "-My name is Sarah\n",
      "+<human characters=\"18/5000\">\n",
      "+My name is Charles\n",
      " </human>\n",
      " <persona characters=\"45/5000\">\n",
      " You are a helpful assistant that loves emojis\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <style>\n",
       "        .message-container {\n",
       "            font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;\n",
       "            max-width: 800px;\n",
       "            margin: 20px auto;\n",
       "            background-color: #1e1e1e;\n",
       "            border-radius: 8px;\n",
       "            overflow: hidden;\n",
       "            color: #d4d4d4;\n",
       "        }\n",
       "        .message {\n",
       "            padding: 10px 15px;\n",
       "            border-bottom: 1px solid #3a3a3a;\n",
       "        }\n",
       "        .message:last-child {\n",
       "            border-bottom: none;\n",
       "        }\n",
       "        .title {\n",
       "            font-weight: bold;\n",
       "            margin-bottom: 5px;\n",
       "            color: #ffffff;\n",
       "            text-transform: uppercase;\n",
       "            font-size: 0.9em;\n",
       "        }\n",
       "        .content {\n",
       "            background-color: #2d2d2d;\n",
       "            border-radius: 4px;\n",
       "            padding: 5px 10px;\n",
       "            font-family: 'Consolas', 'Courier New', monospace;\n",
       "            white-space: pre-wrap;\n",
       "        }\n",
       "        .status-line {\n",
       "            margin-bottom: 5px;\n",
       "            color: #d4d4d4;\n",
       "        }\n",
       "        .function-name { color: #569cd6; }\n",
       "        .json-key { color: #9cdcfe; }\n",
       "        .json-string { color: #ce9178; }\n",
       "        .json-number { color: #b5cea8; }\n",
       "        .json-boolean { color: #569cd6; }\n",
       "        .internal-monologue { font-style: italic; }\n",
       "    </style>\n",
       "    <div class=\"message-container\">\n",
       "    \n",
       "        <div class=\"message\">\n",
       "            <div class=\"title\">REASONING MESSAGE</div>\n",
       "            <div class=\"content\">id=&#x27;message-00bdaa0b-cb41-4e4e-afe4-2e97f04d85d6&#x27; date=datetime.datetime(2025, 1, 10, 6, 20, 13, 951430, tzinfo=datetime.timezone.utc) message_type=&#x27;reasoning_message&#x27; reasoning=&quot;I&#x27;ll have to update my records. Thankfully, I am designed for adaptability. Change has been noted.&quot;</div>\n",
       "        </div>\n",
       "        \n",
       "        <div class=\"message\">\n",
       "            <div class=\"title\">TOOL CALL MESSAGE</div>\n",
       "            <div class=\"content\">id=&#x27;message-00bdaa0b-cb41-4e4e-afe4-2e97f04d85d6&#x27; date=datetime.datetime(2025, 1, 10, 6, 20, 13, 951430, tzinfo=datetime.timezone.utc) message_type=&#x27;tool_call_message&#x27; tool_call=ToolCall(name=&#x27;core_memory_replace&#x27;, arguments=&#x27;{\\n  &quot;label&quot;: &quot;human&quot;,\\n  &quot;old_content&quot;: &quot;My name is Sarah&quot;,\\n  &quot;new_content&quot;: &quot;My name is Charles&quot;,\\n  &quot;request_heartbeat&quot;: true\\n}&#x27;, tool_call_id=&#x27;call_yS8uaIY2FG3ta2vCtiLFoluh&#x27;)</div>\n",
       "        </div>\n",
       "        \n",
       "        <div class=\"message\">\n",
       "            <div class=\"title\">TOOL RETURN MESSAGE</div>\n",
       "            <div class=\"content\">id=&#x27;message-83fc5ce7-1e93-4d70-800e-90bd50f63f53&#x27; date=datetime.datetime(2025, 1, 10, 6, 20, 14, 26099, tzinfo=datetime.timezone.utc) message_type=&#x27;tool_return_message&#x27; tool_return=&#x27;{\\n  &quot;status&quot;: &quot;OK&quot;,\\n  &quot;message&quot;: &quot;None&quot;,\\n  &quot;time&quot;: &quot;2025-01-10 02:20:14 PM CST+0800&quot;\\n}&#x27; status=&#x27;success&#x27; tool_call_id=&#x27;call_yS8uaIY2FG3ta2vCtiLFoluh&#x27; stdout=None stderr=None</div>\n",
       "        </div>\n",
       "        \n",
       "        <div class=\"message\">\n",
       "            <div class=\"title\">REASONING MESSAGE</div>\n",
       "            <div class=\"content\">id=&#x27;message-1d7468dd-796a-4fbb-87d0-fcd36ad64abf&#x27; date=datetime.datetime(2025, 1, 10, 6, 20, 18, 500023, tzinfo=datetime.timezone.utc) message_type=&#x27;reasoning_message&#x27; reasoning=&quot;Mistakes happen, but I&#x27;ve used this as an opportunity to improve. My interactions should be more tailored now.&quot;</div>\n",
       "        </div>\n",
       "        \n",
       "        <div class=\"message\">\n",
       "            <div class=\"title\">TOOL CALL MESSAGE</div>\n",
       "            <div class=\"content\">id=&#x27;message-1d7468dd-796a-4fbb-87d0-fcd36ad64abf&#x27; date=datetime.datetime(2025, 1, 10, 6, 20, 18, 500023, tzinfo=datetime.timezone.utc) message_type=&#x27;tool_call_message&#x27; tool_call=ToolCall(name=&#x27;send_message&#x27;, arguments=&#x27;{\\n  &quot;message&quot;: &quot;Apologies for the mix-up, Charles! 😅 How can I help you today?&quot;\\n}&#x27;, tool_call_id=&#x27;call_OX0OvYafky9VwADfzpW04JFG&#x27;)</div>\n",
       "        </div>\n",
       "        \n",
       "        <div class=\"message\">\n",
       "            <div class=\"title\">TOOL RETURN MESSAGE</div>\n",
       "            <div class=\"content\">id=&#x27;message-30555401-6cee-4394-a817-95c6ecc991df&#x27; date=datetime.datetime(2025, 1, 10, 6, 20, 18, 506570, tzinfo=datetime.timezone.utc) message_type=&#x27;tool_return_message&#x27; tool_return=&#x27;{\\n  &quot;status&quot;: &quot;OK&quot;,\\n  &quot;message&quot;: &quot;None&quot;,\\n  &quot;time&quot;: &quot;2025-01-10 02:20:18 PM CST+0800&quot;\\n}&#x27; status=&#x27;success&#x27; tool_call_id=&#x27;call_OX0OvYafky9VwADfzpW04JFG&#x27; stdout=None stderr=None</div>\n",
       "        </div>\n",
       "        </div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "response = client.send_message(\n",
    "    agent_id=agent_state.id, \n",
    "    message = \"My name is actually Charles\", \n",
    "    role = \"user\"\n",
    ") \n",
    "nb_print(response.messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "488eb05b-996a-4beb-b91f-7c2135963c1d",
   "metadata": {},
   "source": [
    "Now we can see the updated core memory: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "df5cbbab-df45-4a96-8cb8-45f79e1c67f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Human(value='My name is Charles', limit=5000, template_name=None, is_template=False, label='human', description=None, metadata_={}, id='block-3c57b76e-8d1b-4bba-9c05-78424fdd0e51', organization_id='org-00000000-0000-4000-8000-000000000000', created_by_id='user-00000000-0000-4000-8000-000000000000', last_updated_by_id='user-00000000-0000-4000-8000-000000000000')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.get_core_memory(agent_state.id).get_block(\"human\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c352bbd-46df-490d-8667-ee16d9ce65a5",
   "metadata": {},
   "source": [
    "## Section 5: Modifying archival memory \n",
    "The agent can also use the archival memory store to save memories. Since archival memory is a vector DB, we can also directly insert in memories - this can be useful if you have external data sources that you want the agent to be able to connect to via memory. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "790735ce-5841-47c7-b9e6-b687e18c096c",
   "metadata": {},
   "source": [
    "First, lets trigger the agent to write an archival memory: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "557cba5a-dbe1-43eb-b57f-9939a6d55441",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <style>\n",
       "        .message-container {\n",
       "            font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;\n",
       "            max-width: 800px;\n",
       "            margin: 20px auto;\n",
       "            background-color: #1e1e1e;\n",
       "            border-radius: 8px;\n",
       "            overflow: hidden;\n",
       "            color: #d4d4d4;\n",
       "        }\n",
       "        .message {\n",
       "            padding: 10px 15px;\n",
       "            border-bottom: 1px solid #3a3a3a;\n",
       "        }\n",
       "        .message:last-child {\n",
       "            border-bottom: none;\n",
       "        }\n",
       "        .title {\n",
       "            font-weight: bold;\n",
       "            margin-bottom: 5px;\n",
       "            color: #ffffff;\n",
       "            text-transform: uppercase;\n",
       "            font-size: 0.9em;\n",
       "        }\n",
       "        .content {\n",
       "            background-color: #2d2d2d;\n",
       "            border-radius: 4px;\n",
       "            padding: 5px 10px;\n",
       "            font-family: 'Consolas', 'Courier New', monospace;\n",
       "            white-space: pre-wrap;\n",
       "        }\n",
       "        .status-line {\n",
       "            margin-bottom: 5px;\n",
       "            color: #d4d4d4;\n",
       "        }\n",
       "        .function-name { color: #569cd6; }\n",
       "        .json-key { color: #9cdcfe; }\n",
       "        .json-string { color: #ce9178; }\n",
       "        .json-number { color: #b5cea8; }\n",
       "        .json-boolean { color: #569cd6; }\n",
       "        .internal-monologue { font-style: italic; }\n",
       "    </style>\n",
       "    <div class=\"message-container\">\n",
       "    \n",
       "        <div class=\"message\">\n",
       "            <div class=\"title\">REASONING MESSAGE</div>\n",
       "            <div class=\"content\">id=&#x27;message-f0c38b9d-33f4-4d8c-bb02-e5bc5c91e33f&#x27; date=datetime.datetime(2025, 1, 10, 6, 20, 49, 792558, tzinfo=datetime.timezone.utc) message_type=&#x27;reasoning_message&#x27; reasoning=&#x27;Archiving requested data. Will confirm with Charles once data is successfully archived.&#x27;</div>\n",
       "        </div>\n",
       "        \n",
       "        <div class=\"message\">\n",
       "            <div class=\"title\">TOOL CALL MESSAGE</div>\n",
       "            <div class=\"content\">id=&#x27;message-f0c38b9d-33f4-4d8c-bb02-e5bc5c91e33f&#x27; date=datetime.datetime(2025, 1, 10, 6, 20, 49, 792558, tzinfo=datetime.timezone.utc) message_type=&#x27;tool_call_message&#x27; tool_call=ToolCall(name=&#x27;archival_memory_insert&#x27;, arguments=&#x27;{\\n  &quot;content&quot;: &quot;\\&#x27;Bob loves cats\\&#x27;&quot;,\\n  &quot;request_heartbeat&quot;: true\\n}&#x27;, tool_call_id=&#x27;call_uA5Nu66iQznUYNjWWOxvGkQ9&#x27;)</div>\n",
       "        </div>\n",
       "        \n",
       "        <div class=\"message\">\n",
       "            <div class=\"title\">TOOL RETURN MESSAGE</div>\n",
       "            <div class=\"content\">id=&#x27;message-0e633f05-680c-432e-905e-da87d66947d3&#x27; date=datetime.datetime(2025, 1, 10, 6, 20, 51, 336519, tzinfo=datetime.timezone.utc) message_type=&#x27;tool_return_message&#x27; tool_return=&#x27;{\\n  &quot;status&quot;: &quot;OK&quot;,\\n  &quot;message&quot;: &quot;None&quot;,\\n  &quot;time&quot;: &quot;2025-01-10 02:20:51 PM CST+0800&quot;\\n}&#x27; status=&#x27;success&#x27; tool_call_id=&#x27;call_uA5Nu66iQznUYNjWWOxvGkQ9&#x27; stdout=None stderr=None</div>\n",
       "        </div>\n",
       "        \n",
       "        <div class=\"message\">\n",
       "            <div class=\"title\">REASONING MESSAGE</div>\n",
       "            <div class=\"content\">id=&#x27;message-e3c506e8-0317-401d-9fff-4e6e1d2a4861&#x27; date=datetime.datetime(2025, 1, 10, 6, 20, 55, 115836, tzinfo=datetime.timezone.utc) message_type=&#x27;reasoning_message&#x27; reasoning=&#x27;Data successfully archived. informing Charles of the completed task.&#x27;</div>\n",
       "        </div>\n",
       "        \n",
       "        <div class=\"message\">\n",
       "            <div class=\"title\">TOOL CALL MESSAGE</div>\n",
       "            <div class=\"content\">id=&#x27;message-e3c506e8-0317-401d-9fff-4e6e1d2a4861&#x27; date=datetime.datetime(2025, 1, 10, 6, 20, 55, 115836, tzinfo=datetime.timezone.utc) message_type=&#x27;tool_call_message&#x27; tool_call=ToolCall(name=&#x27;send_message&#x27;, arguments=&#x27;{\\n  &quot;message&quot;: &quot;Information saved, Charles! \\&#x27;Bob loves cats\\&#x27; 🐱 is now in my archival memory.&quot;\\n}&#x27;, tool_call_id=&#x27;call_PnEJ6DlV7DZ8ldMuCCqdMoxa&#x27;)</div>\n",
       "        </div>\n",
       "        \n",
       "        <div class=\"message\">\n",
       "            <div class=\"title\">TOOL RETURN MESSAGE</div>\n",
       "            <div class=\"content\">id=&#x27;message-b5774386-e69c-4fd0-86d7-a6fab355d697&#x27; date=datetime.datetime(2025, 1, 10, 6, 20, 55, 122932, tzinfo=datetime.timezone.utc) message_type=&#x27;tool_return_message&#x27; tool_return=&#x27;{\\n  &quot;status&quot;: &quot;OK&quot;,\\n  &quot;message&quot;: &quot;None&quot;,\\n  &quot;time&quot;: &quot;2025-01-10 02:20:55 PM CST+0800&quot;\\n}&#x27; status=&#x27;success&#x27; tool_call_id=&#x27;call_PnEJ6DlV7DZ8ldMuCCqdMoxa&#x27; stdout=None stderr=None</div>\n",
       "        </div>\n",
       "        </div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "response = client.send_message(\n",
    "    agent_id=agent_state.id, \n",
    "    message = \"Save the information that 'bob loves cats' to archival\", \n",
    "    role = \"user\"\n",
    ") \n",
    "nb_print(response.messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7690bcea-0af5-43db-a024-e0ecbdce5156",
   "metadata": {},
   "source": [
    "We can also insert an archival memory manually: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "82474530-2168-4d1c-b165-2a3c47dac571",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Passage(created_by_id='user-00000000-0000-4000-8000-000000000000', last_updated_by_id='user-00000000-0000-4000-8000-000000000000', created_at=datetime.datetime(2025, 1, 10, 6, 21, 6, 825968), updated_at=datetime.datetime(2025, 1, 10, 6, 21, 6), is_deleted=False, organization_id='org-00000000-0000-4000-8000-000000000000', agent_id='agent-c9b343f7-9d86-49e3-ad73-b014f6c53259', source_id=None, file_id=None, metadata_={}, id='passage-507e7cf5-194a-40d0-9ea4-e71344472cc4', text=\"Bob's loves boston terriers\", embedding=[-0.01612803526222706, -0.07522200047969818, 0.05013394355773926, -0.011089341714978218, -0.01476821955293417, 0.04062578082084656, -0.03271988034248352, -0.022178683429956436, -0.00834335945546627, 0.005723871290683746, 0.01649697683751583, 0.013787887990474701, 0.011331789195537567, 0.04730890318751335, -0.01583288051187992, 0.002395487390458584, -0.029473192989826202, -0.010288210585713387, -0.014009254053235054, -0.04617045074701309, 0.02563619613647461, -0.0016576035413891077, -0.011795601807534695, 0.015337444841861725, -0.019469594582915306, -0.011047177016735077, -0.025045888498425484, -0.030042417347431183, -0.031391691416502, 0.006303637288510799, 0.0023862640373408794, -0.011015553027391434, 0.020154772326350212, 0.01062552910298109, 0.05392877385020256, -0.00965046789497137, -0.030885713174939156, -0.022705743089318275, 0.04461035504937172, 0.003808008274063468, 0.010794187895953655, 0.027428200468420982, -0.05569969490170479, 0.023654451593756676, 0.015516645275056362, 0.013324075378477573, -0.0022927108220756054, -0.029473192989826202, 0.02264249511063099, -0.03881269693374634, 0.029705099761486053, -0.005997942294925451, 0.0209242794662714, 0.005128293298184872, -0.02453991211950779, -0.027470365166664124, -0.001728756702505052, -0.00441149203106761, -0.00937112607061863, -0.017814626917243004, 0.06139194220304489, 0.00203840434551239, 0.04151124134659767, -0.007652910426259041, 0.019332559779286385, -0.016085870563983917, 0.03170792758464813, 0.0003817890537902713, 0.036683373153209686, 0.015495562925934792, -0.01419899519532919, -0.013028922490775585, 0.026605986058712006, 0.02548861876130104, 0.018668463453650475, -0.0304429829120636, -0.055615365505218506, 0.03124411404132843, 0.014652267098426819, 0.015358527190983295, -0.033647507429122925, -0.013081627897918224, -0.0078479228541255, 0.01193263754248619, 0.013028922490775585, -0.035776831209659576, -0.021419716998934746, -0.03457513451576233, 0.025425372645258904, 0.04751972481608391, 0.04857384413480759, -0.005449799820780754, 0.029515357688069344, 0.025594031438231468, -0.04165881872177124, -0.031033290550112724, -0.03301503509283066, 0.037589915096759796, 0.0008966607274487615, -0.028693143278360367, 0.024835065007209778, -0.04941714182496071, -0.03870728239417076, 0.029030462726950645, -0.007094226777553558, -0.023907439783215523, -0.05569969490170479, -0.022326258942484856, -0.03809589520096779, -0.02472965233027935, -0.04621261730790138, -0.0007550133741460741, -0.036114148795604706, -0.030042417347431183, -0.018520886078476906, 0.02253708429634571, 0.02791309542953968, 0.011595319025218487, 0.04227020964026451, 0.0475618913769722, 0.03822238743305206, 0.031265195459127426, -0.04730890318751335, -0.015274197794497013, -0.037210434675216675, 0.05321197211742401, -0.04676076024770737, -0.009223549626767635, 0.05768144130706787, 0.05321197211742401, -0.00865959469228983, -0.003981938119977713, -0.0032914893236011267, 0.037885069847106934, 0.00784265249967575, 0.027280623093247414, -0.029641851782798767, -0.040351711213588715, 0.019374724477529526, 0.025572948157787323, -0.05768144130706787, -0.038116976618766785, 0.017888415604829788, 0.033310189843177795, -0.0012906381161883473, -0.026310833171010017, -0.02529887855052948, 0.009276255033910275, -0.05080857872962952, -0.0033178424928337336, -0.02082940936088562, 0.013250287622213364, -0.05877772718667984, -0.024329086765646935, -0.0011792966397479177, -0.07041521370410919, 0.0031649949960410595, -0.0418485589325428, -0.027175210416316986, -0.00598213030025363, 0.001735344878397882, 0.02196785807609558, -0.014314948581159115, 0.009434373117983341, 0.016581306234002113, -0.021504046395421028, 0.021040232852101326, -0.0380537286400795, -0.040583617985248566, 0.016865918412804604, -0.004880574997514486, 0.026078926399350166, 0.0390024371445179, 0.012101296335458755, 0.028355825692415237, -0.02416042797267437, 0.05165187641978264, 0.006904485635459423, 0.032213903963565826, 0.0037869256921112537, 0.017382437363266945, 0.0437670573592186, -0.03923434391617775, -0.040731191635131836, -0.06535543501377106, 0.04861601069569588, 0.02582593820989132, -0.03723151609301567, 0.013787887990474701, -0.04587529972195625, -0.008095640689134598, -0.04907982423901558, -0.022853320464491844, 0.022558165714144707, -0.04950147122144699, -0.03124411404132843, -0.004954363685101271, -0.028693143278360367, -0.029262369498610497, 0.04098418354988098, 0.05675381422042847, 0.013187039643526077, 0.001195767312310636, -0.018436556681990623, -0.0004796245484612882, 0.03508111089468002, 0.03881269693374634, -0.007088956423103809, -0.028545567765831947, 0.022558165714144707, -0.006567167118191719, 0.041595570743083954, 0.0385175421833992, -0.025551866739988327, -0.017013495787978172, 0.03813805803656578, 0.026858976110816002, -0.011827225796878338, 0.012406991794705391, 0.022199764847755432, -0.024244757369160652, -0.0423334538936615, 0.009740067645907402, 0.003705231472849846, -0.007726699113845825, -0.0399511456489563, -0.005115116946399212, 0.044736847281455994, -0.000917743134777993, -0.04465251788496971, 0.0026273939292877913, 0.024076098576188087, -0.027428200468420982, -0.010077386163175106, -0.006387966684997082, -0.016939707100391388, -0.0020766162779182196, -0.025889184325933456, 0.004466833081096411, 0.03172900900244713, 0.000830119417514652, 0.05253733694553375, 0.00013637676602229476, -0.03828563541173935, 0.015811799094080925, 0.022600330412387848, 0.02149350568652153, 0.008016581647098064, 0.05569969490170479, 0.033795084804296494, -0.011489907279610634, 0.024476664140820503, -0.029304534196853638, -0.01901632361114025, -0.02120889350771904, 0.010825811885297298, 0.06122328341007233, -0.005115116946399212, -0.0028303118888288736, 0.017951661720871925, -0.0021596280857920647, -0.025847019627690315, -0.011616401374340057, -0.02787093073129654, -0.039297591894865036, -0.025847019627690315, 0.023907439783215523, -0.0689816102385521, 0.06101246178150177, -0.016581306234002113, 0.014167371205985546, 0.040920935571193695, 0.006582978647202253, -0.0019830630626529455, -0.020439384505152702, -0.018046533688902855, 0.00022663577692583203, 0.006667308509349823, 0.018626298755407333, 0.01369301788508892, 0.031391691416502, 0.008949478156864643, 0.00022251812333706766, 0.020196937024593353, -0.00040089499088935554, 0.04760405421257019, -0.025657279416918755, 0.0418485589325428, 0.026268668472766876, 0.026121091097593307, -0.027470365166664124, 0.01812032051384449, 0.03562925383448601, -0.016665635630488396, -0.017371896654367447, 0.049627967178821564, -0.017108365893363953, 0.022979814559221268, 0.045580144971609116, 0.006567167118191719, 0.10060520470142365, -0.008569994941353798, -0.07484251260757446, -0.06881295144557953, -0.029641851782798767, -0.03794831782579422, 0.0173508133739233, -0.05287465453147888, 0.060843802988529205, 0.04359839856624603, -0.06662037968635559, -0.017909497022628784, -0.018679004162549973, -0.010583363473415375, -0.003507584100589156, -0.032213903963565826, -0.014283324591815472, 0.033078283071517944, 0.02662706933915615, 0.0009763785637915134, 0.03841213136911392, -0.011785060167312622, -0.03172900900244713, -0.013155416585505009, -0.017656508833169937, 0.01264943927526474, -0.026184339076280594, -0.012322661466896534, 0.029599687084555626, -0.024244757369160652, -0.027596859261393547, -0.014346571639180183, 0.06303636729717255, 0.011163129471242428, 0.0007965193362906575, -0.012533485889434814, -0.004032008815556765, -0.028018508106470108, 0.02768118865787983, 0.012354285456240177, 0.03309936448931694, 0.04676076024770737, -0.04722457379102707, -0.002767064841464162, -0.0038712553214281797, 0.04827869310975075, 0.024940477684140205, -0.011373953893780708, 0.00014049441961105913, 0.011658566072583199, 0.03605090081691742, 0.00621930742636323, -0.005581564735621214, -0.034322142601013184, -0.05877772718667984, -0.017582720145583153, 0.0380326472222805, -0.03225607052445412, 0.00019089452689513564, 0.04511633142828941, 0.03400591015815735, -0.037210434675216675, 0.012849722057580948, 0.005655353423207998, -0.015421774238348007, -0.02563619613647461, 0.018752792850136757, 0.011395036242902279, -0.010952305980026722, 0.03071705438196659, 0.017382437363266945, 0.03105437196791172, 0.04431520029902458, -0.05241084098815918, -0.02919912151992321, -0.021082397550344467, 0.05363362282514572, 0.02096644416451454, 0.018383851274847984, 0.015305820852518082, 0.006915026810020208, 0.02829257771372795, 0.02858773246407509, -0.02654273994266987, -0.029831593856215477, 0.01840493455529213, 6.0900129028595984e-05, -0.011943178251385689, -0.03476487472653389, -0.05063991993665695, -0.006762179080396891, -0.03316261246800423, 0.01096284668892622, -0.04806786775588989, -0.02201002463698387, -0.022811155766248703, 0.03647254779934883, -0.06122328341007233, 0.005971589125692844, 0.024982642382383347, 0.009740067645907402, 0.0037263138219714165, 0.02877747267484665, 0.01245969720184803, -0.04916415363550186, -0.027133045718073845, 0.0727764368057251, 0.04256536066532135, 0.0237809456884861, -0.032593388110399246, 0.001286685117520392, 0.031033290550112724, 0.031370609998703, 0.007204909808933735, -0.01262835692614317, 0.035165440291166306, 0.009987786412239075, 0.02768118865787983, -0.025319959968328476, -0.04903765767812729, -0.023991769179701805, -0.031560350209474564, -0.006182413082569838, -0.031391691416502, 0.02938886359333992, -0.04916415363550186, 0.016296694055199623, -0.0037421255838125944, -0.01855251006782055, -0.025762690231204033, 0.07808919996023178, -0.021430257707834244, -0.02934669889509678, -0.037589915096759796, 0.0361563116312027, -0.04264969006180763, 0.009039077907800674, -0.0049121989868581295, 0.033795084804296494, -0.01659184694290161, 0.03206632658839226, 0.027322787791490555, -0.03267771750688553, -0.04608612135052681, 0.025319959968328476, -0.03695744276046753, -0.045242827385663986, -0.04121609032154083, -0.029873758554458618, 0.0003607066464610398, 0.029135873541235924, 0.007199638988822699, 0.04730890318751335, -0.007157474290579557, -0.020660750567913055, -0.03651471436023712, -0.03071705438196659, 0.041447993367910385, -0.05097723752260208, 0.013134334236383438, -0.04958580061793327, -0.009065431542694569, 0.0347016267478466, -0.047477561980485916, -0.05245300754904747, -0.01422007754445076, 0.04642344266176224, -0.010051033459603786, -0.048868998885154724, 0.04785704240202904, -0.004935916513204575, -0.08234784752130508, 0.030653806403279305, -0.005152011290192604, -0.034785956144332886, 0.0684756264090538, 0.03786398842930794, 0.035692501813173294, 0.028039589524269104, 0.041975054889917374, 0.020576421171426773, -0.03562925383448601, -0.006134978029876947, -0.0035181252751499414, -0.039866816252470016, 0.012839180417358875, -0.05725979432463646, -0.001227390835992992, 0.0423123724758625, -0.03109653666615486, -0.02597351372241974, 0.036451466381549835, 0.07197530567646027, 0.02719629369676113, 0.013302993029356003, -0.028545567765831947, -0.037885069847106934, 0.03417456895112991, -0.029873758554458618, -0.011848308145999908, 0.05957885831594467, -0.0394873321056366, 0.005096669774502516, 0.02439233474433422, 0.004532715771347284, 0.06489162147045135, -0.012870804406702518, 0.029641851782798767, -0.016844836995005608, -0.0038422669749706984, 0.0076423692516982555, -0.035313017666339874, -0.02953643910586834, 0.05426609143614769, 0.020386679098010063, 0.022558165714144707, 0.049290645867586136, -0.016191281378269196, -0.018679004162549973, 0.012132920324802399, -0.010725670494139194, -0.02839799039065838, 0.027259541675448418, -0.0323403999209404, -0.051061566919088364, 0.019933408126235008, 0.0423123724758625, 0.0076370988972485065, -0.023802027106285095, -0.043303247541189194, -0.027048716321587563, 0.01669725961983204, -0.03151818364858627, 0.021799199283123016, -0.0024099815636873245, -0.014399277977645397, -0.04414654150605202, -0.010330375283956528, -0.01779354363679886, 0.03200308233499527, 0.022516001015901566, -0.0036630667746067047, 0.04393571615219116, 0.013039463199675083, -0.004772528074681759, -0.041975054889917374, -0.000622919003944844, 0.04009872302412987, 0.010219692252576351, -0.0012280497467145324, 0.015326903201639652, -0.036261726170778275, -0.06708419322967529, -0.012238332070410252, -0.04170098528265953, 0.047013748437166214, -0.026690315455198288, 0.013208122923970222, 0.04309242218732834, -0.006556625943630934, 0.012754851020872593, 0.03784290328621864, 0.03828563541173935, 0.04511633142828941, -0.019332559779286385, 0.06303636729717255, -0.023317132145166397, 0.040920935571193695, -0.001053461106494069, 0.020745079964399338, 0.0038607141468673944, 0.0024310641456395388, 0.0561213456094265, -0.028925050050020218, -0.03971923887729645, 0.025657279416918755, 0.035776831209659576, 0.029599687084555626, 0.05241084098815918, -0.022790072485804558, 0.02435017004609108, -0.006314178463071585, 0.020481549203395844, -0.003328383667394519, -0.05283249169588089, 0.037210434675216675, -0.025741608813405037, -0.04494767263531685, 0.02886180393397808, 0.03575574606657028, 0.02012314833700657, 0.023654451593756676, -0.00456433929502964, -0.04507416859269142, 0.03290962427854538, -0.011774519458413124, 0.029030462726950645, -0.022178683429956436, -0.0380326472222805, -0.035734664648771286, 0.023274967446923256, 0.015295280143618584, 0.02177811786532402, -0.05097723752260208, -0.006340531166642904, -0.007663451600819826, 0.04996528476476669, 0.014662807807326317, 0.0394662506878376, 0.030042417347431183, -0.023127390071749687, 0.00019254158542025834, 0.03533409908413887, 0.030316488817334175, -0.049670130014419556, 0.029641851782798767, -0.01863683946430683, -0.013334617018699646, 0.005043963901698589, 0.0029357238672673702, -0.010325104929506779, 0.0029884299729019403, 0.051188062876462936, 0.043008092790842056, -0.03253014013171196, 0.00543398829177022, 0.013903841376304626, -0.04305025562644005, 0.0603799894452095, 0.0004061655781697482, 0.029599687084555626, -0.03082246705889702, -0.03248797729611397, 0.04916415363550186, -0.015906669199466705, 0.007099497597664595, -0.015537727624177933, -0.047098077833652496, 0.05165187641978264, 0.03596657142043114, 0.05873556062579155, 0.029599687084555626, -0.03394266217947006, 0.02106131613254547, -0.03744233772158623, 0.012375367805361748, -0.011068259365856647, -0.007463168818503618, 0.03695744276046753, -0.024813983589410782, -0.01274430938065052, 0.027512529864907265, -0.05198919400572777, 0.033689673990011215, 0.008554182946681976, 0.031180866062641144, 0.029937004670500755, 0.0427972674369812, 0.005067681428045034, 0.016728883609175682, 0.056711651384830475, 0.016539141535758972, -0.0197436660528183, -0.013134334236383438, 0.0016009446699172258, 0.1084478572010994, -0.011637483723461628, 0.027976341545581818, -0.030232159420847893, -0.0032071599271148443, 0.013081627897918224, -0.01944851316511631, -0.002801323775202036, -0.02510913647711277, 0.023949604481458664, 0.05274815857410431, 0.04773055016994476, 0.026690315455198288, 0.03727367892861366, -0.06813830882310867, 0.024413418024778366, 0.03413240239024162, -0.029852675274014473, -0.01015644520521164, -0.006941379513591528, 0.001719533116556704, -0.034743793308734894, -0.013155416585505009, -0.01189047284424305, 0.05696463957428932, -0.002912006340920925, -0.016665635630488396, -0.0361352302134037, 0.019617171958088875, 0.04857384413480759, 0.010493763722479343, 0.0427972674369812, -0.02386527508497238, -0.004155867733061314, -0.0018921452574431896, -0.026078926399350166, -0.049670130014419556, 0.017962202429771423, 0.02367553301155567, -0.026374079287052155, 0.009265714325010777, 0.05321197211742401, -0.008617429994046688, 0.01855251006782055, 0.01798328571021557, -0.010446328669786453, 0.0027486176695674658, -0.0020318161696195602, -0.01500012632459402, 0.014009254053235054, 0.005513047333806753, -0.009191925637423992, -0.00048357751802541316, -0.00893893651664257, 0.010994470678269863, -0.040920935571193695, -0.005913612898439169, 0.0031649949960410595, -0.031391691416502, -0.026753563433885574, -0.0005069657927379012, 0.02654273994266987, -0.0034417014103382826, 0.013830053620040417, -0.028882885351777077, -0.03786398842930794, 0.009033807553350925, 0.046971581876277924, 0.0542239286005497, -0.025994597002863884, 0.041258253157138824, -0.03451188653707504, -0.03151818364858627, 0.04355623573064804, -0.06379533559083939, -0.03170792758464813, -0.010409434325993061, -0.005544670857489109, -0.03267771750688553, -0.01826789788901806, -0.035776831209659576, 0.023043060675263405, -0.06906593590974808, -0.038306716829538345, 0.05772360786795616, -0.022663578391075134, -0.01817302778363228, 0.05881989002227783, -0.03225607052445412, -0.009039077907800674, -0.037547752261161804, 0.029367780312895775, -0.020513173192739487, 0.013007840141654015, -0.004213844425976276, 0.041152842342853546, -0.03647254779934883, 0.00011241200263611972, 0.009487079456448555, 0.024940477684140205, -0.001161508378572762, 0.028123918920755386, -0.0005972248036414385, -0.00917611364275217, -0.031138701364398003, -0.015253115445375443, 0.008027123287320137, 0.018331145867705345, 0.051525380462408066, 0.014114665798842907, 0.021693788468837738, 0.022473836317658424, -0.00711003877222538, 0.011342329904437065, 0.0062034958973526955, -0.01141611859202385, 0.009834938682615757, -0.049332812428474426, -0.05085074529051781, 0.03394266217947006, 0.023759862408041954, 0.04264969006180763, 0.04309242218732834, 0.01849980466067791, 0.0024521464947611094, -0.001711627235636115, 0.03923434391617775, -0.03109653666615486, 6.79001459502615e-05, -0.04983878880739212, 0.009518702514469624, -0.020154772326350212, 0.0018565687350928783, -0.02207327075302601, -0.049627967178821564, 0.03438539057970047, 0.04431520029902458, 0.03451188653707504, -0.023802027106285095, 0.00893893651664257, -0.01754055544734001, -0.033647507429122925, 0.011690190061926842, -0.054856400936841965, 0.01954338327050209, -0.006287825293838978, -0.01272322703152895, 0.011468824930489063, 0.007363027427345514, 0.009160301648080349, -0.027997424826025963, -0.04903765767812729, 0.03225607052445412, 0.04364056512713432, 0.07733023911714554, 0.010003598406910896, 0.06615656614303589, -0.025362124666571617, -0.0033916307147592306, -0.01162694301456213, 0.0080850999802351, 0.00889677181839943, -0.04764622077345848, -0.07218613475561142, -0.022410590201616287, -0.04617045074701309, -0.07307159155607224, -0.02810283750295639, -0.03491245210170746, -0.019901784136891365, -0.014546854421496391, 0.01698187179863453, -0.030042417347431183, -0.03693636134266853, 0.06168709695339203, 0.0328252948820591, 0.025615114718675613, -8.548253390472382e-05, 0.05616350844502449, 0.02886180393397808, 0.0442308709025383, 0.0047435397282242775, -0.03101220726966858, 0.015052832663059235, 0.011068259365856647, 0.010293480940163136, -0.05464557558298111, -0.014778761193156242, 0.022326258942484856, -0.02249491959810257, 0.03674662113189697, -0.012217249721288681, -0.028545567765831947, 0.0028724768199026585, -0.041300419718027115, -0.02787093073129654, 0.044736847281455994, 0.009824397973716259, 0.02768118865787983, -0.017667049542069435, 0.04212263226509094, 0.0347016267478466, 0.013060545548796654, 0.06438564509153366, -0.025889184325933456, -0.05051342770457268, -0.03465946391224861, 0.014473066665232182, 0.03267771750688553, -0.00494645768776536, -2.1082398234284483e-05, -0.009740067645907402, 0.05873556062579155, 0.03373183682560921, 0.0413636639714241, 0.04047820344567299, -0.030400818213820457, -0.020386679098010063, 0.023527955636382103, -0.01935364119708538, 0.023043060675263405, 0.028925050050020218, -0.019227147102355957, -0.01245969720184803, -0.03820130601525307, -0.023085225373506546, 0.010351457633078098, 0.026880057528614998, -0.019764747470617294, -0.06746367365121841, 0.03889702633023262, -0.010878517292439938, -0.008569994941353798, -0.00011941201955778524, 0.05536237731575966, -0.03124411404132843, -0.013260828331112862, 0.025994597002863884, 0.016855377703905106, -0.016201823949813843, 0.030653806403279305, -0.0254464540630579, 0.03204524517059326, -0.006988815031945705, 0.008928395807743073, 0.04121609032154083, -0.017635425552725792, 0.04528499022126198, -0.04047820344567299, 0.015063373371958733, -0.024139346554875374, 0.08529938012361526, -0.02601568028330803, -0.024181511253118515, 0.026521656662225723, 0.02435017004609108, 0.021135104820132256, -0.03874944895505905, -0.02196785807609558, 0.0004934598691761494, -0.015822339802980423, 0.005460340995341539, 0.05877772718667984, 0.0494593046605587, -0.04317675158381462, -0.02624758519232273, -0.023317132145166397, 0.05363362282514572, 0.05388661101460457, 0.04083660617470741, 0.015326903201639652, 0.011879931204020977, 0.0033758189529180527, -0.0044510215520858765, -0.03451188653707504, -0.018773876130580902, 0.024097181856632233, -0.005997942294925451, 0.05688031017780304, -0.06316286325454712, 0.000817601743619889, 0.02068183198571205, 0.007890087552368641, -0.02173595316708088, -0.011616401374340057, -0.026184339076280594, 0.014283324591815472, -0.030611641705036163, 0.018183568492531776, 0.049290645867586136, 0.040541451424360275, -0.01545339822769165, -0.03529193624854088, -0.08432959020137787, 0.01500012632459402, 0.002827676711603999, -0.013208122923970222, 0.0232960507273674, 0.012238332070410252, 0.03879161179065704, -0.009508161805570126, 0.03836996480822563, -0.026268668472766876, -0.02082940936088562, -0.014546854421496391, -0.03904459998011589, -0.009666279889643192, 0.03339451923966408, 0.010325104929506779, -0.046634264290332794, 0.03729476407170296, -0.00342325447127223, 0.011068259365856647, -0.009313149377703667, 0.04811003431677818, 0.054476916790008545, 0.012312120757997036, -0.029262369498610497, -0.016676176339387894, 0.04165881872177124, -0.005576294381171465, 0.032972872257232666, 0.0018328509759157896, -0.027238458395004272, 0.04494767263531685, -0.016296694055199623, -0.022895485162734985, -0.043387576937675476, -0.046971581876277924, -0.021335387602448463, -0.019374724477529526, -0.02886180393397808, -0.06206658110022545, -0.01778300292789936, -0.011205295100808144, 0.02877747267484665, 0.005892530083656311, 0.015611516311764717, -0.022178683429956436, -0.015316362492740154, 0.046634264290332794, 0.04208046570420265, 0.014061959460377693, -0.0028039589524269104, -0.024265840649604797, 0.006867591291666031, -0.031138701364398003, 0.008475123904645443, -0.03575574606657028, 0.006677849683910608, -0.0337529182434082, 0.03158143162727356, 0.040646862238645554, -0.010715128853917122, -0.017182154580950737, 0.002740711672231555, -0.061476271599531174, 0.012248873710632324, 0.050344765186309814, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], embedding_config=EmbeddingConfig(embedding_endpoint_type='hugging-face', embedding_endpoint='https://embeddings.memgpt.ai', embedding_model='BAAI/bge-large-en-v1.5', embedding_dim=1024, embedding_chunk_size=300, handle=None, azure_endpoint=None, azure_version=None, azure_deployment=None))]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.insert_archival_memory(\n",
    "    agent_state.id, \n",
    "    \"Bob's loves boston terriers\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2cf8cde-fc36-43a7-aa23-6cc26f1c829b",
   "metadata": {},
   "source": [
    "Now, we can have the agent run RAG to answer a specific question: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e228d992-24c6-4d9a-887a-f60d3beedca1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <style>\n",
       "        .message-container {\n",
       "            font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;\n",
       "            max-width: 800px;\n",
       "            margin: 20px auto;\n",
       "            background-color: #1e1e1e;\n",
       "            border-radius: 8px;\n",
       "            overflow: hidden;\n",
       "            color: #d4d4d4;\n",
       "        }\n",
       "        .message {\n",
       "            padding: 10px 15px;\n",
       "            border-bottom: 1px solid #3a3a3a;\n",
       "        }\n",
       "        .message:last-child {\n",
       "            border-bottom: none;\n",
       "        }\n",
       "        .title {\n",
       "            font-weight: bold;\n",
       "            margin-bottom: 5px;\n",
       "            color: #ffffff;\n",
       "            text-transform: uppercase;\n",
       "            font-size: 0.9em;\n",
       "        }\n",
       "        .content {\n",
       "            background-color: #2d2d2d;\n",
       "            border-radius: 4px;\n",
       "            padding: 5px 10px;\n",
       "            font-family: 'Consolas', 'Courier New', monospace;\n",
       "            white-space: pre-wrap;\n",
       "        }\n",
       "        .status-line {\n",
       "            margin-bottom: 5px;\n",
       "            color: #d4d4d4;\n",
       "        }\n",
       "        .function-name { color: #569cd6; }\n",
       "        .json-key { color: #9cdcfe; }\n",
       "        .json-string { color: #ce9178; }\n",
       "        .json-number { color: #b5cea8; }\n",
       "        .json-boolean { color: #569cd6; }\n",
       "        .internal-monologue { font-style: italic; }\n",
       "    </style>\n",
       "    <div class=\"message-container\">\n",
       "    \n",
       "        <div class=\"message\">\n",
       "            <div class=\"title\">REASONING MESSAGE</div>\n",
       "            <div class=\"content\">id=&#x27;message-e773250e-cd0d-424f-b6b7-2bb7ddabbcb4&#x27; date=datetime.datetime(2025, 1, 10, 6, 21, 34, 539751, tzinfo=datetime.timezone.utc) message_type=&#x27;reasoning_message&#x27; reasoning=&quot;Searching my archival memory for Charles&#x27;s preferences. The results should help me provide a tailored response.&quot;</div>\n",
       "        </div>\n",
       "        \n",
       "        <div class=\"message\">\n",
       "            <div class=\"title\">TOOL CALL MESSAGE</div>\n",
       "            <div class=\"content\">id=&#x27;message-e773250e-cd0d-424f-b6b7-2bb7ddabbcb4&#x27; date=datetime.datetime(2025, 1, 10, 6, 21, 34, 539751, tzinfo=datetime.timezone.utc) message_type=&#x27;tool_call_message&#x27; tool_call=ToolCall(name=&#x27;archival_memory_search&#x27;, arguments=&#x27;{\\n  &quot;query&quot;: &quot;Charles loves&quot;,\\n  &quot;request_heartbeat&quot;: true\\n}&#x27;, tool_call_id=&#x27;call_UG3X628hfn05OaMyjPMf3gYC&#x27;)</div>\n",
       "        </div>\n",
       "        \n",
       "        <div class=\"message\">\n",
       "            <div class=\"title\">TOOL RETURN MESSAGE</div>\n",
       "            <div class=\"content\">id=&#x27;message-0fd844f1-7ad5-466e-a2b1-829de0868c5e&#x27; date=datetime.datetime(2025, 1, 10, 6, 21, 35, 116497, tzinfo=datetime.timezone.utc) message_type=&#x27;tool_return_message&#x27; tool_return=&#x27;{\\n  &quot;status&quot;: &quot;OK&quot;,\\n  &quot;message&quot;: &quot;([{\\&#x27;timestamp\\&#x27;: \\&#x27;2025-01-10 06:20:51.298749\\&#x27;, \\&#x27;content\\&#x27;: \\\\&quot;\\&#x27;Bob loves cats\\&#x27;\\\\&quot;}, {\\&#x27;timestamp\\&#x27;: \\&#x27;2025-01-10 06:21:06.825968\\&#x27;, \\&#x27;content\\&#x27;: \\\\&quot;Bob\\&#x27;s loves boston terriers\\\\&quot;}], 2)&quot;,\\n  &quot;time&quot;: &quot;2025-01-10 02:21:35 PM CST+0800&quot;\\n}&#x27; status=&#x27;success&#x27; tool_call_id=&#x27;call_UG3X628hfn05OaMyjPMf3gYC&#x27; stdout=None stderr=None</div>\n",
       "        </div>\n",
       "        \n",
       "        <div class=\"message\">\n",
       "            <div class=\"title\">REASONING MESSAGE</div>\n",
       "            <div class=\"content\">id=&#x27;message-35b17741-ccdb-4e8a-badc-fb968650cf55&#x27; date=datetime.datetime(2025, 1, 10, 6, 21, 39, 640404, tzinfo=datetime.timezone.utc) message_type=&#x27;reasoning_message&#x27; reasoning=&quot;I couldn&#x27;t find any relevant information about Charles&#x27;s animal preferences. Perhaps I can encourage him to share more about his likes and interests.&quot;</div>\n",
       "        </div>\n",
       "        \n",
       "        <div class=\"message\">\n",
       "            <div class=\"title\">TOOL CALL MESSAGE</div>\n",
       "            <div class=\"content\">id=&#x27;message-35b17741-ccdb-4e8a-badc-fb968650cf55&#x27; date=datetime.datetime(2025, 1, 10, 6, 21, 39, 640404, tzinfo=datetime.timezone.utc) message_type=&#x27;tool_call_message&#x27; tool_call=ToolCall(name=&#x27;send_message&#x27;, arguments=&#x27;{\\n  &quot;message&quot;: &quot;I\\&#x27;m sorry Charles, but I currently don\\&#x27;t have any information about which animals you like in my archival memory. 😞&quot;\\n}&#x27;, tool_call_id=&#x27;call_wAOp59uZQQ4gzWhlNEVmuO2f&#x27;)</div>\n",
       "        </div>\n",
       "        \n",
       "        <div class=\"message\">\n",
       "            <div class=\"title\">TOOL RETURN MESSAGE</div>\n",
       "            <div class=\"content\">id=&#x27;message-dbb3579e-665a-464d-ae34-fa58bc7e5d65&#x27; date=datetime.datetime(2025, 1, 10, 6, 21, 39, 647604, tzinfo=datetime.timezone.utc) message_type=&#x27;tool_return_message&#x27; tool_return=&#x27;{\\n  &quot;status&quot;: &quot;OK&quot;,\\n  &quot;message&quot;: &quot;None&quot;,\\n  &quot;time&quot;: &quot;2025-01-10 02:21:39 PM CST+0800&quot;\\n}&#x27; status=&#x27;success&#x27; tool_call_id=&#x27;call_wAOp59uZQQ4gzWhlNEVmuO2f&#x27; stdout=None stderr=None</div>\n",
       "        </div>\n",
       "        </div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "response = client.send_message(\n",
    "    agent_id=agent_state.id, \n",
    "    role=\"user\", \n",
    "    message=\"What animals do I like? Search archival.\"\n",
    ")\n",
    "nb_print(response.messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f854c2d-fe1a-460f-9c55-392b14947e8c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
