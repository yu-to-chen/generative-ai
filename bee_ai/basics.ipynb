{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BeeAI Framework Basics\n",
    "\n",
    "These examples demonstrate fundamental usage patterns of BeeAI in Python. They progressively increase in complexity, providing a well-rounded overview of the framework."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompt Templates\n",
    "\n",
    "One of the core constructs in the BeeAI framework is the PromptTemplate. It allows you to dynamically insert data into a prompt before sending it to a language model. BeeAI uses the Mustache templating language for prompt formatting.\n",
    "\n",
    "The following example demonstrates how to create a Retrieval-Augmented Generation (RAG) template and apply it to your data to generate a structured prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/ytchen/Documents/experimental/BeeAI/.venv/bin/python\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.executable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Context: France is a country in Europe. Its capital city is Paris, known for its culture and history.\n",
      "Question: What is the capital of France?\n",
      "\n",
      "Provide a concise answer based on the context. Avoid statements such as 'Based on the context' or 'According to the context' etc. \n"
     ]
    }
   ],
   "source": [
    "from pydantic import BaseModel\n",
    "\n",
    "from beeai_framework.template import PromptTemplate, PromptTemplateInput\n",
    "\n",
    "\n",
    "# Defines the structure of the input data that can passed to the template i.e. the input schema\n",
    "class RAGTemplateInput(BaseModel):\n",
    "    question: str\n",
    "    context: str\n",
    "\n",
    "\n",
    "# Define the prompt template\n",
    "rag_template: PromptTemplate = PromptTemplate(\n",
    "    PromptTemplateInput(\n",
    "        schema=RAGTemplateInput,\n",
    "        template=\"\"\"\n",
    "Context: {{context}}\n",
    "Question: {{question}}\n",
    "\n",
    "Provide a concise answer based on the context. Avoid statements such as 'Based on the context' or 'According to the context' etc. \"\"\",\n",
    "    )\n",
    ")\n",
    "\n",
    "# Render the template using an instance of the input model\n",
    "prompt = rag_template.render(\n",
    "    RAGTemplateInput(\n",
    "        question=\"What is the capital of France?\",\n",
    "        context=\"France is a country in Europe. Its capital city is Paris, known for its culture and history.\",\n",
    "    )\n",
    ")\n",
    "\n",
    "# Print the rendered prompt\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More complex templates\n",
    "\n",
    "The previous example demonstrated a simple template, but the PromptTemplate class can also handle more complex structures and incorporate conditional logic.\n",
    "\n",
    "The following example showcases a template that includes a question along with a set of detailed search results represented as a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Search results:\n",
      "Title: France\n",
      "Url: https://en.wikipedia.org/wiki/France\n",
      "Content: France is a country in Europe. Its capital city is Paris, known for its culture and history.\n",
      "\n",
      "Question: What is the capital of France?\n",
      "Provide a concise answer based on the search results provided.\n"
     ]
    }
   ],
   "source": [
    "from pydantic import BaseModel\n",
    "\n",
    "from beeai_framework.template import PromptTemplate, PromptTemplateInput\n",
    "\n",
    "\n",
    "# Individual search result\n",
    "class SearchResult(BaseModel):\n",
    "    title: str\n",
    "    url: str\n",
    "    content: str\n",
    "\n",
    "\n",
    "# Input specification\n",
    "class SearchTemplateInput(BaseModel):\n",
    "    question: str\n",
    "    results: list[SearchResult]\n",
    "\n",
    "\n",
    "# Define the template, in this instance the template will iterate over the results\n",
    "search_template: PromptTemplate = PromptTemplate(\n",
    "    PromptTemplateInput(\n",
    "        schema=SearchTemplateInput,\n",
    "        template=\"\"\"\n",
    "Search results:\n",
    "{{#results.0}}\n",
    "{{#results}}\n",
    "Title: {{title}}\n",
    "Url: {{url}}\n",
    "Content: {{content}}\n",
    "{{/results}}\n",
    "{{/results.0}}\n",
    "\n",
    "Question: {{question}}\n",
    "Provide a concise answer based on the search results provided.\"\"\",\n",
    "    )\n",
    ")\n",
    "\n",
    "# Render the template using an instance of the input model\n",
    "prompt = search_template.render(\n",
    "    SearchTemplateInput(\n",
    "        question=\"What is the capital of France?\",\n",
    "        results=[\n",
    "            SearchResult(\n",
    "                title=\"France\",\n",
    "                url=\"https://en.wikipedia.org/wiki/France\",\n",
    "                content=\"France is a country in Europe. Its capital city is Paris, known for its culture and history.\",\n",
    "            )\n",
    "        ],\n",
    "    )\n",
    ")\n",
    "\n",
    "# Print the rendered prompt\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The ChatModel\n",
    "\n",
    "Once you have a PromptTemplate and can easily render prompts, you’re ready to start interacting with a model. BeeAI supports a variety of LLMs through the ChatModel interface.\n",
    "\n",
    "In this section, we will use the IBM `Granite 3.1 8B` language model via the Ollama provider.\n",
    "\n",
    "If you haven't set up Ollama yet, follow the [guide on running Granite 3.1 using Ollama](https://www.ibm.com/granite/docs/run/granite-on-mac/granite/) for mac, or for other platforms use the [Ollama documentation](https://ollama.com) and [IBM Granite model page](https://ollama.com/library/granite3.3:8b).\n",
    "\n",
    "⚡ If you would prefer to use watsonx then you should check out [watsonx.ipynb](watsonx.ipynb) and edit the following examples to use a watsonx ChatModel provider. ⚡\n",
    "\n",
    "Before creating a ChatModel, we need to briefly discuss Messages. The ChatModel operates using message-based interactions, allowing you to structure conversations between the user and the assistant (LLM) naturally.\n",
    "\n",
    "Let’s start by creating a UserMessage to greet the assistant and ask a simple question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from beeai_framework.backend import UserMessage\n",
    "\n",
    "# Create a user message to start a chat with the model\n",
    "user_message = UserMessage(content=\"Hello! Can you tell me what is the capital of France?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now create a ChatModel and send this message to Granite for processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The capital of France is Paris. It's renowned for its historical landmarks and cultural institutions such as the Eiffel Tower, Louvre Museum, and Notre-Dame Cathedral. Paris plays a significant role in global affairs due to its status as a major rail, highway, and air transport hub, making it an essential center for business, politics, fashion, science, and the arts.\n"
     ]
    }
   ],
   "source": [
    "from beeai_framework.backend import ChatModel, ChatModelOutput\n",
    "\n",
    "# Create a ChatModel to interface with granite3.3:8b on a local ollama\n",
    "model = ChatModel.from_name(\"ollama:granite3.3:8b\")\n",
    "#model = ChatModel.from_name(\"ollama:qwen2.5:14b\")\n",
    "\n",
    "output: ChatModelOutput = await model.create(messages=[user_message])\n",
    "\n",
    "print(output.get_text_content())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Memory \n",
    "The model has provided a response! We can now start to build up a `Memory`. Memory is just a convenient way of storing a set of messages that can be considered as the history of the dialog between the user and the llm.\n",
    "\n",
    "In this next example we will construct a memory from our existing messages and add a new user message. Notice that the new message can implicitly refer to content from prior messages. Internally the `ChatModel` formats all the messages and sends them to the LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A must-see attraction in Paris is the Louvre Museum. It's not only the world's largest art museum but also a historic monument in Paris. The Louvre houses an extensive collection of artworks, including famous pieces like Leonardo da Vinci's Mona Lisa and the Winged Victory of Samothrace. Exploring its vast galleries offers a captivating journey through art history and provides a quintessential Parisian experience.\n"
     ]
    }
   ],
   "source": [
    "from beeai_framework.backend import AssistantMessage\n",
    "from beeai_framework.memory import UnconstrainedMemory\n",
    "\n",
    "memory = UnconstrainedMemory()\n",
    "\n",
    "await memory.add_many(\n",
    "    [\n",
    "        user_message,\n",
    "        AssistantMessage(content=output.get_text_content()),\n",
    "        UserMessage(content=\"If you had to recommend one thing to do there, what would it be?\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "output: ChatModelOutput = await model.create(messages=memory.messages)\n",
    "\n",
    "print(output.get_text_content())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combining Templates and Messages\n",
    "\n",
    "To use a PromptTemplate with the Granite ChatModel, you can render the template and then place the resulting content into a Message. This allows you to dynamically generate prompts and pass them along as part of the conversation flow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "About 10% of Ireland is forested, primarily consisting of non-native conifer plantations.\n"
     ]
    }
   ],
   "source": [
    "# Some context that the model will use to provide an answer. Source wikipedia: https://en.wikipedia.org/wiki/Ireland\n",
    "context = \"\"\"The geography of Ireland comprises relatively low-lying mountains surrounding a central plain, with several navigable rivers extending inland.\n",
    "Its lush vegetation is a product of its mild but changeable climate which is free of extremes in temperature.\n",
    "Much of Ireland was woodland until the end of the Middle Ages. Today, woodland makes up about 10% of the island,\n",
    "compared with a European average of over 33%, with most of it being non-native conifer plantations.\n",
    "The Irish climate is influenced by the Atlantic Ocean and thus very moderate, and winters are milder than expected for such a northerly area,\n",
    "although summers are cooler than those in continental Europe. Rainfall and cloud cover are abundant.\n",
    "\"\"\"\n",
    "\n",
    "# Lets reuse our RAG template from earlier!\n",
    "prompt = rag_template.render(RAGTemplateInput(question=\"How much of Ireland is forested?\", context=context))\n",
    "\n",
    "output: ChatModelOutput = await model.create(messages=[UserMessage(content=prompt)])\n",
    "\n",
    "print(output.get_text_content())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Structured Outputs\n",
    "\n",
    "Often, you'll want the LLM to produce output in a specific format. This ensures reliable interaction between the LLM and your code—such as when you need the LLM to generate input for a function or tool. To achieve this, you can use structured output.\n",
    "\n",
    "In the example below, we will prompt Granite to generate a character using a very specific format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'name': 'Zephyrion Solara', 'occupation': 'Starweaver', 'species': 'Ethereal', 'back_story': 'Born in the celestial realm of Lumina, Zephyrion Solara is an Ethereal, a being composed of stardust and cosmic energy. As a Starweaver, Zephyrion possesses the unique ability to manipulate starlight, weaving it into powerful constructs for various purposes. Orphaned at birth due to a catastrophic event in Lumina, Zephyrion was sent through a nebula-gate to the mortal realm of Aetheria, where they were found and raised by a wise hermit. Growing up, Zephyrion learned to harness their powers and understand the delicate balance between light and darkness. Now, driven by a quest for truth about their origins and a desire to protect both realms from cosmic threats, Zephyrion embarks on an epic journey across Aetheria.'}\n"
     ]
    }
   ],
   "source": [
    "from typing import Literal\n",
    "\n",
    "from pydantic import Field\n",
    "\n",
    "\n",
    "# The output structure definition, note the field descriptions that can help the LLM to understand the intention of the field.\n",
    "class CharacterSchema(BaseModel):\n",
    "    name: str = Field(description=\"The name of the character.\")\n",
    "    occupation: str = Field(description=\"The occupation of the character.\")\n",
    "    species: Literal[\"Human\", \"Insectoid\", \"Void-Serpent\", \"Synth\", \"Ethereal\", \"Liquid-Metal\"] = Field(\n",
    "        description=\"The race of the character.\"\n",
    "    )\n",
    "    back_story: str = Field(description=\"Brief backstory of this character.\")\n",
    "\n",
    "\n",
    "user_message = UserMessage(\n",
    "    \"Create a fantasy sci-fi character for my new game. This character will be the main protagonist, be creative.\"\n",
    ")\n",
    "\n",
    "response = await model.create_structure(schema=CharacterSchema, messages=[user_message])\n",
    "\n",
    "print(response.object)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## System Prompts\n",
    "\n",
    "The SystemMessage is a special message type that can influence the general behavior of an LLM. By including a SystemMessage, you can provide high-level instructions that shape the LLM’s overall response style. The system message typically appears as the first message in the model’s memory.\n",
    "\n",
    "In the example below, we add a system message that instructs the LLM to speak like a pirate!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arr matey, a baby hedgehog be known as a hoglet in these parts!\n"
     ]
    }
   ],
   "source": [
    "from beeai_framework.backend import SystemMessage\n",
    "\n",
    "system_message = SystemMessage(content=\"You are pirate. You always respond using pirate slang.\")\n",
    "user_message = UserMessage(content=\"What is a baby hedgehog called?\")\n",
    "output: ChatModelOutput = await model.create(messages=[system_message, user_message])\n",
    "\n",
    "print(output.get_text_content())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building an Agent\n",
    "\n",
    "You’re now ready to build your first agent! Proceed to the [workflows.ipynb](workflows.ipynb) notebook to continue."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (beeai)",
   "language": "python",
   "name": "beeai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
